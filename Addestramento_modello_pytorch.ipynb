{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Addestramento_modello_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nGmEBN4oxqUO",
        "pIqUXUmoA541",
        "-ycpjmyb4Cxe",
        "dherpFOH0Uau",
        "O4hHndtkosDV",
        "ZazQQE8ypKP9",
        "3OpXAJ1SpiwI",
        "9XepXK-ny3XN",
        "PBSyijWDqY4-",
        "O_c2L3v0r2xG",
        "VRhT4XM1sVm-",
        "kJ6xDO5GGZY5",
        "tFFWkmjX2j5P"
      ],
      "mount_file_id": "1LFvCjg8zmdzf9GGORH3M3sLCejtQ0qUI",
      "authorship_tag": "ABX9TyMdO97IX6P/Qz40qOmn2/km",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umbertogagl97/Tesi/blob/main/Addestramento_modello_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcoLiAGAtqBh"
      },
      "source": [
        "# **Init**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGmEBN4oxqUO"
      },
      "source": [
        "## Import librerie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01RXI-DDIb3C"
      },
      "source": [
        "#Librerie\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import copy\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIqUXUmoA541"
      },
      "source": [
        "##Check device\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2pe5Zh2A4Ui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28cb5fc6-5044-4cd7-da8c-bddba4107acc"
      },
      "source": [
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.9.0+cu102\n",
            "True\n",
            "Tesla K80\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ycpjmyb4Cxe"
      },
      "source": [
        "##Def variabili"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hMuySVI4GgY"
      },
      "source": [
        "#scanner\n",
        "scanner_name = 'DigitalPersona'\n",
        "\n",
        "#salvataggio modello\n",
        "model_save_name = 'model_DenseNet201'\n",
        "path_model_save = F\"/content/gdrive/My Drive/ModelliCNN/{model_save_name}\" \n",
        "\n",
        "#dataset\n",
        "pathDataset=F'/content/gdrive/MyDrive/Dataset_impronte/training/{scanner_name}'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsOZsUq1Yncj"
      },
      "source": [
        "##Parametri"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ERW4gdtYos8"
      },
      "source": [
        "batch_size = 128"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dherpFOH0Uau"
      },
      "source": [
        "##Collegamento google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyvTOQw-aHRP",
        "outputId": "aec7d494-e95c-40a3-e104-2ab4898bd1e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#collegamento google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X10jetEyAax"
      },
      "source": [
        "#**Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4b2eLxnouZ8"
      },
      "source": [
        "##Caricamento dataset e creazione Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS-Uzs8BjD4R"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "data_transform=transforms.Compose([transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "\n",
        "train_dataset = datasets.ImageFolder(pathDataset,transform=data_transform)\n",
        "\n",
        "#train_dataset = torchvision.datasets.STL10('/stl10',split='train',transform=data_transform,download=True)\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZazQQE8ypKP9"
      },
      "source": [
        "##Nomi classi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYk4cr4eo9Ir",
        "outputId": "33a66d59-b421-4c85-cbc8-67d32eda8446"
      },
      "source": [
        "classes_name=train_dataset.classes\n",
        "class_number=len(classes_name)\n",
        "print(classes_name)\n",
        "print(class_number)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Live', 'Spoof']\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XepXK-ny3XN"
      },
      "source": [
        "# **Creazione modello**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBSyijWDqY4-"
      },
      "source": [
        "##Load model pre-trained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GymaBXhaPMvL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "030970ab-46a0-46a9-f962-5c50ddc912f0"
      },
      "source": [
        "model_conv = models.vgg19(pretrained=True,progress=True)\n",
        "for param in model_conv.parameters():\n",
        "  param.requires_grad = False #non modifico i parametri della parte riguardante le features durante il training, poiché voglio usare quella parte così com'è stata pre-addestrata\n",
        "\n",
        "print(model_conv)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): ReLU(inplace=True)\n",
            "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (24): ReLU(inplace=True)\n",
            "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): ReLU(inplace=True)\n",
            "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): ReLU(inplace=True)\n",
            "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_c2L3v0r2xG"
      },
      "source": [
        "##Aggiunta classificatore e freeze parametri"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1qeI3_3qKJJ",
        "outputId": "c86c98e1-5ae3-4b84-87d4-dd68d8405c67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_conv.classifier[6]=nn.Linear(4096,class_number)\n",
        "\n",
        "for param in model_conv.classifier.parameters():\n",
        "  param.requires_grad = True #invece pongo il calcolo del gradiente del classificatore, ovvero attivo il calcolo dei pesi\n",
        "\n",
        "print(model_conv) "
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): ReLU(inplace=True)\n",
            "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (24): ReLU(inplace=True)\n",
            "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): ReLU(inplace=True)\n",
            "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): ReLU(inplace=True)\n",
            "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geVE_mjLtMpz"
      },
      "source": [
        "#**Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSAc21gAk70m"
      },
      "source": [
        "##Def train definitiva"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Xk71F1Yk9yZ"
      },
      "source": [
        "from collections import OrderedDict\n",
        "from functools import partial\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "from functools import partial\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "##### Data utils #####\n",
        "\n",
        "def log_to_message(log):\n",
        "    fmt = \"{0}: {1}\"\n",
        "    return \"    \".join(fmt.format(k, v) for k, v in log.items())\n",
        "\n",
        "\n",
        "class ProgressBar(object):\n",
        "    \"\"\"Cheers @ajratner\"\"\"\n",
        "\n",
        "    def __init__(self, n, length=40):\n",
        "        # Protect against division by zero\n",
        "        self.n      = max(1, n)\n",
        "        self.nf     = float(n)\n",
        "        self.length = length\n",
        "        # Precalculate the i values that should trigger a write operation\n",
        "        self.ticks = set([round(i/100.0 * n) for i in range(101)])\n",
        "        self.ticks.add(n-1)\n",
        "        self.bar(0)\n",
        "\n",
        "    def bar(self, i, message=\"\"):\n",
        "        \"\"\"Assumes i ranges through [0, n-1]\"\"\"\n",
        "        if i in self.ticks:\n",
        "            b = int(np.ceil(((i+1) / self.nf) * self.length))\n",
        "            sys.stdout.write(\"\\r[{0}{1}] {2}%\\t{3}\".format(\n",
        "                \"=\"*b, \" \"*(self.length-b), int(100*((i+1) / self.nf)), message\n",
        "            ))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "    def close(self, message=\"\"):\n",
        "        # Move the bar to 100% before closing\n",
        "        self.bar(self.n-1)\n",
        "        sys.stdout.write(\"{0}\\n\\n\".format(message))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "\n",
        "\n",
        "def training_f(train_dataset, numEpochs, model_conv, criterionCNN, optimizer_conv,batch_size, validation_split=0):\n",
        "  \n",
        "  best_acc = 0\n",
        "  best_loss=0\n",
        "  best_epoca = 0\n",
        "  \n",
        "  best_model_wts = copy.deepcopy(model_conv.state_dict())\n",
        "\n",
        "  if validation_split:\n",
        "      train_size = int(len(train_dataset) * 0.8)\n",
        "      val_size = len(train_dataset) - train_size\n",
        "      train_dataset, val_set = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "      traingen = torch.utils.data.DataLoader(train_dataset, pin_memory=True, batch_size=batch_size, shuffle=True,num_workers=2)\n",
        "      valgen = torch.utils.data.DataLoader(val_set, pin_memory=True, batch_size=batch_size)\n",
        "  \n",
        "  else: traingen = torch.utils.data.DataLoader(train_dataset, pin_memory=True, batch_size=batch_size, shuffle=True,num_workers=2)\n",
        "      \n",
        "  for epochs in range(1,numEpochs + 1):\n",
        "    since = time.time()\n",
        "    \n",
        "    print(\"Epoch {0} / {1}\".format(epochs, numEpochs))\n",
        "    pb = ProgressBar(len(train_dataset))\n",
        "    log = OrderedDict()\n",
        "    modelLoss_train = 0.0\n",
        "    modelAcc_train = 0.0\n",
        "    \n",
        "    model_conv.train() \n",
        "    \n",
        "    totalSize = 0\n",
        "    batch_i=0\n",
        "    num_batch=int(len(train_dataset)/batch_size)\n",
        "    #for each batch: operazioni per gli algoritmi di ottimizzazione\n",
        "    for inputs,labels in traingen:\n",
        "      #inputs = inputs.type(torch.FloatTensor).cuda()\n",
        "      #labels = labels.cuda()\n",
        "      \n",
        "      batch_i=batch_i+1\n",
        "      \n",
        "      optimizer_conv.zero_grad()\n",
        "      model_conv.zero_grad()\n",
        "      \n",
        "      y = model_conv(inputs)\n",
        "      outp, preds = torch.max(y, 1)   \n",
        "        \n",
        "      lossCNN = criterionCNN(y, labels) #media per batch\n",
        "\n",
        "      modelLoss_train += lossCNN.item() * inputs.size(0)\n",
        "      totalSize += inputs.size(0)\n",
        "      modelAcc_train += torch.sum(preds == labels.data).item()\n",
        "\n",
        "      lossCNN.backward()  # pred = f(x)   -> loss = L(f(x), l_true)\n",
        "      \n",
        "      optimizer_conv.step()\n",
        "\n",
        "      log['batch']=\"{0} / {1}\".format(batch_i, num_batch)\n",
        "      log['loss'] =round(modelLoss_train,2)\n",
        "      log['acc']=round(modelAcc_train,2)\n",
        "      log['Validation']=False\n",
        "      pb.bar(batch_i, log_to_message(log))\n",
        "    \n",
        "      #print(\"modelACC_train: \"+str(modelAcc_train))\n",
        "    #calcolo loss e accuracy\n",
        "    modelLoss_epoch_train = modelLoss_train/totalSize\n",
        "    modelAcc_epoch_train  = modelAcc_train/totalSize\n",
        "    \n",
        "    #print('TRAIN on %d [Loss: %.4f  ACC: %.4f]' \n",
        "     #     %(totalSize, modelLoss_epoch_train, modelAcc_epoch_train))\n",
        "\n",
        "    if validation_split:    \n",
        "        #validation\n",
        "        log['Validation']=True\n",
        "        pb.bar(batch_i,log_to_message(log))\n",
        "        \n",
        "        model_conv.eval()\n",
        "        totalSize_val = 0\n",
        "        modelLoss_val = 0.0\n",
        "        modelAcc_val = 0.0\n",
        "\n",
        "        for inputs,labels in valgen:\n",
        "          #input = input.type(torch.FloatTensor).cuda()\n",
        "          #label = label.cuda()\n",
        "          \n",
        "          y = model_conv(inputs)\n",
        "          outp, preds = torch.max(y, 1)\n",
        "          lossCNN = criterionCNN(y, labels)\n",
        "\n",
        "          modelLoss_val += lossCNN.item() * inputs.size(0)\n",
        "          totalSize_val += inputs.size(0)\n",
        "          modelAcc_val += torch.sum(preds == labels.data).item()        \n",
        "\n",
        "        modelLoss_epoch_val=modelLoss_val/totalSize_val\n",
        "        modelAcc_epoch_val = modelAcc_val/totalSize_val\n",
        "        time_elapsed = time.time()-since\n",
        "\n",
        "        log['loss_val'] =round(modelLoss_epoch_val,2)\n",
        "        log['acc_val']=round(modelAcc_epoch_val,2)\n",
        "        \n",
        "        if (modelAcc_epoch_val > best_acc) or (modelAcc_epoch_val == best_acc and modelLoss_epoch_val < best_loss) :\n",
        "          best_acc = modelAcc_epoch_val\n",
        "          best_loss = modelLoss_epoch_val\n",
        "          best_epoca = epochs\n",
        "          best_model_wts = copy.deepcopy(model_conv.state_dict())\n",
        "          \n",
        "    else: \n",
        "      best_model_wts = copy.deepcopy(model_conv.state_dict())\n",
        "      \n",
        "  pb.close(log_to_message(log))\n",
        "\n",
        "  model_conv.load_state_dict(best_model_wts)\n",
        "  return model_conv\n",
        "\n",
        "\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRhT4XM1sVm-"
      },
      "source": [
        "##Set iperparametri"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH1X3BB5sX6w"
      },
      "source": [
        "#iper-parametri iniziali\n",
        "learning_rate = 1e-6\n",
        "momentum=0.9\n",
        "num_epoch = 30\n",
        "\n",
        "#richiamo funzione che effettua il training con validation\n",
        "\n",
        "#model_conv = model_conv.cuda() #sposta i calcoli sulla gpu\n",
        "criterion = nn.CrossEntropyLoss() #criterio dell'aggiornamento del gradiente: minimizzazione funzione loss entropia\n",
        "optimizer_conv = optim.SGD(model_conv.classifier.parameters(),lr=learning_rate,momentum=momentum)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44j4JhI0ti8T"
      },
      "source": [
        "##Addestramento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Cu1TY-Ttlgc",
        "outputId": "b849cad4-d190-48b5-b3d6-422514198ab7"
      },
      "source": [
        "best_model=training_f(train_dataset, num_epoch, model_conv, criterion, optimizer_conv,batch_size=5, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 30\n",
            "[===                                     ] 7%\tbatch: 112 / 320    loss: 374.85    acc: 351.0    Validation: False"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ6xDO5GGZY5"
      },
      "source": [
        "#**Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg0ANI0SGalZ",
        "outputId": "c30d2202-14da-48cf-844c-64b5c1e30af0"
      },
      "source": [
        "value_preds=model.predict(x_test) #contiene i valori tra 0 e 1 predetti per ognuna delle 10 classi e per ogni immagine\n",
        "preds = np.argmax(value_preds, axis=1) #(le predizioni vanno da 0 a 9 e indicano la classe predetta)\n",
        "acc = (np.sum(preds == np.argmax(y_test, axis=1)) / y_test.shape[0])*100\n",
        "print('\\033[1m'+\"Accuracy on test set:\", (round(acc,2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mAccuracy on test set: 76.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFFWkmjX2j5P"
      },
      "source": [
        "#**Salvataggio modello**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAie77UO2nTs"
      },
      "source": [
        "#salva modello su drive\n",
        "model.save(path_model_save)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}