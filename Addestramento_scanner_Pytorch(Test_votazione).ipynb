{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Addestramento_scanner_Pytorch(Test_votazione).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nGmEBN4oxqUO",
        "pIqUXUmoA541",
        "-ycpjmyb4Cxe",
        "dherpFOH0Uau",
        "O4hHndtkosDV",
        "ZazQQE8ypKP9",
        "3OpXAJ1SpiwI",
        "9XepXK-ny3XN",
        "PBSyijWDqY4-",
        "O_c2L3v0r2xG",
        "VRhT4XM1sVm-",
        "kJ6xDO5GGZY5",
        "tFFWkmjX2j5P"
      ],
      "mount_file_id": "1LFvCjg8zmdzf9GGORH3M3sLCejtQ0qUI",
      "authorship_tag": "ABX9TyOhJ2c372Ul/KT/jdfTJTCN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umbertogagl97/Tesi/blob/main/Addestramento_scanner_Pytorch(Test_votazione).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcoLiAGAtqBh"
      },
      "source": [
        "# **Init**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGmEBN4oxqUO"
      },
      "source": [
        "## Import librerie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01RXI-DDIb3C"
      },
      "source": [
        "#Librerie\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import copy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIqUXUmoA541"
      },
      "source": [
        "##Check device\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2pe5Zh2A4Ui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c8f20cb-3464-486a-e0df-dc9eb5f1c3af"
      },
      "source": [
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.9.0+cu111\n",
            "True\n",
            "Tesla P100-PCIE-16GB\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ycpjmyb4Cxe"
      },
      "source": [
        "##Def path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hMuySVI4GgY"
      },
      "source": [
        "#scanner\n",
        "scanner_name = 'HiScan'\n",
        "\n",
        "#salvataggio modello\n",
        "model_save_name = 'VGG19_paper_100epoc'\n",
        "path_model_save = F\"/content/gdrive/My Drive/ModelliCNN/Scanner/{scanner_name}/Pytorch{model_save_name}\" \n",
        "\n",
        "#dataset\n",
        "pathDataset=F'/content/gdrive/MyDrive/Dataset_impronte/training/{scanner_name}_patch'\n",
        "pathTestset=F'/content/gdrive/MyDrive/Dataset_impronte/test/{scanner_name}'"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dherpFOH0Uau"
      },
      "source": [
        "##Collegamento google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyvTOQw-aHRP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69441aaf-7cdc-422f-996a-7141858b7adb"
      },
      "source": [
        "#collegamento google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X10jetEyAax"
      },
      "source": [
        "#**Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4b2eLxnouZ8"
      },
      "source": [
        "##Caricamento dataset e creazione Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS-Uzs8BjD4R"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "data_transform=transforms.Compose([transforms.Resize([224,224]),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "\n",
        "train_dataset = datasets.ImageFolder(pathDataset,transform=data_transform)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxWBDGmnjSge",
        "outputId": "eb28c7bf-9e6b-4d51-fd6d-08774773ac32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(train_dataset[0])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<PIL.Image.Image image mode=RGB size=800x800 at 0x7FB1CE8C1B10>, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZazQQE8ypKP9"
      },
      "source": [
        "##Nomi classi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYk4cr4eo9Ir",
        "outputId": "62a5cd21-d7c2-42aa-e636-7c28c6f9a0c1"
      },
      "source": [
        "classes_name=train_dataset.classes\n",
        "class_number=len(classes_name)\n",
        "print(classes_name)\n",
        "print(class_number)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Live', 'Spoof']\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XepXK-ny3XN"
      },
      "source": [
        "# **Creazione modello**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBSyijWDqY4-"
      },
      "source": [
        "##Load model pre-trained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GymaBXhaPMvL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "ab9dbca0ba304b058351cef204fa381d"
          ]
        },
        "outputId": "ad3dc483-9d4d-493a-ccb1-cea4dd57afcb"
      },
      "source": [
        "model_conv = models.vgg19(pretrained=True,progress=True)\n",
        "for param in model_conv.parameters():\n",
        "  param.requires_grad = False #non modifico i parametri della parte riguardante le features durante il training, poiché voglio usare quella parte così com'è stata pre-addestrata\n",
        "\n",
        "print(model_conv)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab9dbca0ba304b058351cef204fa381d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): ReLU(inplace=True)\n",
            "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (24): ReLU(inplace=True)\n",
            "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): ReLU(inplace=True)\n",
            "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): ReLU(inplace=True)\n",
            "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_c2L3v0r2xG"
      },
      "source": [
        "##Aggiunta classificatore e freeze parametri"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1qeI3_3qKJJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58372782-88cf-48e5-e42f-550345641fe4"
      },
      "source": [
        "model_conv.classifier[6]=nn.Linear(4096,2)\n",
        "\n",
        "for param in model_conv.classifier.parameters():\n",
        "  param.requires_grad = True #invece pongo il calcolo del gradiente del classificatore, ovvero attivo il calcolo dei pesi\n",
        "\n",
        "print(model_conv) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): ReLU(inplace=True)\n",
            "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (24): ReLU(inplace=True)\n",
            "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): ReLU(inplace=True)\n",
            "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): ReLU(inplace=True)\n",
            "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOHqOuTVAMaz"
      },
      "source": [
        "##Load pesi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEXv4tQ3AOi0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de326e0-1167-42d6-9836-7eb99197d134"
      },
      "source": [
        "best_model=model_conv\n",
        "best_model.load_state_dict(torch.load(path_model_save,map_location=torch.device('cpu')))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geVE_mjLtMpz"
      },
      "source": [
        "#**Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSAc21gAk70m"
      },
      "source": [
        "##Def train definitiva"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Xk71F1Yk9yZ"
      },
      "source": [
        "from collections import OrderedDict\n",
        "from functools import partial\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "from functools import partial\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "##### Data utils #####\n",
        "\n",
        "def log_to_message(log):\n",
        "    fmt = \"{0}: {1}\"\n",
        "    return \"    \".join(fmt.format(k, v) for k, v in log.items())\n",
        "\n",
        "\n",
        "class ProgressBar(object):\n",
        "    \"\"\"Cheers @ajratner\"\"\"\n",
        "\n",
        "    def __init__(self, n, length=40):\n",
        "        # Protect against division by zero\n",
        "        self.n      = max(1, n)\n",
        "        self.nf     = float(n)\n",
        "        self.length = length\n",
        "        # Precalculate the i values that should trigger a write operation\n",
        "        #self.ticks = set([round(i/100.0 * n) for i in range(101)])\n",
        "        #self.ticks.add(n-1)\n",
        "        self.ticks=range(n)\n",
        "        self.bar(0)\n",
        "\n",
        "    def bar(self, i, message=\"\"):\n",
        "        \"\"\"Assumes i ranges through [0, n-1]\"\"\"\n",
        "        if i in self.ticks:\n",
        "            b = int(np.ceil(((i+1) / self.nf) * self.length))\n",
        "            sys.stdout.write(\"\\r[{0}{1}] {2}%\\t{3}\".format(\n",
        "                \"=\"*b, \" \"*(self.length-b), int(100*((i+1) / self.nf)), message\n",
        "            ))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "    def close(self, message=\"\"):\n",
        "        # Move the bar to 100% before closing\n",
        "        self.bar(self.n-1)\n",
        "        sys.stdout.write(\"{0}\\n\\n\".format(message))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "\n",
        "\n",
        "def training_f(train_dataset, numEpochs, model_conv, criterionCNN, optimizer_conv,batch_size, validation_split=0):\n",
        "  \n",
        "  best_acc = 0\n",
        "  best_loss=0\n",
        "  best_epoca = 0\n",
        "  \n",
        "  best_model_wts = copy.deepcopy(model_conv.state_dict())\n",
        "\n",
        "  if validation_split:\n",
        "      train_size = int(len(train_dataset) * 0.8)\n",
        "      val_size = len(train_dataset) - train_size\n",
        "      train_dataset, val_set = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "      traingen = torch.utils.data.DataLoader(train_dataset, pin_memory=True, batch_size=batch_size, shuffle=True,num_workers=2)\n",
        "      valgen = torch.utils.data.DataLoader(val_set, pin_memory=True, batch_size=batch_size)\n",
        "  \n",
        "  else: traingen = torch.utils.data.DataLoader(train_dataset, pin_memory=True, batch_size=batch_size, shuffle=True,num_workers=2)\n",
        "      \n",
        "  for epochs in range(1,numEpochs + 1):\n",
        "    \n",
        "    print(\"Epoch {0} / {1}\".format(epochs, numEpochs))\n",
        "    log = OrderedDict()\n",
        "    modelLoss_train = 0.0\n",
        "    modelAcc_train = 0.0\n",
        "    \n",
        "    model_conv.train() \n",
        "    \n",
        "    totalSize = 0\n",
        "    batch_i=0\n",
        "    num_batch=int(np.ceil(len(train_dataset)/batch_size))\n",
        "    pb = ProgressBar(num_batch)\n",
        "    #for each batch: operazioni per gli algoritmi di ottimizzazione\n",
        "    for inputs,labels in traingen:\n",
        "      inputs = inputs.type(torch.FloatTensor).cuda()\n",
        "      labels = labels.cuda()\n",
        "      \n",
        "      batch_i=batch_i+1\n",
        "      \n",
        "      optimizer_conv.zero_grad()\n",
        "      model_conv.zero_grad()\n",
        "      \n",
        "      y = model_conv(inputs)\n",
        "      outp, preds = torch.max(y, 1)   \n",
        "        \n",
        "      lossCNN = criterionCNN(y, labels) #media per batch\n",
        "\n",
        "      modelLoss_train += lossCNN.item() * inputs.size(0)\n",
        "      totalSize += inputs.size(0)\n",
        "      modelAcc_train += torch.sum(preds == labels.data).item()\n",
        "\n",
        "      log['batch']=\"{0} / {1}\".format(batch_i, num_batch)\n",
        "      log['loss'] =round(modelLoss_train/totalSize,4)\n",
        "      log['acc']=round(modelAcc_train/totalSize,4)\n",
        "      log['Validation']=False\n",
        "      pb.bar(batch_i, log_to_message(log))\n",
        "\n",
        "      lossCNN.backward()  # pred = f(x)   -> loss = L(f(x), l_true)\n",
        "      \n",
        "      optimizer_conv.step()\n",
        "    \n",
        "    #calcolo loss e accuracy\n",
        "    modelLoss_epoch_train = modelLoss_train/totalSize\n",
        "    modelAcc_epoch_train  = modelAcc_train/totalSize\n",
        "    \n",
        "    log['loss']=round(modelLoss_epoch_train,2)\n",
        "    log['acc']=round(modelAcc_epoch_train*100,2)\n",
        "    pb.bar(num_batch-1, log_to_message(log))\n",
        "\n",
        "    if validation_split:    \n",
        "        #validation\n",
        "               \n",
        "        model_conv.eval()\n",
        "        totalSize_val = 0\n",
        "        modelLoss_val = 0.0\n",
        "        modelAcc_val = 0.0\n",
        "\n",
        "        log['Validation']=True\n",
        "        log['loss_val']=modelLoss_val\n",
        "        log['acc_val']=modelAcc_val\n",
        "        pb.bar(num_batch-1, log_to_message(log))\n",
        "        batch_val_i=0\n",
        "        size_valgen=len(valgen)\n",
        "        for inputs,labels in valgen:\n",
        "          inputs = inputs.type(torch.FloatTensor).cuda()\n",
        "          labels = labels.cuda()\n",
        "          batch_val_i+=1\n",
        "          y = model_conv(inputs)\n",
        "          outp, preds = torch.max(y, 1)\n",
        "          lossCNN = criterionCNN(y, labels)\n",
        "\n",
        "          modelLoss_val += lossCNN.item() * inputs.size(0)\n",
        "          totalSize_val += inputs.size(0)\n",
        "          modelAcc_val += torch.sum(preds == labels.data).item()        \n",
        "\n",
        "          log['batch_val']=\"{0} / {1}\".format(batch_val_i,size_valgen )\n",
        "          log['loss_val'] =round(modelLoss_val/totalSize_val,4)\n",
        "          log['acc_val']=round(modelAcc_val/totalSize_val,4)\n",
        "          pb.bar(num_batch-1, log_to_message(log))\n",
        "\n",
        "        modelLoss_epoch_val=modelLoss_val/totalSize_val\n",
        "        modelAcc_epoch_val = modelAcc_val/totalSize_val\n",
        "\n",
        "        log['loss_val'] =round(modelLoss_epoch_val,2)\n",
        "        log['acc_val']=round(modelAcc_epoch_val*100,2)\n",
        "        \n",
        "        if (modelAcc_epoch_val > best_acc) or (modelAcc_epoch_val == best_acc and modelLoss_epoch_val < best_loss) :\n",
        "          best_acc = modelAcc_epoch_val\n",
        "          best_loss = modelLoss_epoch_val\n",
        "          best_epoca = epochs\n",
        "          best_model_wts = copy.deepcopy(model_conv.state_dict()) #salvo come modello quello che mi ha restituito risultati migliori in validation\n",
        "          \n",
        "        pb.close(log_to_message(log))\n",
        "\n",
        "    else: \n",
        "      best_model_wts = copy.deepcopy(model_conv.state_dict())\n",
        "      pb.close(log_to_message(log))\n",
        "\n",
        "    with open('lossTrain.txt', \"a\") as file_object:\n",
        "      file_object.write(str(modelLoss_epoch_train) +'\\n')\n",
        "      \n",
        "    with open('AccTrain.txt', \"a\") as file_object:\n",
        "      file_object.write(str(modelAcc_epoch_train)+'\\n')\n",
        "      \n",
        "    with open('lossVal.txt', \"a\") as file_object:\n",
        "      file_object.write(str(modelLoss_epoch_val)+'\\n')\n",
        "      \n",
        "    with open('AccVal.txt', \"a\") as file_object:\n",
        "      file_object.write(str(modelAcc_epoch_val)+'\\n')\n",
        "\n",
        "  #pb.close(log_to_message(log))\n",
        "\n",
        "  model_conv.load_state_dict(best_model_wts)\n",
        "  return model_conv\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRhT4XM1sVm-"
      },
      "source": [
        "##Set iperparametri"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH1X3BB5sX6w"
      },
      "source": [
        "#iper-parametri iniziali\n",
        "learning_rate = 1e-6\n",
        "momentum=0.9\n",
        "num_epoch = 100\n",
        "batch_size = 5\n",
        "#richiamo funzione che effettua il training con validation\n",
        "\n",
        "model_conv = model_conv.cuda() #sposta i calcoli sulla gpu\n",
        "criterion = nn.CrossEntropyLoss() #criterio dell'aggiornamento del gradiente: minimizzazione funzione loss entropia\n",
        "optimizer_conv = optim.SGD(model_conv.classifier.parameters(),lr=learning_rate,momentum=momentum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44j4JhI0ti8T"
      },
      "source": [
        "##Addestramento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Cu1TY-Ttlgc",
        "outputId": "d6f57773-ef51-4c1c-80d7-191d01e04cce"
      },
      "source": [
        "best_model=training_f(train_dataset, num_epoch, model_conv, criterion, optimizer_conv,batch_size=batch_size, validation_split=0.2)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.59    acc: 68.23    Validation: True    loss_val: 0.48    acc_val: 82.0    batch_val: 800 / 800\n",
            "\n",
            "Epoch 2 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.46    acc: 80.17    Validation: True    loss_val: 0.41    acc_val: 85.58    batch_val: 800 / 800\n",
            "\n",
            "Epoch 3 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.41    acc: 83.12    Validation: True    loss_val: 0.36    acc_val: 86.48    batch_val: 800 / 800\n",
            "\n",
            "Epoch 4 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.37    acc: 84.86    Validation: True    loss_val: 0.33    acc_val: 87.85    batch_val: 800 / 800\n",
            "\n",
            "Epoch 5 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.35    acc: 85.95    Validation: True    loss_val: 0.31    acc_val: 88.0    batch_val: 800 / 800\n",
            "\n",
            "Epoch 6 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.32    acc: 86.94    Validation: True    loss_val: 0.29    acc_val: 89.25    batch_val: 800 / 800\n",
            "\n",
            "Epoch 7 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.31    acc: 87.81    Validation: True    loss_val: 0.28    acc_val: 89.72    batch_val: 800 / 800\n",
            "\n",
            "Epoch 8 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.3    acc: 87.57    Validation: True    loss_val: 0.27    acc_val: 89.95    batch_val: 800 / 800\n",
            "\n",
            "Epoch 9 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.29    acc: 88.28    Validation: True    loss_val: 0.26    acc_val: 90.28    batch_val: 800 / 800\n",
            "\n",
            "Epoch 10 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.28    acc: 89.09    Validation: True    loss_val: 0.25    acc_val: 90.45    batch_val: 800 / 800\n",
            "\n",
            "Epoch 11 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.27    acc: 88.95    Validation: True    loss_val: 0.24    acc_val: 90.9    batch_val: 800 / 800\n",
            "\n",
            "Epoch 12 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.26    acc: 89.41    Validation: True    loss_val: 0.23    acc_val: 91.2    batch_val: 800 / 800\n",
            "\n",
            "Epoch 13 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.25    acc: 89.74    Validation: True    loss_val: 0.22    acc_val: 91.42    batch_val: 800 / 800\n",
            "\n",
            "Epoch 14 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.25    acc: 89.89    Validation: True    loss_val: 0.22    acc_val: 91.55    batch_val: 800 / 800\n",
            "\n",
            "Epoch 15 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.24    acc: 90.15    Validation: True    loss_val: 0.21    acc_val: 91.7    batch_val: 800 / 800\n",
            "\n",
            "Epoch 16 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.24    acc: 90.35    Validation: True    loss_val: 0.21    acc_val: 91.88    batch_val: 800 / 800\n",
            "\n",
            "Epoch 17 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.23    acc: 90.34    Validation: True    loss_val: 0.2    acc_val: 92.0    batch_val: 800 / 800\n",
            "\n",
            "Epoch 18 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.23    acc: 90.68    Validation: True    loss_val: 0.2    acc_val: 92.38    batch_val: 800 / 800\n",
            "\n",
            "Epoch 19 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.22    acc: 90.89    Validation: True    loss_val: 0.2    acc_val: 92.25    batch_val: 800 / 800\n",
            "\n",
            "Epoch 20 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.22    acc: 90.97    Validation: True    loss_val: 0.19    acc_val: 92.4    batch_val: 800 / 800\n",
            "\n",
            "Epoch 21 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.22    acc: 91.18    Validation: True    loss_val: 0.19    acc_val: 92.75    batch_val: 800 / 800\n",
            "\n",
            "Epoch 22 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.21    acc: 91.32    Validation: True    loss_val: 0.19    acc_val: 92.83    batch_val: 800 / 800\n",
            "\n",
            "Epoch 23 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.21    acc: 91.47    Validation: True    loss_val: 0.18    acc_val: 93.0    batch_val: 800 / 800\n",
            "\n",
            "Epoch 24 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.21    acc: 91.71    Validation: True    loss_val: 0.18    acc_val: 93.2    batch_val: 800 / 800\n",
            "\n",
            "Epoch 25 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.2    acc: 92.09    Validation: True    loss_val: 0.18    acc_val: 93.33    batch_val: 800 / 800\n",
            "\n",
            "Epoch 26 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.2    acc: 91.97    Validation: True    loss_val: 0.17    acc_val: 93.42    batch_val: 800 / 800\n",
            "\n",
            "Epoch 27 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.2    acc: 92.19    Validation: True    loss_val: 0.17    acc_val: 93.45    batch_val: 800 / 800\n",
            "\n",
            "Epoch 28 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.2    acc: 91.94    Validation: True    loss_val: 0.17    acc_val: 93.53    batch_val: 800 / 800\n",
            "\n",
            "Epoch 29 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.19    acc: 92.27    Validation: True    loss_val: 0.17    acc_val: 93.67    batch_val: 800 / 800\n",
            "\n",
            "Epoch 30 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.19    acc: 92.51    Validation: True    loss_val: 0.16    acc_val: 93.8    batch_val: 800 / 800\n",
            "\n",
            "Epoch 31 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.19    acc: 92.51    Validation: True    loss_val: 0.16    acc_val: 93.9    batch_val: 800 / 800\n",
            "\n",
            "Epoch 32 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.19    acc: 92.66    Validation: True    loss_val: 0.16    acc_val: 93.95    batch_val: 800 / 800\n",
            "\n",
            "Epoch 33 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.18    acc: 92.79    Validation: True    loss_val: 0.16    acc_val: 94.05    batch_val: 800 / 800\n",
            "\n",
            "Epoch 34 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.18    acc: 92.71    Validation: True    loss_val: 0.16    acc_val: 94.05    batch_val: 800 / 800\n",
            "\n",
            "Epoch 35 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.18    acc: 92.91    Validation: True    loss_val: 0.15    acc_val: 94.17    batch_val: 800 / 800\n",
            "\n",
            "Epoch 36 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.18    acc: 92.76    Validation: True    loss_val: 0.15    acc_val: 94.27    batch_val: 800 / 800\n",
            "\n",
            "Epoch 37 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.17    acc: 93.14    Validation: True    loss_val: 0.15    acc_val: 94.35    batch_val: 800 / 800\n",
            "\n",
            "Epoch 38 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.17    acc: 93.25    Validation: True    loss_val: 0.15    acc_val: 94.45    batch_val: 800 / 800\n",
            "\n",
            "Epoch 39 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.17    acc: 93.06    Validation: True    loss_val: 0.15    acc_val: 94.5    batch_val: 800 / 800\n",
            "\n",
            "Epoch 40 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.17    acc: 93.39    Validation: True    loss_val: 0.14    acc_val: 94.53    batch_val: 800 / 800\n",
            "\n",
            "Epoch 41 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.17    acc: 93.07    Validation: True    loss_val: 0.14    acc_val: 94.73    batch_val: 800 / 800\n",
            "\n",
            "Epoch 42 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.17    acc: 93.42    Validation: True    loss_val: 0.14    acc_val: 94.67    batch_val: 800 / 800\n",
            "\n",
            "Epoch 43 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.16    acc: 93.68    Validation: True    loss_val: 0.14    acc_val: 94.75    batch_val: 800 / 800\n",
            "\n",
            "Epoch 44 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.16    acc: 93.54    Validation: True    loss_val: 0.14    acc_val: 95.08    batch_val: 800 / 800\n",
            "\n",
            "Epoch 45 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.16    acc: 93.65    Validation: True    loss_val: 0.14    acc_val: 94.97    batch_val: 800 / 800\n",
            "\n",
            "Epoch 46 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.16    acc: 93.38    Validation: True    loss_val: 0.14    acc_val: 95.1    batch_val: 800 / 800\n",
            "\n",
            "Epoch 47 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.16    acc: 94.11    Validation: True    loss_val: 0.14    acc_val: 95.15    batch_val: 800 / 800\n",
            "\n",
            "Epoch 48 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.16    acc: 93.74    Validation: True    loss_val: 0.13    acc_val: 95.2    batch_val: 800 / 800\n",
            "\n",
            "Epoch 49 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.16    acc: 93.84    Validation: True    loss_val: 0.13    acc_val: 95.3    batch_val: 800 / 800\n",
            "\n",
            "Epoch 50 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.16    acc: 93.83    Validation: True    loss_val: 0.13    acc_val: 95.3    batch_val: 800 / 800\n",
            "\n",
            "Epoch 51 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.15    acc: 94.13    Validation: True    loss_val: 0.13    acc_val: 95.35    batch_val: 800 / 800\n",
            "\n",
            "Epoch 52 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.15    acc: 94.21    Validation: True    loss_val: 0.13    acc_val: 95.28    batch_val: 800 / 800\n",
            "\n",
            "Epoch 53 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.15    acc: 93.88    Validation: True    loss_val: 0.13    acc_val: 95.35    batch_val: 800 / 800\n",
            "\n",
            "Epoch 54 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.15    acc: 94.09    Validation: True    loss_val: 0.13    acc_val: 95.4    batch_val: 800 / 800\n",
            "\n",
            "Epoch 55 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.15    acc: 94.09    Validation: True    loss_val: 0.13    acc_val: 95.4    batch_val: 800 / 800\n",
            "\n",
            "Epoch 56 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.15    acc: 94.34    Validation: True    loss_val: 0.12    acc_val: 95.47    batch_val: 800 / 800\n",
            "\n",
            "Epoch 57 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.14    acc: 94.27    Validation: True    loss_val: 0.12    acc_val: 95.43    batch_val: 800 / 800\n",
            "\n",
            "Epoch 58 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.15    acc: 94.24    Validation: True    loss_val: 0.12    acc_val: 95.38    batch_val: 800 / 800\n",
            "\n",
            "Epoch 59 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.14    acc: 94.54    Validation: True    loss_val: 0.12    acc_val: 95.45    batch_val: 800 / 800\n",
            "\n",
            "Epoch 60 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.14    acc: 94.42    Validation: True    loss_val: 0.12    acc_val: 95.47    batch_val: 800 / 800\n",
            "\n",
            "Epoch 61 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.14    acc: 94.28    Validation: True    loss_val: 0.12    acc_val: 95.53    batch_val: 800 / 800\n",
            "\n",
            "Epoch 62 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.14    acc: 94.4    Validation: True    loss_val: 0.12    acc_val: 95.6    batch_val: 800 / 800\n",
            "\n",
            "Epoch 63 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.14    acc: 94.56    Validation: True    loss_val: 0.12    acc_val: 95.58    batch_val: 800 / 800\n",
            "\n",
            "Epoch 64 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.14    acc: 94.62    Validation: True    loss_val: 0.12    acc_val: 95.65    batch_val: 800 / 800\n",
            "\n",
            "Epoch 65 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.13    acc: 94.71    Validation: True    loss_val: 0.12    acc_val: 95.62    batch_val: 800 / 800\n",
            "\n",
            "Epoch 66 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.13    acc: 94.87    Validation: True    loss_val: 0.12    acc_val: 95.67    batch_val: 800 / 800\n",
            "\n",
            "Epoch 67 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.13    acc: 94.82    Validation: True    loss_val: 0.12    acc_val: 95.7    batch_val: 800 / 800\n",
            "\n",
            "Epoch 68 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.13    acc: 94.75    Validation: True    loss_val: 0.11    acc_val: 95.73    batch_val: 800 / 800\n",
            "\n",
            "Epoch 69 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.13    acc: 94.66    Validation: True    loss_val: 0.11    acc_val: 95.73    batch_val: 800 / 800\n",
            "\n",
            "Epoch 70 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.13    acc: 94.85    Validation: True    loss_val: 0.11    acc_val: 95.8    batch_val: 800 / 800\n",
            "\n",
            "Epoch 71 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.13    acc: 94.98    Validation: True    loss_val: 0.11    acc_val: 95.78    batch_val: 800 / 800\n",
            "\n",
            "Epoch 72 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.13    acc: 94.73    Validation: True    loss_val: 0.11    acc_val: 95.83    batch_val: 800 / 800\n",
            "\n",
            "Epoch 73 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.13    acc: 95.09    Validation: True    loss_val: 0.11    acc_val: 95.9    batch_val: 800 / 800\n",
            "\n",
            "Epoch 74 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.13    acc: 95.08    Validation: True    loss_val: 0.11    acc_val: 95.83    batch_val: 800 / 800\n",
            "\n",
            "Epoch 75 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.13    acc: 95.06    Validation: True    loss_val: 0.11    acc_val: 95.85    batch_val: 800 / 800\n",
            "\n",
            "Epoch 76 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.13    acc: 95.19    Validation: True    loss_val: 0.11    acc_val: 95.93    batch_val: 800 / 800\n",
            "\n",
            "Epoch 77 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.13    acc: 94.98    Validation: True    loss_val: 0.11    acc_val: 95.93    batch_val: 800 / 800\n",
            "\n",
            "Epoch 78 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.12    acc: 95.08    Validation: True    loss_val: 0.11    acc_val: 95.95    batch_val: 800 / 800\n",
            "\n",
            "Epoch 79 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.12    acc: 95.32    Validation: True    loss_val: 0.11    acc_val: 95.97    batch_val: 800 / 800\n",
            "\n",
            "Epoch 80 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.12    acc: 95.18    Validation: True    loss_val: 0.11    acc_val: 96.0    batch_val: 800 / 800\n",
            "\n",
            "Epoch 81 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.12    acc: 95.15    Validation: True    loss_val: 0.1    acc_val: 96.08    batch_val: 800 / 800\n",
            "\n",
            "Epoch 82 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.12    acc: 95.14    Validation: True    loss_val: 0.1    acc_val: 96.03    batch_val: 800 / 800\n",
            "\n",
            "Epoch 83 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.12    acc: 95.33    Validation: True    loss_val: 0.1    acc_val: 96.03    batch_val: 800 / 800\n",
            "\n",
            "Epoch 84 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.12    acc: 95.47    Validation: True    loss_val: 0.1    acc_val: 96.15    batch_val: 800 / 800\n",
            "\n",
            "Epoch 85 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.12    acc: 95.34    Validation: True    loss_val: 0.1    acc_val: 96.1    batch_val: 800 / 800\n",
            "\n",
            "Epoch 86 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.12    acc: 95.37    Validation: True    loss_val: 0.1    acc_val: 96.17    batch_val: 800 / 800\n",
            "\n",
            "Epoch 87 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.12    acc: 95.45    Validation: True    loss_val: 0.1    acc_val: 96.17    batch_val: 800 / 800\n",
            "\n",
            "Epoch 88 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.12    acc: 95.64    Validation: True    loss_val: 0.1    acc_val: 96.12    batch_val: 800 / 800\n",
            "\n",
            "Epoch 89 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.11    acc: 95.64    Validation: True    loss_val: 0.1    acc_val: 96.25    batch_val: 800 / 800\n",
            "\n",
            "Epoch 90 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.12    acc: 95.62    Validation: True    loss_val: 0.1    acc_val: 96.23    batch_val: 800 / 800\n",
            "\n",
            "Epoch 91 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.12    acc: 95.51    Validation: True    loss_val: 0.1    acc_val: 96.23    batch_val: 800 / 800\n",
            "\n",
            "Epoch 92 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.12    acc: 95.58    Validation: True    loss_val: 0.1    acc_val: 96.38    batch_val: 800 / 800\n",
            "\n",
            "Epoch 93 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.11    acc: 95.53    Validation: True    loss_val: 0.1    acc_val: 96.4    batch_val: 800 / 800\n",
            "\n",
            "Epoch 94 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.11    acc: 95.8    Validation: True    loss_val: 0.1    acc_val: 96.47    batch_val: 800 / 800\n",
            "\n",
            "Epoch 95 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.11    acc: 95.67    Validation: True    loss_val: 0.1    acc_val: 96.43    batch_val: 800 / 800\n",
            "\n",
            "Epoch 96 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.11    acc: 95.69    Validation: True    loss_val: 0.1    acc_val: 96.38    batch_val: 800 / 800\n",
            "\n",
            "Epoch 97 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.11    acc: 95.77    Validation: True    loss_val: 0.1    acc_val: 96.53    batch_val: 800 / 800\n",
            "\n",
            "Epoch 98 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.11    acc: 95.76    Validation: True    loss_val: 0.1    acc_val: 96.58    batch_val: 800 / 800\n",
            "\n",
            "Epoch 99 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.11    acc: 95.69    Validation: True    loss_val: 0.1    acc_val: 96.55    batch_val: 800 / 800\n",
            "\n",
            "Epoch 100 / 100\n",
            "[========================================] 100%\tbatch: 3200 / 3200    loss: 0.11    acc: 95.88    Validation: True    loss_val: 0.09    acc_val: 96.58    batch_val: 800 / 800\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYevfAg2j7Vs",
        "outputId": "69c5e148-6c21-46a7-d5e9-ef23194ad29c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(best_model)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): ReLU(inplace=True)\n",
            "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (24): ReLU(inplace=True)\n",
            "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): ReLU(inplace=True)\n",
            "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): ReLU(inplace=True)\n",
            "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAU1QYvhrGVs"
      },
      "source": [
        "#**Testing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmnQizLIrQVJ"
      },
      "source": [
        "##Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzduG7-irULM"
      },
      "source": [
        "def calc_size(n):\n",
        "  return tuple(int(np.ceil(i * (80/100))) for i in n)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBtPmfqdrIg8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfb91c39-fcaf-49c2-a1cc-11b97f597068"
      },
      "source": [
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "#best_model=model_conv\n",
        "Test = pd.DataFrame()\n",
        "best_model.eval()\n",
        "best_model.cuda()\n",
        "\n",
        "data_transform_test= transforms.Compose([transforms.Resize([224,224]),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "Directory = os.listdir(pathTestset)\n",
        "for classe in Directory:\n",
        "  classes_path = os.listdir(pathTestset +\"/\"+ classe)  \n",
        "  print(classe)\n",
        "  for input in classes_path:\n",
        "    if (input.endswith('.png') or input.endswith('.bmp')):  \n",
        "      img = Image.open(pathTestset+\"/\"+classe+'/' + input)    \n",
        "      #img = img.convert('RGB')  per immagini bmp, le legge in bianco e nero\n",
        "      n=img.size\n",
        "      n_mod=calc_size(n)\n",
        "      crop_transform=transforms.TenCrop((n_mod[1],n_mod[0])).to(device)\n",
        "      crops=crop_transform(img)\n",
        "      live=0\n",
        "      spoof=0\n",
        "      for crop in crops:\n",
        "        crop=data_transform_test(crop).to(device)\n",
        "        crop=crop.unsqueeze_(0)\n",
        "        outputs = best_model(crop)\n",
        "        live+=outputs[0][0]\n",
        "        spoof+=outputs[0][1]\n",
        "      live=live/10\n",
        "      spoof=spoof/10\n",
        "      predicted=np.argmax([live,spoof])\n",
        "      Test = Test.append({'img': input.split('.')[0],\n",
        "                          'predicted': classes_name[predicted],\n",
        "                          'real': classe} ,ignore_index = True) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tCHClGZsZAv"
      },
      "source": [
        "true_label = Test.real.values\n",
        "predicted = Test.predicted.values\n",
        "\n",
        "print(round((np.sum((true_label == predicted).astype(int)))/Test.shape[0],4)*100)\n",
        "#84,8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7V5_4G5OsXkP"
      },
      "source": [
        "print(Test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFFWkmjX2j5P"
      },
      "source": [
        "#**Salvataggio modello**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAie77UO2nTs"
      },
      "source": [
        "#salva modello su drive\n",
        "torch.save(best_model.state_dict(),path_model_save)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DpXrJh9Lhc5"
      },
      "source": [
        "#Plot delle curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InbGX9NfLjhN"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "lossModel_Train = []\n",
        "lossModel_val = []\n",
        "accModel_Train = []\n",
        "accModel_val = []\n",
        "\n",
        "file = open('lossTrain.txt', 'r')\n",
        "Testo = file.readlines()\n",
        "for element in Testo:\n",
        "  lossModel_Train.append(float(element))\n",
        "\n",
        "file = open('lossVal.txt', 'r')\n",
        "Testo = file.readlines()\n",
        "for element in Testo:\n",
        "  lossModel_val.append(float(element))\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Model: Training Vs Validation Losses\")\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(list(range(1,len(lossModel_Train)+1)), lossModel_Train, color='r', label=\"Training Loss\")\n",
        "plt.plot(list(range(1, len(lossModel_val)+1)), lossModel_val, color='g', label=\"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.savefig('LossTrainVal.png')\n",
        "\n",
        "file = open('AccTrain.txt', 'r')\n",
        "Testo = file.readlines()\n",
        "for element in Testo:\n",
        "  accModel_Train.append(float(element))\n",
        "\n",
        "file = open('AccVal.txt', 'r')\n",
        "Testo = file.readlines()\n",
        "for element in Testo:\n",
        "  accModel_val.append(float(element))\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Training Vs Validation Accuracies\")\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(list(range(1, len(accModel_Train)+1)), accModel_Train, color='r', label=\"Training Accuracy\")\n",
        "plt.plot(list(range(1, len(accModel_val)+1)), accModel_val, color='g', label=\"Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.savefig('AccTrainVal.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz-V7H3Rey7d"
      },
      "source": [
        "#Mat conf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZsC6u75e0KE"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "\n",
        "c = confusion_matrix(true_label,predicted, labels=['Live','Spoof'])\n",
        "df_cm = pd.DataFrame(c , index = [i for i in list(range(1,3))],\n",
        "                  columns = [i for i in list(range(1,3))])\n",
        "plt.figure(figsize = (2,2))\n",
        "sn.heatmap(df_cm, annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}