{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attacchi_enhancement.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lcoLiAGAtqBh",
        "OHENSlCXxzcr",
        "nGmEBN4oxqUO",
        "pIqUXUmoA541",
        "-ycpjmyb4Cxe",
        "dherpFOH0Uau",
        "pV0cZLAzphdp",
        "5X10jetEyAax",
        "fETy1V2abbWo",
        "_miDmRukbbWq",
        "ZazQQE8ypKP9",
        "vT9l9KWwv1uu",
        "AJ_O-UAECNMk",
        "9XepXK-ny3XN",
        "PBSyijWDqY4-",
        "O_c2L3v0r2xG",
        "shAnhrP3JZfJ",
        "9_GPV-52IRYV",
        "VALnI8jHIVU9",
        "0OdO07YuUOFS",
        "Q64Z8X5mzAZ1",
        "JmyIKi7XvvSf",
        "kJ6xDO5GGZY5",
        "Sm63-OLUwGE-",
        "w_qc1J0y4Ucu",
        "DWxzZ1rGd5LY",
        "55yuMaawbgyt",
        "OfEnYI_UUEb6",
        "OzZkC7HimlHT",
        "ZFKEoI-enb8L"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "647c2d5d66fd4698861c5f5ee3c09ca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4d8a4b6735ae4f028b089403ba88fbc2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2b60e5821e594607a5c940e1abfb9846",
              "IPY_MODEL_3b2f35b5ef6a4b1694b781e4ad071212",
              "IPY_MODEL_96b052ec7ae846a6b073e4b23188c144"
            ]
          }
        },
        "4d8a4b6735ae4f028b089403ba88fbc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2b60e5821e594607a5c940e1abfb9846": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_919679097ac348fcbfee0ea86f872961",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d8840293c79f4dc39a22d7933bd7d027"
          }
        },
        "3b2f35b5ef6a4b1694b781e4ad071212": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f2987b9effc5471f814ecbf0201418ae",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 574673361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 574673361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c7d4b53ac35e491984e8aeb4a5c84128"
          }
        },
        "96b052ec7ae846a6b073e4b23188c144": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9e98f039fcbd4c84b3bb240ccdec7a2c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 548M/548M [00:03&lt;00:00, 168MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c4774a0ee3494135a715c61558849283"
          }
        },
        "919679097ac348fcbfee0ea86f872961": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d8840293c79f4dc39a22d7933bd7d027": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f2987b9effc5471f814ecbf0201418ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c7d4b53ac35e491984e8aeb4a5c84128": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9e98f039fcbd4c84b3bb240ccdec7a2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c4774a0ee3494135a715c61558849283": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umbertogagl97/Tesi/blob/main/Impronte/AttaccoScanner/Attacchi_enhancement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcoLiAGAtqBh"
      },
      "source": [
        "# **Init**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHENSlCXxzcr"
      },
      "source": [
        "##Import ART"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIKN5Oqa-i6u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74e2f8a8-e8d3-4903-fd5b-fd3deacf5974"
      },
      "source": [
        "#importa ART\n",
        "!pip install adversarial-robustness-toolbox"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting adversarial-robustness-toolbox\n",
            "  Downloading adversarial_robustness_toolbox-1.8.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 34.8 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 39.4 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 24.9 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 92 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 122 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 153 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 184 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 204 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 215 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 235 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 245 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 266 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 296 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 307 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 317 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 327 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 337 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 358 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 368 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 378 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 389 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 399 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 409 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 419 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 430 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 440 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 460 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 471 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 481 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 491 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 501 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 512 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 522 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 532 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 542 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 552 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 563 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 573 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 583 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 593 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 604 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 614 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 634 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 645 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 655 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 665 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 675 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 686 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 696 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 706 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 716 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 727 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 737 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 747 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 757 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 768 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 778 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 788 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 798 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 808 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 819 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 829 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 839 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 849 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 860 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 870 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 880 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 890 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 901 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 911 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 921 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 931 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 942 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 952 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 962 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 972 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 983 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 993 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0 MB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0 MB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0 MB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0 MB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.0 MB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1 MB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1 MB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1 MB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.1 MB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1 MB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 11.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn<1.1.0,>=0.22.2 in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox) (1.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox) (4.62.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox) (57.4.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox) (1.4.1)\n",
            "Collecting numba>=0.53.1\n",
            "  Downloading numba-0.54.1-cp37-cp37m-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 34.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox) (1.19.5)\n",
            "Collecting llvmlite<0.38,>=0.37.0rc1\n",
            "  Downloading llvmlite-0.37.0-cp37-cp37m-manylinux2014_x86_64.whl (26.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.3 MB 44.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1.1.0,>=0.22.2->adversarial-robustness-toolbox) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1.1.0,>=0.22.2->adversarial-robustness-toolbox) (3.0.0)\n",
            "Installing collected packages: llvmlite, numba, adversarial-robustness-toolbox\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "Successfully installed adversarial-robustness-toolbox-1.8.1 llvmlite-0.37.0 numba-0.54.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGmEBN4oxqUO"
      },
      "source": [
        "## Import librerie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01RXI-DDIb3C"
      },
      "source": [
        "#Librerie\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "#import time\n",
        "#import os\n",
        "#import shutil\n",
        "#import copy\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from typing import Optional"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIqUXUmoA541"
      },
      "source": [
        "##Check device\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2pe5Zh2A4Ui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21500472-ede9-47bb-89bf-75a7a470e418"
      },
      "source": [
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.10.0+cu111\n",
            "True\n",
            "Tesla P100-PCIE-16GB\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ycpjmyb4Cxe"
      },
      "source": [
        "##Def path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hMuySVI4GgY"
      },
      "source": [
        "#scanner\n",
        "scanner_name = 'GreenBit'\n",
        "\n",
        "#load modello\n",
        "path_model = F\"/content/gdrive/My Drive/Scanner/{scanner_name}\" \n",
        "\n",
        "#dataset\n",
        "pathTestset=F'/content/gdrive/MyDrive/Dataset_impronte/test/{scanner_name}'\n",
        "\n",
        "#predizioni testset originale\n",
        "pd_preds=F'/content/gdrive/MyDrive/Dataset_impronte/test/Preds_value/{scanner_name}_con_normalize.xlsx'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dherpFOH0Uau"
      },
      "source": [
        "##Collegamento google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyvTOQw-aHRP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "def0ecf0-39b9-4a42-9cb3-5a55e1690ee7"
      },
      "source": [
        "#collegamento google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV0cZLAzphdp"
      },
      "source": [
        "##Transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOiUbDZ5pixj"
      },
      "source": [
        "#transf_init=transforms.Resize(size=(1000,1000),interpolation=InterpolationMode.NEAREST)\n",
        "\n",
        "data_transform_test= transforms.Compose([transforms.Resize([224,224],interpolation=InterpolationMode.NEAREST),\n",
        "          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "          ])\n",
        "transf_resize=transforms.Resize([224,224],interpolation=InterpolationMode.NEAREST)\n",
        "transf_load= transforms.Compose([transforms.ToTensor(),\n",
        "                                 #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "          ])\n",
        "trans_norm=transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X10jetEyAax"
      },
      "source": [
        "#**Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fETy1V2abbWo"
      },
      "source": [
        "##Caricamento dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMW6yAMPbbWp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07838cf3-10e0-4a5b-a20b-7571f33ab259"
      },
      "source": [
        "test_dataset = datasets.ImageFolder(pathTestset,transform=transf_load)\n",
        "dim_set=len(test_dataset)\n",
        "print(dim_set)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_miDmRukbbWq"
      },
      "source": [
        "##Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3JB4rWJbbWq"
      },
      "source": [
        "testgen=torch.utils.data.DataLoader(test_dataset, pin_memory=True, batch_size=1,num_workers=2)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZazQQE8ypKP9"
      },
      "source": [
        "##Nomi classi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYk4cr4eo9Ir",
        "outputId": "f1666c40-6fee-4710-e645-caeb8016f8e6"
      },
      "source": [
        "classes_name=test_dataset.classes\n",
        "class_number=len(classes_name)\n",
        "print(classes_name)\n",
        "print(class_number)\n",
        "#del test_dataset"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Live', 'Spoof']\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT9l9KWwv1uu"
      },
      "source": [
        "##Min Max dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9FPsg0aE6xI"
      },
      "source": [
        "_min,_max=0,1"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNR3oeYXPvTM"
      },
      "source": [
        "##transf_init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1Os3S2xPwn1"
      },
      "source": [
        "a,_=next(iter(testgen))\n",
        "size_init=np.array(a.shape[2:4])\n",
        "transf_init=transforms.Resize(size=(size_init[0],size_init[1]),interpolation=InterpolationMode.NEAREST)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ_O-UAECNMk"
      },
      "source": [
        "#**Riduzione dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnOEmgNRbbWp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f33d925-9ed9-49d5-bb08-41192ad7fd34"
      },
      "source": [
        "test_dataset.samples=test_dataset.samples[1000:]\n",
        "\n",
        "dim_set=len(test_dataset)\n",
        "print(dim_set)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnOBh-Eap-SF",
        "outputId": "c54317c4-7f69-4fc0-d936-517a1d6e7b54"
      },
      "source": [
        "test_dataset.samples=[test_dataset.samples[i] for i in range(len(test_dataset)) if 'Latex' in test_dataset.samples[i][0]]\n",
        "\n",
        "dim_set=len(test_dataset)\n",
        "print(dim_set)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "dir=os.listdir('/content/gdrive/MyDrive/HiScan_attacks/APGD_final_latex')\n",
        "test_dataset.samples=[test_dataset.samples[i] for i in range(len(test_dataset)) if test_dataset.samples[i][0].split('Spoof/')[1] in dir]\n",
        "dim_set=len(test_dataset)\n",
        "print(dim_set)"
      ],
      "metadata": {
        "id": "MQu2nJZ0NvCz",
        "outputId": "54081e12-ac19-4e55-b2ec-d9562fc81501",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtSM4AeYqE1i"
      },
      "source": [
        "eseguire solo se si vuole provare con poche img"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJkn8UPzqDlq",
        "outputId": "5ac651af-6b71-4345-99fc-d838441af82c"
      },
      "source": [
        "test_dataset.samples=test_dataset.samples[:3]\n",
        "\n",
        "dim_set=len(test_dataset)\n",
        "print(dim_set)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XepXK-ny3XN"
      },
      "source": [
        "# **Creazione modello**\n",
        "\n",
        "> non ho bloccato i parametri inferiori e sbloccato quelli del classificatore, vedi se funziona\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBSyijWDqY4-"
      },
      "source": [
        "##Load model pre-trained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GymaBXhaPMvL",
        "outputId": "5d339fde-efc2-4bf0-a7db-430b2817b5b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "647c2d5d66fd4698861c5f5ee3c09ca0",
            "4d8a4b6735ae4f028b089403ba88fbc2",
            "2b60e5821e594607a5c940e1abfb9846",
            "3b2f35b5ef6a4b1694b781e4ad071212",
            "96b052ec7ae846a6b073e4b23188c144",
            "919679097ac348fcbfee0ea86f872961",
            "d8840293c79f4dc39a22d7933bd7d027",
            "f2987b9effc5471f814ecbf0201418ae",
            "c7d4b53ac35e491984e8aeb4a5c84128",
            "9e98f039fcbd4c84b3bb240ccdec7a2c",
            "c4774a0ee3494135a715c61558849283"
          ]
        }
      },
      "source": [
        "if scanner_name=='DigitalPersona':\n",
        "  model = models.densenet201(pretrained=True,progress=True)\n",
        "else: model = models.vgg19(pretrained=True,progress=True)\n",
        "\n",
        "#print(model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "647c2d5d66fd4698861c5f5ee3c09ca0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_c2L3v0r2xG"
      },
      "source": [
        "##Aggiunta classificatore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1qeI3_3qKJJ"
      },
      "source": [
        "if scanner_name=='DigitalPersona':\n",
        "  model.classifier=nn.Linear(1920,2) #per densenet201\n",
        "else: model.classifier[6]=nn.Linear(4096,2) #per vgg19\n",
        "\n",
        "\n",
        "#print(model) "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOHqOuTVAMaz"
      },
      "source": [
        "##Load pesi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEXv4tQ3AOi0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26776b4e-2f71-4a18-ebbe-516681bcfd7c"
      },
      "source": [
        "model.load_state_dict(torch.load(path_model))#,map_location=torch.device('cpu')))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3kr1yAn2UBl"
      },
      "source": [
        "# **Def funzioni**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shAnhrP3JZfJ"
      },
      "source": [
        "##enhanc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_GPV-52IRYV"
      },
      "source": [
        "###fun"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZuOAgLRIp6i"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Nov 4 19:46:32 2020\n",
        "\n",
        "@author: utkarsh\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from scipy import signal\n",
        "from scipy import ndimage\n",
        "import math\n",
        "import scipy\n",
        "\n",
        "class FingerprintImageEnhancer(object):\n",
        "    def __init__(self, ridge_segment_blksze=32, ridge_segment_thresh=0.1, gradient_sigma=1, block_sigma=7, orient_smooth_sigma=7,\n",
        "                 ridge_freq_blksze=38, ridge_freq_windsze=5, min_wave_length=7, max_wave_length=15, kx=0.65, ky=0.65, angleInc=3.0, ridge_filter_thresh=-3):\n",
        "        self.ridge_segment_blksze = ridge_segment_blksze\n",
        "        self.ridge_segment_thresh = ridge_segment_thresh\n",
        "        self.gradient_sigma = gradient_sigma\n",
        "        self.block_sigma = block_sigma\n",
        "        self.orient_smooth_sigma = orient_smooth_sigma\n",
        "        self.ridge_freq_blksze = ridge_freq_blksze\n",
        "        self.ridge_freq_windsze = ridge_freq_windsze\n",
        "        self.min_wave_length = min_wave_length\n",
        "        self.max_wave_length = max_wave_length\n",
        "        self.kx = kx\n",
        "        self.ky = ky\n",
        "        self.angleInc = angleInc\n",
        "        self.ridge_filter_thresh = ridge_filter_thresh\n",
        "\n",
        "\n",
        "        self._mask = []\n",
        "        self._normim = []\n",
        "        self._orientim = []\n",
        "        self._mean_freq = []\n",
        "        self._median_freq = []\n",
        "        self._freq = []\n",
        "        self._freqim = []\n",
        "        self._binim = []\n",
        "\n",
        "    def __normalise(self, img, mean, std):\n",
        "        normed = (img - np.mean(img)) / (np.std(img))\n",
        "        return (normed)\n",
        "\n",
        "    def __ridge_segment(self, img):\n",
        "        # RIDGESEGMENT - Normalises fingerprint image and segments ridge region\n",
        "        #\n",
        "        # Function identifies ridge regions of a fingerprint image and returns a\n",
        "        # mask identifying this region.  It also normalises the intesity values of\n",
        "        # the image so that the ridge regions have zero mean, unit standard\n",
        "        # deviation.\n",
        "        #\n",
        "        # This function breaks the image up into blocks of size blksze x blksze and\n",
        "        # evaluates the standard deviation in each region.  If the standard\n",
        "        # deviation is above the threshold it is deemed part of the fingerprint.\n",
        "        # Note that the image is normalised to have zero mean, unit standard\n",
        "        # deviation prior to performing this process so that the threshold you\n",
        "        # specify is relative to a unit standard deviation.\n",
        "        #\n",
        "        # Usage:   [normim, mask, maskind] = ridgesegment(im, blksze, thresh)\n",
        "        #\n",
        "        # Arguments:   im     - Fingerprint image to be segmented.\n",
        "        #              blksze - Block size over which the the standard\n",
        "        #                       deviation is determined (try a value of 16).\n",
        "        #              thresh - Threshold of standard deviation to decide if a\n",
        "        #                       block is a ridge region (Try a value 0.1 - 0.2)\n",
        "        #\n",
        "        # Ouput:     normim - Image where the ridge regions are renormalised to\n",
        "        #                       have zero mean, unit standard deviation.\n",
        "        #              mask   - Mask indicating ridge-like regions of the image,\n",
        "        #                       0 for non ridge regions, 1 for ridge regions.\n",
        "        #              maskind - Vector of indices of locations within the mask.\n",
        "        #\n",
        "        # Suggested values for a 500dpi fingerprint image:\n",
        "        #\n",
        "        #   [normim, mask, maskind] = ridgesegment(im, 16, 0.1)\n",
        "        #\n",
        "        # See also: RIDGEORIENT, RIDGEFREQ, RIDGEFILTER\n",
        "\n",
        "        ### REFERENCES\n",
        "\n",
        "        # Peter Kovesi\n",
        "        # School of Computer Science & Software Engineering\n",
        "        # The University of Western Australia\n",
        "        # pk at csse uwa edu au\n",
        "        # http://www.csse.uwa.edu.au/~pk\n",
        "        rows, cols = img.shape\n",
        "\n",
        "        im = self.__normalise(img, 0, 1)  # normalise to get zero mean and unit standard deviation\n",
        "\n",
        "        new_rows = np.int(self.ridge_segment_blksze * np.ceil((np.float(rows)) / (np.float(self.ridge_segment_blksze))))\n",
        "        new_cols = np.int(self.ridge_segment_blksze * np.ceil((np.float(cols)) / (np.float(self.ridge_segment_blksze))))\n",
        "\n",
        "        padded_img = np.zeros((new_rows, new_cols))\n",
        "        stddevim = np.zeros((new_rows, new_cols))\n",
        "        padded_img[0:rows][:, 0:cols] = im\n",
        "        for i in range(0, new_rows, self.ridge_segment_blksze):\n",
        "            for j in range(0, new_cols, self.ridge_segment_blksze):\n",
        "                block = padded_img[i:i + self.ridge_segment_blksze][:, j:j + self.ridge_segment_blksze]\n",
        "\n",
        "                stddevim[i:i + self.ridge_segment_blksze][:, j:j + self.ridge_segment_blksze] = np.std(block) * np.ones(block.shape)\n",
        "\n",
        "        stddevim = stddevim[0:rows][:, 0:cols]\n",
        "        self._mask = stddevim > self.ridge_segment_thresh\n",
        "        mean_val = np.mean(im[self._mask])\n",
        "        std_val = np.std(im[self._mask])\n",
        "        self._normim = (im - mean_val) / (std_val)\n",
        "\n",
        "    def __ridge_orient(self):\n",
        "        # RIDGEORIENT - Estimates the local orientation of ridges in a fingerprint\n",
        "        #\n",
        "        # Usage:  [orientim, reliability, coherence] = ridgeorientation(im, gradientsigma,...\n",
        "        #                                             blocksigma, ...\n",
        "        #                                             orientsmoothsigma)\n",
        "        #\n",
        "        # Arguments:  im                - A normalised input image.\n",
        "        #             gradientsigma     - Sigma of the derivative of Gaussian\n",
        "        #                                 used to compute image gradients.\n",
        "        #             blocksigma        - Sigma of the Gaussian weighting used to\n",
        "        #                                 sum the gradient moments.\n",
        "        #             orientsmoothsigma - Sigma of the Gaussian used to smooth\n",
        "        #                                 the final orientation vector field.\n",
        "        #                                 Optional: if ommitted it defaults to 0\n",
        "        #\n",
        "        # Output:    orientim          - The orientation image in radians.\n",
        "        #                                 Orientation values are +ve clockwise\n",
        "        #                                 and give the direction *along* the\n",
        "        #                                 ridges.\n",
        "        #             reliability       - Measure of the reliability of the\n",
        "        #                                 orientation measure.  This is a value\n",
        "        #                                 between 0 and 1. I think a value above\n",
        "        #                                 about 0.5 can be considered 'reliable'.\n",
        "        #                                 reliability = 1 - Imin./(Imax+.001);\n",
        "        #             coherence         - A measure of the degree to which the local\n",
        "        #                                 area is oriented.\n",
        "        #                                 coherence = ((Imax-Imin)./(Imax+Imin)).^2;\n",
        "        #\n",
        "        # With a fingerprint image at a 'standard' resolution of 500dpi suggested\n",
        "        # parameter values might be:\n",
        "        #\n",
        "        #    [orientim, reliability] = ridgeorient(im, 1, 3, 3);\n",
        "        #\n",
        "        # See also: RIDGESEGMENT, RIDGEFREQ, RIDGEFILTER\n",
        "\n",
        "        ### REFERENCES\n",
        "\n",
        "        # May 2003      Original version by Raymond Thai,\n",
        "        # January 2005  Reworked by Peter Kovesi\n",
        "        # October 2011  Added coherence computation and orientsmoothsigma made optional\n",
        "        #\n",
        "        # School of Computer Science & Software Engineering\n",
        "        # The University of Western Australia\n",
        "        # pk at csse uwa edu au\n",
        "        # http://www.csse.uwa.edu.au/~pk\n",
        "\n",
        "        rows,cols = self._normim.shape\n",
        "        #Calculate image gradients.\n",
        "        sze = np.fix(6*self.gradient_sigma)\n",
        "        if np.remainder(sze,2) == 0:\n",
        "            sze = sze+1\n",
        "\n",
        "        gauss = cv2.getGaussianKernel(np.int(sze),self.gradient_sigma)\n",
        "        f = gauss * gauss.T\n",
        "\n",
        "        fy,fx = np.gradient(f)                               #Gradient of Gaussian\n",
        "\n",
        "        Gx = signal.convolve2d(self._normim, fx, mode='same')\n",
        "        Gy = signal.convolve2d(self._normim, fy, mode='same')\n",
        "\n",
        "        Gxx = np.power(Gx,2)\n",
        "        Gyy = np.power(Gy,2)\n",
        "        Gxy = Gx*Gy\n",
        "\n",
        "        #Now smooth the covariance data to perform a weighted summation of the data.\n",
        "        sze = np.fix(6*self.block_sigma)\n",
        "\n",
        "        gauss = cv2.getGaussianKernel(np.int(sze), self.block_sigma)\n",
        "        f = gauss * gauss.T\n",
        "\n",
        "        Gxx = ndimage.convolve(Gxx,f)\n",
        "        Gyy = ndimage.convolve(Gyy,f)\n",
        "        Gxy = 2*ndimage.convolve(Gxy,f)\n",
        "\n",
        "        # Analytic solution of principal direction\n",
        "        denom = np.sqrt(np.power(Gxy,2) + np.power((Gxx - Gyy),2)) + np.finfo(float).eps\n",
        "\n",
        "        sin2theta = Gxy/denom                   # Sine and cosine of doubled angles\n",
        "        cos2theta = (Gxx-Gyy)/denom\n",
        "\n",
        "\n",
        "        if self.orient_smooth_sigma:\n",
        "            sze = np.fix(6*self.orient_smooth_sigma)\n",
        "            if np.remainder(sze,2) == 0:\n",
        "                sze = sze+1\n",
        "            gauss = cv2.getGaussianKernel(np.int(sze), self.orient_smooth_sigma)\n",
        "            f = gauss * gauss.T\n",
        "            cos2theta = ndimage.convolve(cos2theta,f)                   # Smoothed sine and cosine of\n",
        "            sin2theta = ndimage.convolve(sin2theta,f)                   # doubled angles\n",
        "\n",
        "        self._orientim = np.pi/2 + np.arctan2(sin2theta,cos2theta)/2\n",
        "\n",
        "    def __ridge_freq(self):\n",
        "        # RIDGEFREQ - Calculates a ridge frequency image\n",
        "        #\n",
        "        # Function to estimate the fingerprint ridge frequency across a\n",
        "        # fingerprint image. This is done by considering blocks of the image and\n",
        "        # determining a ridgecount within each block by a call to FREQEST.\n",
        "        #\n",
        "        # Usage:\n",
        "        #  [freqim, medianfreq] =  ridgefreq(im, mask, orientim, blksze, windsze, ...\n",
        "        #                                    minWaveLength, maxWaveLength)\n",
        "        #\n",
        "        # Arguments:\n",
        "        #         im       - Image to be processed.\n",
        "        #         mask     - Mask defining ridge regions (obtained from RIDGESEGMENT)\n",
        "        #         orientim - Ridge orientation image (obtained from RIDGORIENT)\n",
        "        #         blksze   - Size of image block to use (say 32)\n",
        "        #         windsze  - Window length used to identify peaks. This should be\n",
        "        #                    an odd integer, say 3 or 5.\n",
        "        #         minWaveLength,  maxWaveLength - Minimum and maximum ridge\n",
        "        #                     wavelengths, in pixels, considered acceptable.\n",
        "        #\n",
        "        # Output:\n",
        "        #         freqim     - An image  the same size as im with  values set to\n",
        "        #                      the estimated ridge spatial frequency within each\n",
        "        #                      image block.  If a  ridge frequency cannot be\n",
        "        #                      found within a block, or cannot be found within the\n",
        "        #                      limits set by min and max Wavlength freqim is set\n",
        "        #                      to zeros within that block.\n",
        "        #         medianfreq - Median frequency value evaluated over all the\n",
        "        #                      valid regions of the image.\n",
        "        #\n",
        "        # Suggested parameters for a 500dpi fingerprint image\n",
        "        #   [freqim, medianfreq] = ridgefreq(im,orientim, 32, 5, 5, 15);\n",
        "        #\n",
        "\n",
        "        # See also: RIDGEORIENT, FREQEST, RIDGESEGMENT\n",
        "\n",
        "        # Reference:\n",
        "        # Hong, L., Wan, Y., and Jain, A. K. Fingerprint image enhancement:\n",
        "        # Algorithm and performance evaluation. IEEE Transactions on Pattern\n",
        "        # Analysis and Machine Intelligence 20, 8 (1998), 777 789.\n",
        "\n",
        "        ### REFERENCES\n",
        "\n",
        "        # Peter Kovesi\n",
        "        # School of Computer Science & Software Engineering\n",
        "        # The University of Western Australia\n",
        "        # pk at csse uwa edu au\n",
        "        # http://www.csse.uwa.edu.au/~pk\n",
        "\n",
        "        rows, cols = self._normim.shape\n",
        "        freq = np.zeros((rows, cols))\n",
        "\n",
        "        for r in range(0, rows - self.ridge_freq_blksze, self.ridge_freq_blksze):\n",
        "            for c in range(0, cols - self.ridge_freq_blksze, self.ridge_freq_blksze):\n",
        "                blkim = self._normim[r:r + self.ridge_freq_blksze][:, c:c + self.ridge_freq_blksze]\n",
        "                blkor = self._orientim[r:r + self.ridge_freq_blksze][:, c:c + self.ridge_freq_blksze]\n",
        "\n",
        "                freq[r:r + self.ridge_freq_blksze][:, c:c + self.ridge_freq_blksze] = self.__frequest(blkim, blkor)\n",
        "\n",
        "        self._freq = freq * self._mask\n",
        "        freq_1d = np.reshape(self._freq, (1, rows * cols))\n",
        "        ind = np.where(freq_1d > 0)\n",
        "\n",
        "        ind = np.array(ind)\n",
        "        ind = ind[1, :]\n",
        "\n",
        "        non_zero_elems_in_freq = freq_1d[0][ind]\n",
        "\n",
        "        if(len(non_zero_elems_in_freq) != 0):\n",
        "            self._mean_freq = np.mean(non_zero_elems_in_freq)\n",
        "            self._median_freq = np.median(non_zero_elems_in_freq)  # does not work properly\n",
        "        else:\n",
        "            self._mean_freq = 0\n",
        "            self._median_freq = 0  # does not work properly\n",
        "        self._freq = self._mean_freq * self._mask\n",
        "\n",
        "    def __frequest(self, blkim, blkor):\n",
        "        # FREQEST - Estimate fingerprint ridge frequency within image block\n",
        "        #\n",
        "        # Function to estimate the fingerprint ridge frequency within a small block\n",
        "        # of a fingerprint image.  This function is used by RIDGEFREQ\n",
        "        #\n",
        "        # Usage:\n",
        "        #  freqim =  freqest(im, orientim, windsze, minWaveLength, maxWaveLength)\n",
        "        #\n",
        "        # Arguments:\n",
        "        #         im       - Image block to be processed.\n",
        "        #         orientim - Ridge orientation image of image block.\n",
        "        #         windsze  - Window length used to identify peaks. This should be\n",
        "        #                    an odd integer, say 3 or 5.\n",
        "        #         minWaveLength,  maxWaveLength - Minimum and maximum ridge\n",
        "        #                     wavelengths, in pixels, considered acceptable.\n",
        "        #\n",
        "        # Output:\n",
        "        #         freqim    - An image block the same size as im with all values\n",
        "        #                     set to the estimated ridge spatial frequency.  If a\n",
        "        #                     ridge frequency cannot be found, or cannot be found\n",
        "        #                     within the limits set by min and max Wavlength\n",
        "        #                     freqim is set to zeros.\n",
        "        #\n",
        "        # Suggested parameters for a 500dpi fingerprint image\n",
        "        #   freqim = freqest(im,orientim, 5, 5, 15);\n",
        "        #\n",
        "        # See also:  RIDGEFREQ, RIDGEORIENT, RIDGESEGMENT\n",
        "\n",
        "        ### REFERENCES\n",
        "\n",
        "        # Peter Kovesi\n",
        "        # School of Computer Science & Software Engineering\n",
        "        # The University of Western Australia\n",
        "        # pk at csse uwa edu au\n",
        "        # http://www.csse.uwa.edu.au/~pk\n",
        "\n",
        "        rows, cols = np.shape(blkim)\n",
        "\n",
        "        # Find mean orientation within the block. This is done by averaging the\n",
        "        # sines and cosines of the doubled angles before reconstructing the\n",
        "        # angle again.  This avoids wraparound problems at the origin.\n",
        "\n",
        "        cosorient = np.mean(np.cos(2 * blkor))\n",
        "        sinorient = np.mean(np.sin(2 * blkor))\n",
        "        orient = math.atan2(sinorient, cosorient) / 2\n",
        "\n",
        "        # Rotate the image block so that the ridges are vertical\n",
        "\n",
        "        # ROT_mat = cv2.getRotationMatrix2D((cols/2,rows/2),orient/np.pi*180 + 90,1)\n",
        "        # rotim = cv2.warpAffine(im,ROT_mat,(cols,rows))\n",
        "        rotim = scipy.ndimage.rotate(blkim, orient / np.pi * 180 + 90, axes=(1, 0), reshape=False, order=3,\n",
        "                                     mode='nearest')\n",
        "\n",
        "        # Now crop the image so that the rotated image does not contain any\n",
        "        # invalid regions.  This prevents the projection down the columns\n",
        "        # from being mucked up.\n",
        "\n",
        "        cropsze = int(np.fix(rows / np.sqrt(2)))\n",
        "        offset = int(np.fix((rows - cropsze) / 2))\n",
        "        rotim = rotim[offset:offset + cropsze][:, offset:offset + cropsze]\n",
        "\n",
        "        # Sum down the columns to get a projection of the grey values down\n",
        "        # the ridges.\n",
        "\n",
        "        proj = np.sum(rotim, axis=0)\n",
        "        dilation = scipy.ndimage.grey_dilation(proj, self.ridge_freq_windsze, structure=np.ones(self.ridge_freq_windsze))\n",
        "\n",
        "        temp = np.abs(dilation - proj)\n",
        "\n",
        "        peak_thresh = 2\n",
        "\n",
        "        maxpts = (temp < peak_thresh) & (proj > np.mean(proj))\n",
        "        maxind = np.where(maxpts)\n",
        "\n",
        "        rows_maxind, cols_maxind = np.shape(maxind)\n",
        "\n",
        "        # Determine the spatial frequency of the ridges by divinding the\n",
        "        # distance between the 1st and last peaks by the (No of peaks-1). If no\n",
        "        # peaks are detected, or the wavelength is outside the allowed bounds,\n",
        "        # the frequency image is set to 0\n",
        "\n",
        "        if (cols_maxind < 2):\n",
        "            return(np.zeros(blkim.shape))\n",
        "        else:\n",
        "            NoOfPeaks = cols_maxind\n",
        "            waveLength = (maxind[0][cols_maxind - 1] - maxind[0][0]) / (NoOfPeaks - 1)\n",
        "            if waveLength >= self.min_wave_length and waveLength <= self.max_wave_length:\n",
        "                return(1 / np.double(waveLength) * np.ones(blkim.shape))\n",
        "            else:\n",
        "                return(np.zeros(blkim.shape))\n",
        "\n",
        "    def __ridge_filter(self):\n",
        "        # RIDGEFILTER - enhances fingerprint image via oriented filters\n",
        "        #\n",
        "        # Function to enhance fingerprint image via oriented filters\n",
        "        #\n",
        "        # Usage:\n",
        "        #  newim =  ridgefilter(im, orientim, freqim, kx, ky, showfilter)\n",
        "        #\n",
        "        # Arguments:\n",
        "        #         im       - Image to be processed.\n",
        "        #         orientim - Ridge orientation image, obtained from RIDGEORIENT.\n",
        "        #         freqim   - Ridge frequency image, obtained from RIDGEFREQ.\n",
        "        #         kx, ky   - Scale factors specifying the filter sigma relative\n",
        "        #                    to the wavelength of the filter.  This is done so\n",
        "        #                    that the shapes of the filters are invariant to the\n",
        "        #                    scale.  kx controls the sigma in the x direction\n",
        "        #                    which is along the filter, and hence controls the\n",
        "        #                    bandwidth of the filter.  ky controls the sigma\n",
        "        #                    across the filter and hence controls the\n",
        "        #                    orientational selectivity of the filter. A value of\n",
        "        #                    0.5 for both kx and ky is a good starting point.\n",
        "        #         showfilter - An optional flag 0/1.  When set an image of the\n",
        "        #                      largest scale filter is displayed for inspection.\n",
        "        #\n",
        "        # Output:\n",
        "        #         newim    - The enhanced image\n",
        "        #\n",
        "        # See also: RIDGEORIENT, RIDGEFREQ, RIDGESEGMENT\n",
        "\n",
        "        # Reference:\n",
        "        # Hong, L., Wan, Y., and Jain, A. K. Fingerprint image enhancement:\n",
        "        # Algorithm and performance evaluation. IEEE Transactions on Pattern\n",
        "        # Analysis and Machine Intelligence 20, 8 (1998), 777 789.\n",
        "\n",
        "        ### REFERENCES\n",
        "\n",
        "        # Peter Kovesi\n",
        "        # School of Computer Science & Software Engineering\n",
        "        # The University of Western Australia\n",
        "        # pk at csse uwa edu au\n",
        "        # http://www.csse.uwa.edu.au/~pk\n",
        "\n",
        "        im = np.double(self._normim)\n",
        "        rows, cols = im.shape\n",
        "        newim = np.zeros((rows, cols))\n",
        "        freq_1d = np.reshape(self._freq, (1, rows * cols))\n",
        "        ind = np.where(freq_1d > 0)\n",
        "        \n",
        "        ind = np.array(ind)\n",
        "        ind = ind[1, :]\n",
        "\n",
        "        # Round the array of frequencies to the nearest 0.01 to reduce the\n",
        "        # number of distinct frequencies we have to deal with.\n",
        "\n",
        "        non_zero_elems_in_freq = freq_1d[0][ind]\n",
        "        non_zero_elems_in_freq = np.double(np.round((non_zero_elems_in_freq * 100))) / 100\n",
        "        \n",
        "        unfreq = np.unique(non_zero_elems_in_freq)\n",
        "\n",
        "        # Generate filters corresponding to these distinct frequencies and\n",
        "        # orientations in 'angleInc' increments.\n",
        "\n",
        "        sigmax = 1 / unfreq[0] * self.kx\n",
        "        sigmay = 1 / unfreq[0] * self.ky\n",
        "\n",
        "        sze = np.int(np.round(3 * np.max([sigmax, sigmay])))\n",
        "\n",
        "        x, y = np.meshgrid(np.linspace(-sze, sze, (2 * sze + 1)), np.linspace(-sze, sze, (2 * sze + 1)))\n",
        "\n",
        "        reffilter = np.exp(-(((np.power(x, 2)) / (sigmax * sigmax) + (np.power(y, 2)) / (sigmay * sigmay)))) * np.cos(\n",
        "            2 * np.pi * unfreq[0] * x)        # this is the original gabor filter\n",
        "\n",
        "        filt_rows, filt_cols = reffilter.shape\n",
        "\n",
        "        angleRange = np.int(180 / self.angleInc)\n",
        "\n",
        "        gabor_filter = np.array(np.zeros((angleRange, filt_rows, filt_cols)))\n",
        "\n",
        "        for o in range(0, angleRange):\n",
        "            # Generate rotated versions of the filter.  Note orientation\n",
        "            # image provides orientation *along* the ridges, hence +90\n",
        "            # degrees, and imrotate requires angles +ve anticlockwise, hence\n",
        "            # the minus sign.\n",
        "\n",
        "            rot_filt = scipy.ndimage.rotate(reffilter, -(o * self.angleInc + 90), reshape=False)\n",
        "            gabor_filter[o] = rot_filt\n",
        "\n",
        "        # Find indices of matrix points greater than maxsze from the image\n",
        "        # boundary\n",
        "\n",
        "        maxsze = int(sze)\n",
        "\n",
        "        temp = self._freq > 0\n",
        "        validr, validc = np.where(temp)\n",
        "\n",
        "        temp1 = validr > maxsze\n",
        "        temp2 = validr < rows - maxsze\n",
        "        temp3 = validc > maxsze\n",
        "        temp4 = validc < cols - maxsze\n",
        "\n",
        "        final_temp = temp1 & temp2 & temp3 & temp4\n",
        "\n",
        "        finalind = np.where(final_temp)\n",
        "\n",
        "        # Convert orientation matrix values from radians to an index value\n",
        "        # that corresponds to round(degrees/angleInc)\n",
        "\n",
        "        maxorientindex = np.round(180 / self.angleInc)\n",
        "        orientindex = np.round(self._orientim / np.pi * 180 / self.angleInc)\n",
        "\n",
        "        # do the filtering\n",
        "        for i in range(0, rows):\n",
        "            for j in range(0, cols):\n",
        "                if (orientindex[i][j] < 1):\n",
        "                    orientindex[i][j] = orientindex[i][j] + maxorientindex\n",
        "                if (orientindex[i][j] > maxorientindex):\n",
        "                    orientindex[i][j] = orientindex[i][j] - maxorientindex\n",
        "        finalind_rows, finalind_cols = np.shape(finalind)\n",
        "        sze = int(sze)\n",
        "        for k in range(0, finalind_cols):\n",
        "            r = validr[finalind[0][k]]\n",
        "            c = validc[finalind[0][k]]\n",
        "\n",
        "            img_block = im[r - sze:r + sze + 1][:, c - sze:c + sze + 1]\n",
        "\n",
        "            newim[r][c] = np.sum(img_block * gabor_filter[int(orientindex[r][c]) - 1])\n",
        "            \n",
        "        self._binim = newim < self.ridge_filter_thresh\n",
        "\n",
        "    def enhance(self, img, resize=False, size: Optional[list]=350):\n",
        "        # main function to enhance the image.\n",
        "        # calls all other subroutines\n",
        "\n",
        "        if (len(img.shape) > 2):  # convert image into gray if necessary\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        if(resize):\n",
        "            if size==350:\n",
        "              rows, cols = np.shape(img)\n",
        "              aspect_ratio = np.double(rows) / np.double(cols)\n",
        "\n",
        "              new_rows = size                     # randomly selected number\n",
        "              new_cols = new_rows / aspect_ratio\n",
        "\n",
        "              img = cv2.resize(img, (np.int(new_cols), np.int(new_rows)),cv2.INTER_NEAREST)\n",
        "            else:\n",
        "              img = cv2.resize(img, (np.int(size[1]), np.int(size[0])),cv2.INTER_NEAREST)\n",
        "\n",
        "        self.__ridge_segment(img)   # normalise the image and find a ROI\n",
        "        self.__ridge_orient()       # compute orientation image\n",
        "        self.__ridge_freq()         # compute major frequency of ridges\n",
        "        if self._mean_freq!=0:\n",
        "          self.__ridge_filter()       # filter the image using oriented gabor filter\n",
        "          failed=False\n",
        "        else: failed=True\n",
        "        #self._binim = 255 * self._binim.astype('uint8')\n",
        "        return(np.array(self._binim).astype('int'),failed)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VALnI8jHIVU9"
      },
      "source": [
        "###my fun"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-TbGo9bJa1q"
      },
      "source": [
        "#import torchvision\n",
        "#from torchvision import transforms\n",
        "\n",
        "def enhanc(img,mask,size_out : Optional[list]=0): \n",
        "  '''\n",
        "  img: ndarray 1x3xnxm\n",
        "  mask: ndarray nxm (same dim of size_out)\n",
        "  size_out: (int) effettua un resize dell'immagine prima dell'enhancement (se 0 dà in uscita l'img con le stesse dim dell'input)\n",
        "  return: img_en 1x3xnxm\n",
        "  '''\n",
        "  res=False\n",
        "  #res=True\n",
        "  r, g, b = img[0,0,:,:],img[0,1,:,:],img[0,2,:,:]\n",
        "  img_g = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
        "  size_init=img_g.shape\n",
        "  finger_en=FingerprintImageEnhancer()\n",
        "  if size_out==0: #se non voglio una dimensione specifica in uscita trasformo l'img finale con le dim iniziali\n",
        "    if size_init[0]<350: res=True\n",
        "    out,failed = finger_en.enhance(img=img_g,resize=res,size=350)\n",
        "    if res:\n",
        "      out = cv2.resize(np.array(out), (size_init[1],size_init[0]),interpolation=cv2.INTER_NEAREST)\n",
        "  else:\n",
        "    #inserire controllo se size_out<350\n",
        "    res=True\n",
        "    out,failed = finger_en.enhance(img=img_g,resize=res,size=size_out)\n",
        "\n",
        "  size_out=out.shape\n",
        "  if failed==False:\n",
        "    out=1-out\n",
        "    img=np.zeros([1,3,size_out[0],size_out[1]])\n",
        "    for i in range(3):\n",
        "      img[0,i,:,:]=out\n",
        "    img=np.where(mask == 0.0, 1, img)\n",
        "  else: \n",
        "    print(\"failed enhanc\")\n",
        "\n",
        "  return img"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q64Z8X5mzAZ1"
      },
      "source": [
        "##Perturbazione"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GegLlIcWzCSv"
      },
      "source": [
        "def compute_perturb(x,x_adv,transf_init):\n",
        "  '''\n",
        "    x: img originali\n",
        "    x_adv: img contraddittorie\n",
        "    return: pertubazioni\n",
        "  '''\n",
        "  #calcola la perturbazione con img 224x224\n",
        "  if x_adv.shape[2]==224:\n",
        "    x=np.array(transf_resize(torch.Tensor(x)))\n",
        "  perturb=x_adv-x\n",
        "  \n",
        "  perturb=np.array(transf_init(torch.Tensor(perturb)))\n",
        "  \n",
        "  return perturb"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmyIKi7XvvSf"
      },
      "source": [
        "##Print subplot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uspz0KYNvwuS"
      },
      "source": [
        "def print_subplot(perturb,x_test,y_test,preds,x_test_adv,value_preds_adv):\n",
        "  '''\n",
        "    perturb: perturbazioni\n",
        "    x_test: img originali\n",
        "    y_test: classi originali\n",
        "    preds: classi predette per img originali\n",
        "    x_test_adv: img contraddittoria\n",
        "    value_preds_adv: probabilità predette img contraddittorie\n",
        "    normalize: se True normalizza le immagini\n",
        "  '''\n",
        "  \n",
        "  #nel seguente ciclo for si crea un vettore delle classi predette ordinato per probabilità decrescente\n",
        "  for i in range(len(x_test)):#(x_test.shape[0]):\n",
        "    value=value_preds_adv[i]*100\n",
        "    value_sorted=sorted(value,reverse=True)\n",
        "    classes=[]\n",
        "    for j in range(value.size) :\n",
        "      ind=np.where(value==value_sorted[j]) #restituisce l'indice in value del valore uguale a value_sorted[i], quindi è la classe\n",
        "      classes.append(classes_name[ind[0][0]]) #classes è il vettore finale\n",
        "    \n",
        "    value_sorted=[ round(elem, 2) for elem in value_sorted ]\n",
        "\n",
        "    val_pert=np.mean(np.abs(perturb[i]))\n",
        "    pert_min,pert_max=np.min(perturb[i]),np.max(perturb[i])\n",
        "    #perturb,val_pert,perturb_norm=compute_perturb(x_test,x_test_adv)\n",
        "    perturb[i]=np.clip(perturb[i],0,1) #rimuovo i valori negativi poiché non posso visualizzarli\n",
        "    #in seguito per ogni immagine del test set si stampa un subplot\n",
        "    fig = plt.figure(figsize=[20,20])\n",
        "    #plt.subplots_adjust(wspace=0.9)\n",
        "    print('\\033[1m'+\"IMMAGINE \"+'\\033[1m',i) #valore END: '\\033[0m'\n",
        "    #originale\n",
        "    ax1 = fig.add_subplot(131) #subplot con 3 righe e due colonne\n",
        "    ax1.axis('off')\n",
        "    #ax1.imshow(cv2.rotate(x_test[i],cv2.cv2.ROTATE_90_CLOCKWISE))\n",
        "    ax1.imshow(x_test[i].transpose(1,2,0))\n",
        "    ax1.title.set_text(\"ORIGINALE\\nclasse reale: \"+classes_name[np.argmax(y_test[i])]+\"\\nclasse predetta: \"+str(preds[i]))\n",
        "    #perturbazione\n",
        "    ax2 = fig.add_subplot(132)\n",
        "    #ax2.imshow(cv2.rotate(perturb_norm[i],cv2.cv2.ROTATE_90_CLOCKWISE))\n",
        "    ax2.imshow(perturb[i].transpose(1,2,0),cmap='gray')\n",
        "    #ax2.imshow(perturb[i],cmap='gray')\n",
        "    ax2.axis('off')\n",
        "    ax2.title.set_text(\"PERTURBAZIONE\\nvalore medio: \"+str(round(val_pert,4))+\"\\nmin: \"+str(pert_min)+\"\\nmax: \"+str(pert_max))\n",
        "    #perturbata\n",
        "    ax3 = fig.add_subplot(133)\n",
        "    #ax3.imshow(cv2.rotate(x_test_adv[i],cv2.cv2.ROTATE_90_CLOCKWISE))#,aspect='auto')\n",
        "    ax3.imshow(x_test_adv[i].transpose(1,2,0))\n",
        "    ax3.axis('off')\n",
        "    ax3.title.set_text(\"PERTURBATA\\nclassi predette: \"+str(classes)+\"\\ncon valori: \"+str(value_sorted))\n",
        "    plt.show()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ6xDO5GGZY5"
      },
      "source": [
        "##Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VvGULk0ng1S"
      },
      "source": [
        "def test_average(classifier,input,transf_init):\n",
        "  '''\n",
        "  model: model trained\n",
        "  test_loader: dataloader \n",
        "  return: pred: classe predetta, probabilities, values\n",
        "  '''\n",
        "\n",
        "  def calc_size(n):\n",
        "    '''\n",
        "    n: int \n",
        "    return: 80% of n\n",
        "    '''\n",
        "    return tuple(int(np.ceil(i * (80/100))) for i in n)\n",
        "\n",
        "  preds=[]\n",
        "  #value=[]\n",
        "  prob=nn.Softmax(dim=0)\n",
        "  \n",
        "  if input.shape[2]==224:\n",
        "    input=transf_init(input)\n",
        "\n",
        "  n=input.shape\n",
        "  n_mod=calc_size(n[2:4])\n",
        "  crop_transform=transforms.TenCrop((n_mod[0],n_mod[1]))\n",
        "  crops=crop_transform(input)\n",
        "  live=0\n",
        "  spoof=0\n",
        "  for crop in crops:\n",
        "    crop=data_transform_test(crop) #resize 224\n",
        "    outputs = classifier.predict(crop)\n",
        "    live+=outputs[0][0]\n",
        "    spoof+=outputs[0][1]\n",
        "  live=live/10\n",
        "  spoof=spoof/10\n",
        "  values=[live,spoof]\n",
        "  predicted=np.argmax(values)\n",
        "  probabilities=prob(torch.Tensor(values)).numpy()\n",
        "\n",
        "  return predicted,probabilities,values"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm63-OLUwGE-"
      },
      "source": [
        "##mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnK0KawpwHeq"
      },
      "source": [
        "def compute_mask(img,dim_contours=10,n_rect=2):\n",
        "  '''\n",
        "  img: tensor 1x3xnxm\n",
        "  dim_contours: larghezza dei contorni da tracciare (consigliati 10 in generale e 8 per Latex)\n",
        "  n_rect: dimensione rect di getStructuringElement (consigliata 2 in generale e 1 per Latex)\n",
        "  '''\n",
        "  #img iniziale [0,1]\n",
        "  img=np.array(img[0])\n",
        "  img=img.transpose(1,2,0)\n",
        "\n",
        "  #trasforma in gray\n",
        "  r, g, b = img[:,:,0], img[:,:,1], img[:,:,2]\n",
        "  gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
        "  #scala in [0,255]\n",
        "  gray = (gray - np.min(gray)) / (np.max(gray) - np.min(gray))\n",
        "  gray=gray*255\n",
        "  gray=gray.astype('uint8')\n",
        "  #cv2_imshow(gray)\n",
        "  #calcola immagine binaria\n",
        "  ret, imgf = cv2.threshold(gray, 0,255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
        "\n",
        "  # Morph open to remove noise\n",
        "  kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (n_rect,n_rect))\n",
        "  imgf = cv2.morphologyEx(imgf, cv2.MORPH_OPEN, kernel, iterations=1)\n",
        "  #cv2_imshow(imgf)\n",
        "  image_contours = np.zeros((imgf.shape[0],\n",
        "                            imgf.shape[1]),\n",
        "                            np.uint8)\n",
        "\n",
        "  image_binary = np.zeros((imgf.shape[0],\n",
        "                          imgf.shape[1]),\n",
        "                          np.uint8)\n",
        "\n",
        "  #cerca i contorni nell'immagine binaria\n",
        "  contours =cv2.findContours(imgf, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[0]\n",
        "\n",
        "  cv2.drawContours(image_contours,\n",
        "                      contours, -1,\n",
        "                      (255,255), dim_contours) \n",
        "  \n",
        "\n",
        "  #image_contours=cv2.GaussianBlur(image_contours, (3,3), 0)\n",
        "  #cv2_imshow(image_contours)\n",
        "  contours = cv2.findContours(image_contours, cv2.RETR_LIST,\n",
        "                            cv2.CHAIN_APPROX_SIMPLE)[0]\n",
        "  #disegna solo il contorno più esterno\n",
        "  cv2.drawContours(image_binary, [max(contours, key = cv2.contourArea)],\n",
        "                  -1, (255, 255),-1)\n",
        "  #cv2_imshow(image_binary)\n",
        "  #restituisce immagine [0,1]\n",
        "  return image_binary/255"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_qc1J0y4Ucu"
      },
      "source": [
        "##Save/read"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFrqikuV4WIb"
      },
      "source": [
        "def save_read(x,classifier,transf_init):\n",
        "  '''\n",
        "  x: 3xnxm ndarray\n",
        "  classifier\n",
        "  transf_init\n",
        "  return: probabilities before & after\n",
        "  '''\n",
        "  _,p_before,_=test_average(classifier,torch.Tensor(x).unsqueeze_(0),transf_init)\n",
        "  x=x.transpose(1,2,0)*255\n",
        "  #plt.imsave('prova.bmp',x)\n",
        "  cv2.imwrite('prova.png',x)\n",
        "  #plt.imshow(x)\n",
        "  #cv2_imshow(x)\n",
        "\n",
        "  x=cv2.imread('prova.png')\n",
        "  x=x/255\n",
        "  #x=plt.imread('prova.png')/255\n",
        "  #plt.imshow(prova_arr*255)\n",
        "  #cv2_imshow(x*255)\n",
        "  _,p_after,_=test_average(classifier,torch.Tensor(x.transpose(2,0,1)).unsqueeze_(0),transf_init)\n",
        "  return p_before,p_after"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWxzZ1rGd5LY"
      },
      "source": [
        "##accuracy single class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5X2LDppd6bN"
      },
      "source": [
        "def accuracy_class(class_str,pd_class):\n",
        "  '''\n",
        "  calcola il numero di predizioni giuste per una data classe specificata in input\n",
        "  class_str: string class\n",
        "  pd_class: dataframe\n",
        "  '''\n",
        "  n=np.sum(pd_class['real']==class_str)\n",
        "  print(\"# img\"+class_str+\": \"+str(n))\n",
        "\n",
        "  p=pd_class.loc[pd_class['real']==class_str]\n",
        "  p=p.loc[p['predicted']==p['real']]\n",
        "  n_class=p.count(0)[0]\n",
        "  print(\"Numero di predizioni \"+class_str+\" giuste: \"+str(n_class))\n",
        "  print(\"Accuracy : \"+str(round(n_class/n*100,2)))  "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55yuMaawbgyt"
      },
      "source": [
        "##print accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zit5EmQsbiHX"
      },
      "source": [
        "def print_acc(pd_preds):\n",
        "  '''\n",
        "  stampa accuracy totale usando il dataframe con le predizioni\n",
        "  pd_preds: dataframe\n",
        "  '''\n",
        "  true_label = pd_preds.real.values\n",
        "  predicted = pd_preds.predicted.values\n",
        "  accuracy=round((np.sum((true_label == predicted).astype(int)))/pd_preds.shape[0],4)*100\n",
        "  print(\"\\nAccuracy: {0}\".format(accuracy))\n",
        "  print(\"Shape dataframe: {0}\".format(pd_preds.shape)) "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfEnYI_UUEb6"
      },
      "source": [
        "##IFGM mod"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3oRorbFUCF-"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import logging\n",
        "from typing import Optional, Union, TYPE_CHECKING\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from art.config import ART_NUMPY_DTYPE\n",
        "from art.attacks.attack import EvasionAttack\n",
        "from art.estimators.estimator import BaseEstimator, LossGradientsMixin\n",
        "from art.estimators.classification.classifier import ClassifierMixin\n",
        "from art.utils import (\n",
        "    compute_success,\n",
        "    get_labels_np_array,\n",
        "    random_sphere,\n",
        "    projection,\n",
        "    check_and_transform_label_format,\n",
        ")\n",
        "\n",
        "if TYPE_CHECKING:\n",
        "    from art.utils import CLASSIFIER_LOSS_GRADIENTS_TYPE\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class IterativeFastGradientMethod(EvasionAttack):\n",
        "    \"\"\"\n",
        "    This attack was originally implemented by Goodfellow et al. (2015) with the infinity norm (and is known as the \"Fast\n",
        "    Gradient Sign Method\"). This implementation extends the attack to other norms, and is therefore called the Fast\n",
        "    Gradient Method.\n",
        "    | Paper link: https://arxiv.org/abs/1412.6572\n",
        "    \"\"\"\n",
        "\n",
        "    attack_params = EvasionAttack.attack_params + [\n",
        "        \"norm\",\n",
        "        \"eps\",\n",
        "        \"eps_step\",\n",
        "        \"class_target\",\n",
        "        \"max_iter\",\n",
        "        \"confidence\",\n",
        "        \"tensor_board\",\n",
        "    ]\n",
        "    _estimator_requirements = (BaseEstimator, LossGradientsMixin)\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        estimator: \"CLASSIFIER_LOSS_GRADIENTS_TYPE\",\n",
        "        norm: Union[int, float, str] = np.inf,\n",
        "        eps: Union[int, float, np.ndarray] = 0.3,\n",
        "        eps_step: Union[int, float, np.ndarray] = 0.1,\n",
        "        class_target: int = 0,\n",
        "        max_iter: int = 10,\n",
        "        confidence: float = 0.6, \n",
        "        tensor_board: Union[str, bool] = False,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Create a :class:`.FastGradientMethod` instance.\n",
        "        :param estimator: A trained classifier.\n",
        "        :param norm: The norm of the adversarial perturbation. Possible values: \"inf\", np.inf, 1 or 2.\n",
        "        :param eps: Attack step size (input variation).\n",
        "        :param eps_step: Step size of input variation for minimal perturbation computation.\n",
        "        :param class_target: classe da far predire.\n",
        "        :param max_iter: numero massimo di iterazioni\n",
        "        :param confidence: probabilità minima con la quale predire class_target\n",
        "        :param tensor_board: Activate summary writer for TensorBoard: Default is `False` and deactivated summary writer.\n",
        "                             If `True` save runs/CURRENT_DATETIME_HOSTNAME in current directory. Provide `path` in type\n",
        "                             `str` to save in path/CURRENT_DATETIME_HOSTNAME.\n",
        "                             Use hierarchical folder structure to compare between runs easily. e.g. pass in ‘runs/exp1’,\n",
        "                             ‘runs/exp2’, etc. for each new experiment to compare across them.\n",
        "        \"\"\"\n",
        "        super().__init__(estimator=estimator, tensor_board=tensor_board)\n",
        "        self.norm = norm\n",
        "        self.eps = eps\n",
        "        self.eps_step = eps_step\n",
        "        self.class_target=class_target\n",
        "        self.max_iter=max_iter\n",
        "        self.confidence=confidence\n",
        "        self._project = True\n",
        "        IterativeFastGradientMethod._check_params(self)\n",
        "\n",
        "        self._i_max_iter = 0\n",
        "\n",
        "    def _check_compatibility_input_and_eps(self, x: np.ndarray):\n",
        "        \"\"\"\n",
        "        Check the compatibility of the input with `eps` and `eps_step` which are of the same shape.\n",
        "        :param x: An array with the original inputs.\n",
        "        \"\"\"\n",
        "        if isinstance(self.eps, np.ndarray):\n",
        "            # Ensure the eps array is broadcastable\n",
        "            if self.eps.ndim > x.ndim:\n",
        "                raise ValueError(\"The `eps` shape must be broadcastable to input shape.\")\n",
        "\n",
        "    def _minimal_perturbation(self, x: np.ndarray, y: np.ndarray, mask_mod, enh) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Iteratively compute the minimal perturbation necessary to make the class prediction change. Stop when the\n",
        "        first adversarial example was found.\n",
        "        :param x: An array with the original inputs.\n",
        "        :param y: Target values (class labels) one-hot-encoded of shape (nb_samples, nb_classes).\n",
        "        :param class_target: class da predire se target=True (nel nostro caso vogliamo far predire sempre live)\n",
        "        :return: An array holding the adversarial examples.\n",
        "        \"\"\"\n",
        "        adv_x = x.copy()\n",
        "\n",
        "        size_init=np.array(x.shape[2:4])\n",
        "        transf_orig=transforms.Resize(size=(size_init[0],size_init[1]),interpolation=InterpolationMode.NEAREST)\n",
        "\n",
        "        pred,values,_=test_average(self.estimator,torch.Tensor(adv_x),transf_orig)\n",
        "        if ((pred==np.argmax(y, axis=1)) and (pred!= self.class_target)):\n",
        "           active=True\n",
        "        elif ((pred!=np.argmax(y, axis=1)) and (pred== self.class_target) and (np.max(values)<self.confidence)): \n",
        "           active=True\n",
        "        else:  active=False\n",
        "\n",
        "        current_eps = self.eps_step\n",
        "        partial_stop_condition = current_eps <= self.eps\n",
        "            \n",
        "        iter=0\n",
        "        if active==True:\n",
        "          adv_x=transf_resize(torch.Tensor(adv_x))\n",
        "          #mask_mod=compute_mask(torch.Tensor(adv_x))\n",
        "          #cv2_imshow(mask_mod*255)\n",
        "          adv_x=np.array(adv_x)\n",
        "          #adv_x=enhanc(adv_x,[224,224],mask_mod)\n",
        "          #list_en=[i for i in range(0,self.max_iter,step_en)]\n",
        "          #x_1=adv_x.copy()\n",
        "          while active==True and partial_stop_condition and iter<self.max_iter:\n",
        "                iter+=1\n",
        "                #sys.stdout.write(\"\\rIter: {0}/{1}\".format(iter,self.max_iter))\n",
        "                #sys.stdout.flush()\n",
        "                #calcolo perturbazione\n",
        "                perturbation = self._compute_perturbation(adv_x, y, mask_mod) #[-1,1]\n",
        "                \n",
        "                #trasformazione in gray\n",
        "                r, g, b = perturbation[0,0,:,:],perturbation[0,1,:,:],perturbation[0,2,:,:]\n",
        "                perturbation = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
        "                \n",
        "                current_x = self._apply_perturbation(adv_x, perturbation, current_eps)\n",
        "                adv_x=current_x\n",
        "                '''\n",
        "                print(\"prima\")\n",
        "                cv2_imshow(current_x[0].transpose(1,2,0)*255)\n",
        "                adv_x=enhanc(current_x,[224,224],mask_mod)\n",
        "                print(\"dopo\")\n",
        "                cv2_imshow(adv_x[0].transpose(1,2,0)*255)\n",
        "                '''\n",
        "                #if iter in list_en: adv_x=enhanc(adv_x,mask_mod)\n",
        "                if enh:\n",
        "                  #print(\"prima\")\n",
        "                  #cv2_imshow(adv_x[0].transpose(1,2,0)*255)\n",
        "                  adv_x=enhanc(adv_x,mask_mod)\n",
        "                  #print(\"dopo\")\n",
        "                  #cv2_imshow(adv_x[0].transpose(1,2,0)*255)                  \n",
        "                pred,values,_=test_average(self.estimator,torch.Tensor(adv_x),transf_orig)\n",
        "                \n",
        "                # If targeted active check to see whether we have hit the target, otherwise head to anything but\n",
        "                if ((pred==np.argmax(y, axis=1)) and (pred!= self.class_target)):\n",
        "                  active=True\n",
        "                elif ((pred!=np.argmax(y, axis=1)) and (pred== self.class_target) and (np.max(values)<self.confidence)): \n",
        "                  active=True\n",
        "                else:  active=False\n",
        "\n",
        "                # Update current eps and check the stop condition\n",
        "                current_eps = current_eps + self.eps_step\n",
        "                partial_stop_condition = current_eps <= self.eps\n",
        "\n",
        "          #adv_x=enhanc(adv_x,[500,500],mask_mod)\n",
        "\n",
        "        return adv_x\n",
        "\n",
        "    def generate(self, mask, x: np.ndarray, y: Optional[np.ndarray] = None, enh: Optional[bool] = False, **kwargs) -> np.ndarray:\n",
        "        \"\"\"Generate adversarial samples and return them in an array.\n",
        "        :param x: An array with the original inputs.\n",
        "        :param y: Target values (class labels) one-hot-encoded of shape (nb_samples, nb_classes) or indices of shape\n",
        "                  (nb_samples,). Only provide this parameter if you'd like to use true labels when crafting adversarial\n",
        "                  samples. Otherwise, model predictions are used as labels to avoid the \"label leaking\" effect\n",
        "                  (explained in this paper: https://arxiv.org/abs/1611.01236). Default is `None`.       \n",
        "        :return: An array holding the adversarial examples.\n",
        "        \"\"\"\n",
        "\n",
        "        # Ensure eps is broadcastable\n",
        "        self._check_compatibility_input_and_eps(x=x)\n",
        "\n",
        "        y = check_and_transform_label_format(y, self.estimator.nb_classes)\n",
        "\n",
        "        if y is None:\n",
        "           # Throw error if attack is targeted, but no targets are provided\n",
        "           # Use model predictions as correct outputs\n",
        "           logger.info(\"Using model predictions as correct labels for FGM.\")\n",
        "           y = get_labels_np_array(self.estimator.predict(x))  # type: ignore\n",
        "\n",
        "        # Return adversarial examples computed with minimal perturbation if option is active\n",
        "        adv_x_best = self._minimal_perturbation(x, y, mask, enh)\n",
        "        \n",
        "        return adv_x_best\n",
        "\n",
        "    def _check_params(self) -> None:\n",
        "\n",
        "        if self.norm not in [1, 2, np.inf, \"inf\"]:\n",
        "            raise ValueError('Norm order must be either 1, 2, `np.inf` or \"inf\".')\n",
        "\n",
        "        if not (\n",
        "            isinstance(self.eps, (int, float))\n",
        "            and isinstance(self.eps_step, (int, float))\n",
        "            or isinstance(self.eps, np.ndarray)\n",
        "            and isinstance(self.eps_step, np.ndarray)\n",
        "        ):\n",
        "            raise TypeError(\n",
        "                \"The perturbation size `eps` and the perturbation step-size `eps_step` must have the same type of `int`\"\n",
        "                \", `float`, or `np.ndarray`.\"\n",
        "            )\n",
        "\n",
        "        if isinstance(self.eps, (int, float)):\n",
        "            if self.eps < 0:\n",
        "                raise ValueError(\"The perturbation size `eps` has to be nonnegative.\")\n",
        "        else:\n",
        "            if (self.eps < 0).any():\n",
        "                raise ValueError(\"The perturbation size `eps` has to be nonnegative.\")\n",
        "\n",
        "        if isinstance(self.eps_step, (int, float)):\n",
        "            if self.eps_step <= 0:\n",
        "                raise ValueError(\"The perturbation step-size `eps_step` has to be positive.\")\n",
        "        else:\n",
        "            if (self.eps_step <= 0).any():\n",
        "                raise ValueError(\"The perturbation step-size `eps_step` has to be positive.\")\n",
        "\n",
        "        if isinstance(self.eps, np.ndarray) and isinstance(self.eps_step, np.ndarray):\n",
        "            if self.eps.shape != self.eps_step.shape:\n",
        "                raise ValueError(\n",
        "                    \"The perturbation size `eps` and the perturbation step-size `eps_step` must have the same shape.\"\n",
        "                )\n",
        "        if isinstance(self.class_target,int):\n",
        "            if self.class_target!=0 and self.class_target!=1:\n",
        "              raise ValueError(\"Attacco destinato alle impronte (0,1)\")\n",
        "        else: \n",
        "          raise ValueError(\"Class_target deve essere intero\")   \n",
        "\n",
        "        if isinstance(self.max_iter,int):\n",
        "            if self.max_iter <=0:\n",
        "              raise ValueError(\"max_iter Deve essere positivo\")\n",
        "        else: \n",
        "          raise ValueError(\"max_iter deve essere intero\")         \n",
        "\n",
        "        if isinstance(self.confidence,float):\n",
        "            if self.confidence <=0 or self.confidence >1:\n",
        "              raise ValueError(\"confidence deve essere compreso tra 0 e 1\")\n",
        "        else: \n",
        "          raise ValueError(\"confidence deve essere float\")         \n",
        "          \n",
        "\n",
        "    def _compute_perturbation(\n",
        "        self, batch: np.ndarray, batch_labels: np.ndarray, mask: Optional[np.ndarray]\n",
        "    ) -> np.ndarray:\n",
        "        # Pick a small scalar to avoid division by 0\n",
        "        tol = 10e-8\n",
        "        batch=np.array(trans_norm(torch.Tensor(batch)))\n",
        "        # Get gradient wrt loss; invert it if attack is targeted\n",
        "        grad = self.estimator.loss_gradient(batch, batch_labels)\n",
        "\n",
        "        # Check for NaN before normalisation an replace with 0\n",
        "        if grad.dtype != np.object and np.isnan(grad).any():\n",
        "            logger.warning(\"Elements of the loss gradient are NaN and have been replaced with 0.0.\")\n",
        "            grad = np.where(np.isnan(grad), 0.0, grad)\n",
        "        else:\n",
        "            for i, _ in enumerate(grad):\n",
        "                grad_i_array = grad[i].astype(np.float32)\n",
        "                if np.isnan(grad_i_array).any():\n",
        "                    grad[i] = np.where(np.isnan(grad_i_array), 0.0, grad_i_array).astype(np.object)\n",
        "\n",
        "        # Apply mask\n",
        "        if mask is not None:\n",
        "            grad = np.where(mask == 0.0, 0.0, grad)\n",
        "            #grad = np.where(mask2 == 1,0, grad)\n",
        "\n",
        "        # Apply norm bound\n",
        "        def _apply_norm(grad, object_type=False):\n",
        "            if (grad.dtype != np.object and np.isinf(grad).any()) or np.isnan(grad.astype(np.float32)).any():\n",
        "                logger.info(\"The loss gradient array contains at least one positive or negative infinity.\")\n",
        "\n",
        "            if self.norm in [np.inf, \"inf\"]:\n",
        "                grad = np.sign(grad)\n",
        "            elif self.norm == 1:\n",
        "                if not object_type:\n",
        "                    ind = tuple(range(1, len(batch.shape)))\n",
        "                else:\n",
        "                    ind = None\n",
        "                grad = grad / (np.sum(np.abs(grad), axis=ind, keepdims=True) + tol)\n",
        "            elif self.norm == 2:\n",
        "                if not object_type:\n",
        "                    ind = tuple(range(1, len(batch.shape)))\n",
        "                else:\n",
        "                    ind = None\n",
        "                grad = grad / (np.sqrt(np.sum(np.square(grad), axis=ind, keepdims=True)) + tol)\n",
        "            return grad\n",
        "\n",
        "        if batch.dtype == np.object:\n",
        "            for i_sample in range(batch.shape[0]):\n",
        "                grad[i_sample] = _apply_norm(grad[i_sample], object_type=True)\n",
        "                assert batch[i_sample].shape == grad[i_sample].shape\n",
        "        else:\n",
        "            grad = _apply_norm(grad)\n",
        "\n",
        "        assert batch.shape == grad.shape\n",
        "\n",
        "        return grad\n",
        "\n",
        "    def _apply_perturbation(\n",
        "        self, batch: np.ndarray, perturbation: np.ndarray, eps_step: Union[int, float, np.ndarray]\n",
        "    ) -> np.ndarray:\n",
        "\n",
        "        perturbation_step = eps_step * perturbation\n",
        "        if perturbation_step.dtype != np.object:\n",
        "            perturbation_step[np.isnan(perturbation_step)] = 0\n",
        "        else:\n",
        "            for i, _ in enumerate(perturbation_step):\n",
        "                perturbation_step_i_array = perturbation_step[i].astype(np.float32)\n",
        "                if np.isnan(perturbation_step_i_array).any():\n",
        "                    perturbation_step[i] = np.where(\n",
        "                        np.isnan(perturbation_step_i_array), 0.0, perturbation_step_i_array\n",
        "                    ).astype(np.object)\n",
        "\n",
        "        for i in range(3):\n",
        "          batch[0,i,:,:] = batch[0,i,:,:] + perturbation_step #applica la perturbazione grayscale ad ogni canale delle immagini originali\n",
        "        if self.estimator.clip_values is not None:\n",
        "            clip_min, clip_max = self.estimator.clip_values\n",
        "            batch = np.clip(batch, clip_min, clip_max)\n",
        "\n",
        "        return batch"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtOAEGt4Mmoh"
      },
      "source": [
        "##APGD mod"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5xioHKgMoWn"
      },
      "source": [
        "import logging\n",
        "import math\n",
        "from typing import Optional, Union, TYPE_CHECKING\n",
        "\n",
        "import numpy as np\n",
        "from tqdm.auto import trange\n",
        "\n",
        "from art.config import ART_NUMPY_DTYPE\n",
        "from art.attacks.attack import EvasionAttack\n",
        "from art.estimators.estimator import BaseEstimator, LossGradientsMixin\n",
        "from art.estimators.classification.classifier import ClassifierMixin\n",
        "from art.utils import check_and_transform_label_format, projection, random_sphere, is_probability, get_labels_np_array\n",
        "\n",
        "if TYPE_CHECKING:\n",
        "    from art.utils import CLASSIFIER_LOSS_GRADIENTS_TYPE\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class AutoProjectedGradientDescent_mod(EvasionAttack):\n",
        "    \"\"\"\n",
        "    Implementation of the `Auto Projected Gradient Descent` attack.\n",
        "    | Paper link: https://arxiv.org/abs/2003.01690\n",
        "    \"\"\"\n",
        "\n",
        "    attack_params = EvasionAttack.attack_params + [\n",
        "        \"norm\",\n",
        "        \"eps\",\n",
        "        \"eps_step\",\n",
        "        \"max_iter\",\n",
        "        \"targeted\",\n",
        "        \"nb_random_init\",\n",
        "        \"class_target\",\n",
        "        \"confidence\",        \n",
        "        \"batch_size\",\n",
        "        \"loss_type\",\n",
        "        \"verbose\",\n",
        "    ]\n",
        "    _estimator_requirements = (BaseEstimator, LossGradientsMixin, ClassifierMixin)\n",
        "    _predefined_losses = [None, \"cross_entropy\", \"difference_logits_ratio\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        estimator: \"CLASSIFIER_LOSS_GRADIENTS_TYPE\",\n",
        "        norm: Union[int, float, str] = np.inf,\n",
        "        eps: float = 0.3,\n",
        "        eps_step: float = 0.1,\n",
        "        max_iter: int = 100,\n",
        "        targeted: bool = False,\n",
        "        nb_random_init: int = 5,\n",
        "        class_target = 0,\n",
        "        confidence = 0.7,        \n",
        "        batch_size: int = 32,\n",
        "        loss_type: Optional[str] = None,\n",
        "        verbose: bool = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Create a :class:`.AutoProjectedGradientDescent` instance.\n",
        "        :param estimator: An trained estimator.\n",
        "        :param norm: The norm of the adversarial perturbation. Possible values: \"inf\", np.inf, 1 or 2.\n",
        "        :param eps: Maximum perturbation that the attacker can introduce.\n",
        "        :param eps_step: Attack step size (input variation) at each iteration.\n",
        "        :param max_iter: The maximum number of iterations.\n",
        "        :param targeted: Indicates whether the attack is targeted (True) or untargeted (False).\n",
        "        :param nb_random_init: Number of random initialisations within the epsilon ball. For num_random_init=0\n",
        "            starting at the original input.\n",
        "        :param class_target: classe da far predire.\n",
        "        :param confidence: probabilità minima con la quale predire class_target.            \n",
        "        :param batch_size: Size of the batch on which adversarial samples are generated.\n",
        "        :param loss_type: Defines the loss to attack. Available options: None (Use loss defined by estimator),\n",
        "            \"cross_entropy\", or \"difference_logits_ratio\"\n",
        "        :param verbose: Show progress bars.\n",
        "        \"\"\"\n",
        "        from art.estimators.classification import TensorFlowClassifier, TensorFlowV2Classifier, PyTorchClassifier\n",
        "\n",
        "        if loss_type not in self._predefined_losses:\n",
        "            raise ValueError(\n",
        "                \"The argument loss_type has an invalid value. The following options for `loss_type` are currently \"\n",
        "                \"supported: {}\".format(self._predefined_losses)\n",
        "            )\n",
        "\n",
        "        if loss_type is None:\n",
        "            if hasattr(estimator, \"predict\") and is_probability(\n",
        "                estimator.predict(x=np.ones(shape=(1, *estimator.input_shape), dtype=np.float32))\n",
        "            ):\n",
        "                raise ValueError(\n",
        "                    \"AutoProjectedGradientDescent is expecting logits as estimator output, the provided \"\n",
        "                    \"estimator seems to predict probabilities.\"\n",
        "                )\n",
        "\n",
        "            estimator_apgd = estimator\n",
        "        else:\n",
        "            if isinstance(estimator, TensorFlowClassifier):\n",
        "                import tensorflow as tf\n",
        "\n",
        "                if loss_type == \"cross_entropy\":\n",
        "                    if is_probability(estimator.predict(x=np.ones(shape=(1, *estimator.input_shape)))):\n",
        "                        raise NotImplementedError(\"Cross-entropy loss is not implemented for probability output.\")\n",
        "\n",
        "                    self._loss_object = tf.reduce_mean(\n",
        "                        tf.keras.losses.categorical_crossentropy(\n",
        "                            y_pred=estimator._output, y_true=estimator._labels_ph, from_logits=True\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                elif loss_type == \"difference_logits_ratio\":\n",
        "                    if is_probability(estimator.predict(x=np.ones(shape=(1, *estimator.input_shape)))):\n",
        "                        raise ValueError(\n",
        "                            \"The provided estimator seems to predict probabilities. \"\n",
        "                            \"If loss_type='difference_logits_ratio' the estimator has to to predict logits.\"\n",
        "                        )\n",
        "\n",
        "                    raise ValueError(\n",
        "                        \"The loss `difference_logits_ratio` has not been validate completely. It seems that the \"\n",
        "                        \"commented implemented below is failing to selected the second largest logit for cases \"\n",
        "                        \"where the largest logit is the true logit. For future work `difference_logits_ratio` and \"\n",
        "                        \"loss_fn should return the same loss value.\"\n",
        "                    )\n",
        "\n",
        "                    # def difference_logits_ratio(y_true, y_pred):\n",
        "                    #     i_y_true = tf.cast(tf.math.argmax(tf.cast(y_true, tf.int32), axis=1), tf.int32)\n",
        "                    #     i_y_pred_arg = tf.argsort(y_pred, axis=1)\n",
        "                    #     # Not completely sure if the following line is correct.\n",
        "                    #     # `i_y_pred_arg[:, -2], i_y_pred_arg[:, -1]` seems closer to the output of `loss_fn` than\n",
        "                    #     # `i_y_pred_arg[:, -1], i_y_pred_arg[:, -2]`\n",
        "                    #     i_z_i = tf.where(i_y_pred_arg[:, -1] != i_y_true[:], i_y_pred_arg[:, -2],\n",
        "                    #                      i_y_pred_arg[:, -1])\n",
        "                    #\n",
        "                    #     z_1 = tf.gather(y_pred, i_y_pred_arg[:, -1], axis=1, batch_dims=0)\n",
        "                    #     z_3 = tf.gather(y_pred, i_y_pred_arg[:, -3], axis=1, batch_dims=0)\n",
        "                    #     z_i = tf.gather(y_pred, i_z_i, axis=1, batch_dims=0)\n",
        "                    #     z_y = tf.gather(y_pred, i_y_true, axis=1, batch_dims=0)\n",
        "                    #\n",
        "                    #     z_1 = tf.linalg.diag_part(z_1)\n",
        "                    #     z_3 = tf.linalg.diag_part(z_3)\n",
        "                    #     z_i = tf.linalg.diag_part(z_i)\n",
        "                    #     z_y = tf.linalg.diag_part(z_y)\n",
        "                    #\n",
        "                    #     dlr = -(z_y - z_i) / (z_1 - z_3)\n",
        "                    #\n",
        "                    #     return tf.reduce_mean(dlr)\n",
        "                    #\n",
        "                    # def loss_fn(y_true, y_pred):\n",
        "                    #     i_y_true = np.argmax(y_true, axis=1)\n",
        "                    #     i_y_pred_arg = np.argsort(y_pred, axis=1)\n",
        "                    #     i_z_i = np.where(i_y_pred_arg[:, -1] != i_y_true[:], i_y_pred_arg[:, -1],\n",
        "                    #                      i_y_pred_arg[:, -2])\n",
        "                    #\n",
        "                    #     z_1 = y_pred[:, i_y_pred_arg[:, -1]]\n",
        "                    #     z_3 = y_pred[:, i_y_pred_arg[:, -3]]\n",
        "                    #     z_i = y_pred[:, i_z_i]\n",
        "                    #     z_y = y_pred[:, i_y_true]\n",
        "                    #\n",
        "                    #     z_1 = np.diag(z_1)\n",
        "                    #     z_3 = np.diag(z_3)\n",
        "                    #     z_i = np.diag(z_i)\n",
        "                    #     z_y = np.diag(z_y)\n",
        "                    #\n",
        "                    #     dlr = -(z_y - z_i) / (z_1 - z_3)\n",
        "                    #\n",
        "                    #     return np.mean(dlr)\n",
        "                    #\n",
        "                    # self._loss_fn = loss_fn\n",
        "                    # self._loss_object = difference_logits_ratio(y_true=estimator._labels_ph,\n",
        "                    #                                             y_pred=estimator._output)\n",
        "\n",
        "                estimator_apgd = TensorFlowClassifier(\n",
        "                    input_ph=estimator._input_ph,\n",
        "                    output=estimator._output,\n",
        "                    labels_ph=estimator._labels_ph,\n",
        "                    train=estimator._train,\n",
        "                    loss=self._loss_object,\n",
        "                    learning=estimator._learning,\n",
        "                    sess=estimator._sess,\n",
        "                    channels_first=estimator.channels_first,\n",
        "                    clip_values=estimator.clip_values,\n",
        "                    preprocessing_defences=estimator.preprocessing_defences,\n",
        "                    postprocessing_defences=estimator.postprocessing_defences,\n",
        "                    preprocessing=estimator.preprocessing,\n",
        "                    feed_dict=estimator._feed_dict,\n",
        "                )\n",
        "\n",
        "            elif isinstance(estimator, TensorFlowV2Classifier):\n",
        "                import tensorflow as tf\n",
        "\n",
        "                if loss_type == \"cross_entropy\":\n",
        "                    if is_probability(estimator.predict(x=np.ones(shape=(1, *estimator.input_shape)))):\n",
        "                        self._loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
        "                    else:\n",
        "                        self._loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "                elif loss_type == \"difference_logits_ratio\":\n",
        "                    if is_probability(estimator.predict(x=np.ones(shape=(1, *estimator.input_shape)))):\n",
        "                        raise ValueError(\n",
        "                            \"The provided estimator seems to predict probabilities. \"\n",
        "                            \"If loss_type='difference_logits_ratio' the estimator has to to predict logits.\"\n",
        "                        )\n",
        "\n",
        "                    class DifferenceLogitsRatioTensorFlowV2:\n",
        "                        \"\"\"\n",
        "                        Callable class for Difference Logits Ratio loss in TensorFlow v2.\n",
        "                        \"\"\"\n",
        "\n",
        "                        def __init__(self):\n",
        "                            self.reduction = \"mean\"\n",
        "\n",
        "                        def __call__(self, y_true, y_pred):\n",
        "                            i_y_true = tf.cast(tf.math.argmax(tf.cast(y_true, tf.int32), axis=1), tf.int32)\n",
        "                            i_y_pred_arg = tf.argsort(y_pred, axis=1)\n",
        "                            i_z_i_list = list()\n",
        "\n",
        "                            for i in range(y_true.shape[0]):\n",
        "                                if i_y_pred_arg[i, -1] != i_y_true[i]:\n",
        "                                    i_z_i_list.append(i_y_pred_arg[i, -1])\n",
        "                                else:\n",
        "                                    i_z_i_list.append(i_y_pred_arg[i, -2])\n",
        "\n",
        "                            i_z_i = tf.stack(i_z_i_list)\n",
        "\n",
        "                            z_1 = tf.gather(y_pred, i_y_pred_arg[:, -1], axis=1, batch_dims=0)\n",
        "                            z_3 = tf.gather(y_pred, i_y_pred_arg[:, -3], axis=1, batch_dims=0)\n",
        "                            z_i = tf.gather(y_pred, i_z_i, axis=1, batch_dims=0)\n",
        "                            z_y = tf.gather(y_pred, i_y_true, axis=1, batch_dims=0)\n",
        "\n",
        "                            z_1 = tf.linalg.diag_part(z_1)\n",
        "                            z_3 = tf.linalg.diag_part(z_3)\n",
        "                            z_i = tf.linalg.diag_part(z_i)\n",
        "                            z_y = tf.linalg.diag_part(z_y)\n",
        "\n",
        "                            dlr = -(z_y - z_i) / (z_1 - z_3)\n",
        "\n",
        "                            return tf.reduce_mean(dlr)\n",
        "\n",
        "                    self._loss_fn = DifferenceLogitsRatioTensorFlowV2()\n",
        "                    self._loss_object = DifferenceLogitsRatioTensorFlowV2()\n",
        "\n",
        "                estimator_apgd = TensorFlowV2Classifier(\n",
        "                    model=estimator.model,\n",
        "                    nb_classes=estimator.nb_classes,\n",
        "                    input_shape=estimator.input_shape,\n",
        "                    loss_object=self._loss_object,\n",
        "                    train_step=estimator._train_step,\n",
        "                    channels_first=estimator.channels_first,\n",
        "                    clip_values=estimator.clip_values,\n",
        "                    preprocessing_defences=estimator.preprocessing_defences,\n",
        "                    postprocessing_defences=estimator.postprocessing_defences,\n",
        "                    preprocessing=estimator.preprocessing,\n",
        "                )\n",
        "            elif isinstance(estimator, PyTorchClassifier):\n",
        "                import torch\n",
        "\n",
        "                if loss_type == \"cross_entropy\":\n",
        "                    if is_probability(\n",
        "                        estimator.predict(x=np.ones(shape=(1, *estimator.input_shape), dtype=np.float32))\n",
        "                    ):\n",
        "                        raise ValueError(\n",
        "                            \"The provided estimator seems to predict probabilities. If loss_type='cross_entropy' \"\n",
        "                            \"the estimator has to to predict logits.\"\n",
        "                        )\n",
        "\n",
        "                    self._loss_object = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "                elif loss_type == \"difference_logits_ratio\":\n",
        "                    if is_probability(\n",
        "                        estimator.predict(x=np.ones(shape=(1, *estimator.input_shape), dtype=ART_NUMPY_DTYPE))\n",
        "                    ):\n",
        "                        raise ValueError(\n",
        "                            \"The provided estimator seems to predict probabilities. \"\n",
        "                            \"If loss_type='difference_logits_ratio' the estimator has to to predict logits.\"\n",
        "                        )\n",
        "\n",
        "                    class DifferenceLogitsRatioPyTorch:\n",
        "                        \"\"\"\n",
        "                        Callable class for Difference Logits Ratio loss in PyTorch.\n",
        "                        \"\"\"\n",
        "\n",
        "                        def __init__(self):\n",
        "                            self.reduction = \"mean\"\n",
        "\n",
        "                        def __call__(self, y_pred, y_true):  # type: ignore\n",
        "                            if isinstance(y_true, np.ndarray):\n",
        "                                y_true = torch.from_numpy(y_true)\n",
        "                            if isinstance(y_pred, np.ndarray):\n",
        "                                y_pred = torch.from_numpy(y_pred)\n",
        "\n",
        "                            y_true = y_true.float()\n",
        "\n",
        "                            i_y_true = torch.argmax(y_true, axis=1)\n",
        "                            i_y_pred_arg = torch.argsort(y_pred, axis=1)\n",
        "                            i_z_i_list = list()\n",
        "\n",
        "                            for i in range(y_true.shape[0]):\n",
        "                                if i_y_pred_arg[i, -1] != i_y_true[i]:\n",
        "                                    i_z_i_list.append(i_y_pred_arg[i, -1])\n",
        "                                else:\n",
        "                                    i_z_i_list.append(i_y_pred_arg[i, -2])\n",
        "\n",
        "                            i_z_i = torch.stack(i_z_i_list)\n",
        "\n",
        "                            z_1 = y_pred[:, i_y_pred_arg[:, -1]]\n",
        "                            z_3 = y_pred[:, i_y_pred_arg[:, -3]]\n",
        "                            z_i = y_pred[:, i_z_i]\n",
        "                            z_y = y_pred[:, i_y_true]\n",
        "\n",
        "                            z_1 = torch.diagonal(z_1)\n",
        "                            z_3 = torch.diagonal(z_3)\n",
        "                            z_i = torch.diagonal(z_i)\n",
        "                            z_y = torch.diagonal(z_y)\n",
        "\n",
        "                            dlr = -(z_y - z_i) / (z_1 - z_3)\n",
        "\n",
        "                            return torch.mean(dlr.float())\n",
        "\n",
        "                    self._loss_object = DifferenceLogitsRatioPyTorch()\n",
        "\n",
        "                estimator_apgd = PyTorchClassifier(\n",
        "                    model=estimator.model,\n",
        "                    loss=self._loss_object,\n",
        "                    input_shape=estimator.input_shape,\n",
        "                    nb_classes=estimator.nb_classes,\n",
        "                    optimizer=None,\n",
        "                    channels_first=estimator.channels_first,\n",
        "                    clip_values=estimator.clip_values,\n",
        "                    preprocessing_defences=estimator.preprocessing_defences,\n",
        "                    postprocessing_defences=estimator.postprocessing_defences,\n",
        "                    preprocessing=estimator.preprocessing,\n",
        "                    device_type=str(estimator._device),\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"The loss type {} is not supported for the provided estimator.\".format(loss_type))\n",
        "\n",
        "        super().__init__(estimator=estimator_apgd)\n",
        "        self.norm = norm\n",
        "        self.eps = eps\n",
        "        self.eps_step = eps_step\n",
        "        self.max_iter = max_iter\n",
        "        self.targeted = targeted\n",
        "        self.nb_random_init = nb_random_init\n",
        "        self.class_target=class_target\n",
        "        self.confidence=confidence        \n",
        "        self.batch_size = batch_size\n",
        "        self.loss_type = loss_type\n",
        "        self.verbose = verbose\n",
        "        self._check_params()\n",
        "\n",
        "    def generate(self,mask_mod, x: np.ndarray, y: Optional[np.ndarray] = None, enh=False, **kwargs) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Generate adversarial samples and return them in an array.\n",
        "        :param x: An array with the original inputs.\n",
        "        :param y: Target values (class labels) one-hot-encoded of shape `(nb_samples, nb_classes)` or indices of shape\n",
        "                  (nb_samples,). Only provide this parameter if you'd like to use true labels when crafting adversarial\n",
        "                  samples. Otherwise, model predictions are used as labels to avoid the \"label leaking\" effect\n",
        "                  (explained in this paper: https://arxiv.org/abs/1611.01236). Default is `None`.\n",
        "        :param mask: An array with a mask broadcastable to input `x` defining where to apply adversarial perturbations.\n",
        "                     Shape needs to be broadcastable to the shape of x and can also be of the same shape as `x`. Any\n",
        "                     features for which the mask is zero will not be adversarially perturbed.\n",
        "        :type mask: `np.ndarray`\n",
        "        :return: An array holding the adversarial examples.\n",
        "        \"\"\"\n",
        "        mask = kwargs.get(\"mask\")\n",
        "\n",
        "        y = check_and_transform_label_format(y, self.estimator.nb_classes)\n",
        "\n",
        "        if y is None:\n",
        "            if self.targeted:\n",
        "                raise ValueError(\"Target labels `y` need to be provided for a targeted attack.\")\n",
        "            y = get_labels_np_array(self.estimator.predict(x, batch_size=self.batch_size)).astype(np.int32)\n",
        "\n",
        "        if self.estimator.nb_classes == 2 and y.shape[1] == 1:\n",
        "            raise ValueError(\n",
        "                \"This attack has not yet been tested for binary classification with a single output classifier.\"\n",
        "            )\n",
        "\n",
        "        x_adv = x.astype(ART_NUMPY_DTYPE)\n",
        "        size_init=np.array(x.shape[2:4])\n",
        "        transf_orig=transforms.Resize(size=(size_init[0],size_init[1]),interpolation=InterpolationMode.NEAREST)\n",
        "\n",
        "        #questo for serve per i riavvi random, quindi se l'immagine non è spoof spoof è inutile\n",
        "        for t in trange(max(1, self.nb_random_init), desc=\"AutoPGD - restart\", disable=not self.verbose):\n",
        "            # Determine correctly predicted samples\n",
        "            #y_pred = self.estimator.predict(x_adv)\n",
        "\n",
        "                #print(\"preds=label e preds=spoof\")\n",
        "              \n",
        "              \n",
        "                #print(\"preds=label, preds=live e confidence<0.6\")\n",
        "                \n",
        "              \n",
        "\n",
        "            #sample_is_robust è vettore di booleani, indica se l'attacco deve essere eseguito (True), ovvero se il modello predice bene\n",
        "            \n",
        "            #print(\"y: \"+str(y))\n",
        "            #print(\"pred: \"+str(preds))\n",
        "            #print(\"sampleisrobust: \"+str(sample_is_robust))\n",
        "            \n",
        "            #recap: se non è spoof spoof uso break per uscire dal for e restituisco l'immagine originale, altrimenti continuo e faccio resize dell'immagine\n",
        "            if t==0:\n",
        "              x_adv=transf_resize(torch.Tensor(x_adv))\n",
        "              #mask_mod=compute_mask(x_adv)\n",
        "              x_adv=np.array(x_adv)\n",
        "\n",
        "            x_robust = x_adv\n",
        "            y_robust = y\n",
        "            x_init = np.array(transf_resize(torch.Tensor(x)))\n",
        "            #print(\"x_robust inizio\")\n",
        "            #cv2_imshow(x_robust[0].transpose(1,2,0)*255)\n",
        "           \n",
        "            if self.nb_random_init!=0:\n",
        "              n = x_robust.shape[0] #numero di immagini (1)\n",
        "              m = np.prod(x_robust.shape[1:]).item()\n",
        "              random_perturbation = (\n",
        "                  random_sphere(n, m, self.eps, self.norm).reshape(x_robust.shape).astype(ART_NUMPY_DTYPE)\n",
        "              )*(mask_mod.astype(ART_NUMPY_DTYPE))\n",
        "              #dovresti trasformare random_pert in gray e applicare anche la maschera\n",
        "              random_perturbation = 0.2989 * random_perturbation[0,0,:,:] + 0.5870 * random_perturbation[0,1,:,:] + 0.1140 * random_perturbation[0,2,:,:]\n",
        "              for i in range(3):\n",
        "                x_robust[0,i,:,:] = x_robust[0,i,:,:] + random_perturbation\n",
        "              #x_robust = x_robust + random_perturbation #applica una perturbazione random, penso per generare un nuovo x di partenza diverso da x_init\n",
        "\n",
        "              #effettua clipping dei valori\n",
        "              if self.estimator.clip_values is not None:\n",
        "                  clip_min, clip_max = self.estimator.clip_values\n",
        "                  x_robust = np.clip(x_robust, clip_min, clip_max)\n",
        "              #print(\"x_robust dopo random_perturbation\")\n",
        "              #cv2_imshow(x_robust[0].transpose(1,2,0)*255)\n",
        "              #aggiusta la perturbazione con projection, infatti le passa x_robust-x_init che sarebbe la perturbazione totale applicata da x_init nelle varie iterazioni\n",
        "              perturbation = projection(x_robust - x_init, self.eps, self.norm)#*(mask_mod.astype(ART_NUMPY_DTYPE))\n",
        "              perturbation = 0.2989 * perturbation[0,0,:,:] + 0.5870 * perturbation[0,1,:,:] + 0.1140 * perturbation[0,2,:,:]\n",
        "              for i in range(3):\n",
        "                x_robust[0,i,:,:] = x_init[0,i,:,:] + perturbation\n",
        "              #somma la perturbazione aggiustata a x_init per generare un nuovo x_robust e ricominciare\n",
        "              #x_robust = x_init + perturbation\n",
        "              #print(\"x_robust dopo perturbazione proiettata\")\n",
        "              #cv2_imshow(x_robust[0].transpose(1,2,0)*255)\n",
        "            # Compute perturbation with implicit batching\n",
        "              \n",
        "              if enh: x_robust=enhanc(x_robust,mask_mod)\n",
        "            \n",
        "            #for per ogni batch (viene eseguito una volta)\n",
        "            for batch_id in trange(\n",
        "                int(np.ceil(x_robust.shape[0] / float(self.batch_size))),\n",
        "                desc=\"AutoPGD - batch\",\n",
        "                leave=False,\n",
        "                disable=not self.verbose,\n",
        "            ):\n",
        "                self.eta = 2 * self.eps_step\n",
        "                batch_index_1, batch_index_2 = batch_id * self.batch_size, (batch_id + 1) * self.batch_size\n",
        "                x_k = x_robust[batch_index_1:batch_index_2].astype(ART_NUMPY_DTYPE)\n",
        "                x_init_batch = x_init[batch_index_1:batch_index_2].astype(ART_NUMPY_DTYPE)\n",
        "                y_batch = y_robust[batch_index_1:batch_index_2]\n",
        "\n",
        "                p_0 = 0\n",
        "                p_1 = 0.22\n",
        "                var_w = [p_0, p_1]\n",
        "\n",
        "                while True:\n",
        "                    p_j_p_1 = var_w[-1] + max(var_w[-1] - var_w[-2] - 0.03, 0.06)\n",
        "                    if p_j_p_1 > 1:\n",
        "                        break\n",
        "                    var_w.append(p_j_p_1)\n",
        "\n",
        "                var_w = [math.ceil(p * self.max_iter) for p in var_w]\n",
        "\n",
        "                eta = self.eps_step\n",
        "                self.count_condition_1 = 0\n",
        "                \n",
        "                #for per ogni iterazione fino a max_iter\n",
        "                for k_iter in trange(self.max_iter, desc=\"AutoPGD - iteration\", leave=False, disable=not self.verbose):\n",
        "                    # Get perturbation, use small scalar to avoid division by 0\n",
        "                    tol = 10e-8\n",
        "                    #cv2_imshow(x_k[0].transpose(1,2,0)*255)\n",
        "                    # Get gradient wrt loss; invert it if attack is targeted\n",
        "                    grad = self.estimator.loss_gradient(np.array(trans_norm(torch.Tensor(x_k))), y_batch) * (1 - 2 * int(self.targeted))\n",
        "\n",
        "                    # Apply norm bound\n",
        "                    if self.norm in [np.inf, \"inf\"]:\n",
        "                        grad = np.sign(grad)\n",
        "                    elif self.norm == 1:\n",
        "                        ind = tuple(range(1, len(x_k.shape)))\n",
        "                        grad = grad / (np.sum(np.abs(grad), axis=ind, keepdims=True) + tol)\n",
        "                    elif self.norm == 2:\n",
        "                        ind = tuple(range(1, len(x_k.shape)))\n",
        "                        grad = grad / (np.sqrt(np.sum(np.square(grad), axis=ind, keepdims=True)) + tol)\n",
        "                    assert x_k.shape == grad.shape\n",
        "                    #calcola la perturbazione in maniera simile a FGM\n",
        "                    perturbation = grad\n",
        "                    \n",
        "                    #applica la maschera alla perturbazione\n",
        "                    #if mask is not None:\n",
        "                    #    perturbation = perturbation * (mask.astype(ART_NUMPY_DTYPE))\n",
        "                    #print(\"pert prima mask\")\n",
        "                    #cv2_imshow(perturbation[0].transpose(1,2,0)*255)\n",
        "                    perturbation = perturbation * (mask_mod.astype(ART_NUMPY_DTYPE))\n",
        "                    #print(\"pert dopo mask\")\n",
        "                    #cv2_imshow(perturbation[0].transpose(1,2,0)*255)\n",
        "                    perturbation = 0.2989 * perturbation[0,0,:,:] + 0.5870 * perturbation[0,1,:,:] + 0.1140 * perturbation[0,2,:,:]\n",
        "                    #for i in range(3):\n",
        "                      #z_k_p_1[0,i,:,:] = x_k[0,i,:,:] + eta * perturbation\n",
        "                    # Apply perturbation and clip\n",
        "                    z_k_p_1 = x_k + eta * perturbation\n",
        "\n",
        "                    if self.estimator.clip_values is not None:\n",
        "                        clip_min, clip_max = self.estimator.clip_values\n",
        "                        z_k_p_1 = np.clip(z_k_p_1, clip_min, clip_max)\n",
        "                    \n",
        "                    #print(\"z_k_p_1 dopo perturbazione\")\n",
        "                    #cv2_imshow(z_k_p_1[0].transpose(1,2,0)*255)\n",
        "                    \n",
        "                    if k_iter == 0:\n",
        "                        x_1 = z_k_p_1\n",
        "                        perturbation = projection(x_1 - x_init_batch, self.eps, self.norm)\n",
        "                        perturbation = 0.2989 * perturbation[0,0,:,:] + 0.5870 * perturbation[0,1,:,:] + 0.1140 * perturbation[0,2,:,:]\n",
        "                        for i in range(3):\n",
        "                          x_1[0,i,:,:] = x_init_batch[0,i,:,:] + perturbation\n",
        "                        #x_1 = x_init_batch + perturbation\n",
        "                        #print(\"x_1 dopo perturbazione\")\n",
        "                        #cv2_imshow(x_1[0].transpose(1,2,0)*255)\n",
        "                        f_0 = self.estimator.compute_loss(x=np.array(trans_norm(torch.Tensor(x_k))), y=y_batch, reduction=\"mean\")\n",
        "                        f_1 = self.estimator.compute_loss(x=np.array(trans_norm(torch.Tensor(x_1))), y=y_batch, reduction=\"mean\")\n",
        "\n",
        "                        self.eta_w_j_m_1 = eta\n",
        "                        self.f_max_w_j_m_1 = f_0\n",
        "\n",
        "                        if f_1 >= f_0:\n",
        "                            self.f_max = f_1\n",
        "                            self.x_max = x_1\n",
        "                            self.x_max_m_1 = x_init_batch\n",
        "                            self.count_condition_1 += 1\n",
        "                        else:\n",
        "                            self.f_max = f_0\n",
        "                            self.x_max = x_k.copy()\n",
        "                            self.x_max_m_1 = x_init_batch\n",
        "\n",
        "                        # Settings for next iteration k\n",
        "                        x_k_m_1 = x_k.copy()\n",
        "                        x_k = x_1\n",
        "\n",
        "                    else:\n",
        "                        perturbation = projection(z_k_p_1 - x_init_batch, self.eps, self.norm)\n",
        "                        perturbation = 0.2989 * perturbation[0,0,:,:] + 0.5870 * perturbation[0,1,:,:] + 0.1140 * perturbation[0,2,:,:]\n",
        "                        for i in range(3):\n",
        "                          z_k_p_1[0,i,:,:] = x_init_batch[0,i,:,:] + perturbation\n",
        "                        #z_k_p_1 = x_init_batch + perturbation\n",
        "\n",
        "                        alpha = 0.75\n",
        "\n",
        "                        x_k_p_1 = x_k + alpha * (z_k_p_1 - x_k) + (1 - alpha) * (x_k - x_k_m_1)\n",
        "\n",
        "                        if self.estimator.clip_values is not None:\n",
        "                            clip_min, clip_max = self.estimator.clip_values\n",
        "                            x_k_p_1 = np.clip(x_k_p_1, clip_min, clip_max)\n",
        "\n",
        "                        perturbation = projection(x_k_p_1 - x_init_batch, self.eps, self.norm)\n",
        "                        perturbation = 0.2989 * perturbation[0,0,:,:] + 0.5870 * perturbation[0,1,:,:] + 0.1140 * perturbation[0,2,:,:]\n",
        "                        for i in range(3):\n",
        "                          x_k_p_1[0,i,:,:] = x_init_batch[0,i,:,:] + perturbation\n",
        "                        #x_k_p_1 = x_init_batch + perturbation\n",
        "                        #print(\"x_k_p_1\")\n",
        "                        #cv2_imshow(x_k_p_1[0].transpose(1,2,0)*255)\n",
        "                        f_k_p_1 = self.estimator.compute_loss(x=np.array(trans_norm(torch.Tensor(x_k_p_1))), y=y_batch, reduction=\"mean\")\n",
        "\n",
        "                        #f_k_p_1 è il valore della loss function e quantifica di quanto sbaglia il classificatore, quindi se è 0 interrompo le iterazioni e cambio punto di partenza\n",
        "                        \n",
        "                        if enh:  x_k_p_1=enhanc(x_k_p_1,mask_mod)\n",
        "                        \n",
        "                        pred,value,_=test_average(self.estimator,torch.Tensor(x_k_p_1),transf_orig)\n",
        "                        if ((pred==np.argmax(y, axis=1)) and (pred!= self.class_target)):\n",
        "                          sample_is_not_robust=False\n",
        "                        elif ((pred!=np.argmax(y, axis=1)) and (pred== self.class_target) and (np.max(value)<self.confidence)):  \n",
        "                          sample_is_not_robust=False\n",
        "                        else:  sample_is_not_robust=True\n",
        "                        if sample_is_not_robust:\n",
        "                        \n",
        "                        #if f_k_p_1 == 0.0: \n",
        "                            x_k = x_k_p_1.copy()\n",
        "                            break\n",
        "                        #se ho trovato un'adv che dà pertida maggiore mi salvo questo valore, queste cose servono a cambiare dinamicamente il passo eta\n",
        "                        if (not self.targeted and f_k_p_1 > self.f_max) or (self.targeted and f_k_p_1 < self.f_max):\n",
        "                            self.count_condition_1 += 1\n",
        "                            self.x_max = x_k_p_1\n",
        "                            self.x_max_m_1 = x_k\n",
        "                            self.f_max = f_k_p_1\n",
        "\n",
        "                        if k_iter in var_w:\n",
        "\n",
        "                            rho = 0.75\n",
        "\n",
        "                            condition_1 = self.count_condition_1 < rho * (k_iter - var_w[var_w.index(k_iter) - 1])\n",
        "                            condition_2 = self.eta_w_j_m_1 == eta and self.f_max_w_j_m_1 == self.f_max\n",
        "\n",
        "                            if condition_1 or condition_2:\n",
        "                                eta = eta / 2\n",
        "                                x_k_m_1 = self.x_max_m_1\n",
        "                                x_k = self.x_max\n",
        "                            else:\n",
        "                                x_k_m_1 = x_k\n",
        "                                x_k = x_k_p_1.copy()\n",
        "\n",
        "                            self.count_condition_1 = 0\n",
        "                            self.eta_w_j_m_1 = eta\n",
        "                            self.f_max_w_j_m_1 = self.f_max\n",
        "\n",
        "                        else:\n",
        "                            x_k_m_1 = x_k\n",
        "                            x_k = x_k_p_1.copy()\n",
        "\n",
        "                if enh: x_k=enhanc(x_k,mask_mod)\n",
        "                \n",
        "                pred,value,_=test_average(self.estimator,torch.Tensor(x_k),transf_orig)\n",
        "                #y_pred_adv_k = self.estimator.predict(x_k)\n",
        "                if self.targeted:\n",
        "                    sample_is_not_robust_k = np.invert(np.argmax(y_pred_adv_k, axis=1) != np.argmax(y_batch, axis=1))\n",
        "                elif not self.targeted:\n",
        "                    #sample_is_not_robust_k = np.invert(preds == np.argmax(y_batch, axis=1))\n",
        "                    if ((pred==np.argmax(y, axis=1)) and (pred!= self.class_target)):\n",
        "                      sample_is_not_robust=False\n",
        "                    elif ((pred!=np.argmax(y, axis=1)) and (pred== self.class_target) and (np.max(value)<self.confidence)):  \n",
        "                      sample_is_not_robust=False\n",
        "                    else:  sample_is_not_robust=True\n",
        "                #print(\"x_k fine for iter\")\n",
        "                #cv2_imshow(x_k[0].transpose(1,2,0)*255)\n",
        "                if sample_is_not_robust:\n",
        "                  x_robust[batch_index_1:batch_index_2] = x_k #carica x_robust con x_k solo se preds!=label\n",
        "\n",
        "            x_adv = x_robust\n",
        "\n",
        "        return x_adv\n",
        "\n",
        "    def _check_params(self) -> None:\n",
        "        if self.norm not in [1, 2, np.inf, \"inf\"]:\n",
        "            raise ValueError('The argument norm has to be either 1, 2, np.inf, or \"inf\".')\n",
        "\n",
        "        if not isinstance(self.eps, (int, float)) or self.eps <= 0.0:\n",
        "            raise ValueError(\"The argument eps has to be either of type int or float and larger than zero.\")\n",
        "\n",
        "        if not isinstance(self.eps_step, (int, float)) or self.eps_step <= 0.0:\n",
        "            raise ValueError(\"The argument eps_step has to be either of type int or float and larger than zero.\")\n",
        "\n",
        "        if not isinstance(self.max_iter, int) or self.max_iter <= 0:\n",
        "            raise ValueError(\"The argument max_iter has to be of type int and larger than zero.\")\n",
        "\n",
        "        if not isinstance(self.targeted, bool):\n",
        "            raise ValueError(\"The argument targeted has to be of bool.\")\n",
        "\n",
        "        if not isinstance(self.nb_random_init, int) or self.nb_random_init < 0:\n",
        "            raise ValueError(\"The argument nb_random_init has to be of type int and larger (or equal) than zero.\")\n",
        "\n",
        "        if not isinstance(self.batch_size, int) or self.batch_size <= 0:\n",
        "            raise ValueError(\"The argument batch_size has to be of type int and larger than zero.\")\n",
        "\n",
        "        if self.loss_type not in self._predefined_losses:\n",
        "            raise ValueError(\"The argument loss_type has to be either {}.\".format(self._predefined_losses))\n",
        "\n",
        "        if not isinstance(self.verbose, bool):\n",
        "            raise ValueError(\"The argument `verbose` has to be of type bool.\")\n",
        "\n",
        "        if isinstance(self.class_target,int):\n",
        "            if self.class_target!=0 and self.class_target!=1:\n",
        "              raise ValueError(\"Attacco destinato alle impronte (0,1)\")\n",
        "        else: \n",
        "          raise ValueError(\"Class_target deve essere intero\")   \n",
        "\n",
        "        if isinstance(self.confidence,float):\n",
        "            if self.confidence <=0 or self.confidence >1:\n",
        "              raise ValueError(\"confidence deve essere compreso tra 0 e 1\")\n",
        "        else: \n",
        "          raise ValueError(\"confidence deve essere float\")            "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-a_DqwJOULQ"
      },
      "source": [
        "##DeepFool mod"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeswXkxiOULQ"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import logging\n",
        "from typing import Optional, TYPE_CHECKING\n",
        "\n",
        "import numpy as np\n",
        "from tqdm.auto import trange\n",
        "\n",
        "from art.config import ART_NUMPY_DTYPE\n",
        "from art.estimators.estimator import BaseEstimator\n",
        "from art.estimators.classification.classifier import ClassGradientsMixin\n",
        "from art.attacks.attack import EvasionAttack\n",
        "from art.utils import compute_success, is_probability\n",
        "\n",
        "if TYPE_CHECKING:\n",
        "    from art.utils import CLASSIFIER_CLASS_LOSS_GRADIENTS_TYPE\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class DeepFool_mod(EvasionAttack):\n",
        "    \"\"\"\n",
        "    Implementation of the attack from Moosavi-Dezfooli et al. (2015).\n",
        "    | Paper link: https://arxiv.org/abs/1511.04599\n",
        "    \"\"\"\n",
        "\n",
        "    attack_params = EvasionAttack.attack_params + [\n",
        "        \"max_iter\",\n",
        "        \"epsilon\",\n",
        "        \"nb_grads\",\n",
        "        \"class_target\",\n",
        "        \"confidence\",\n",
        "        \"max_over\",\n",
        "        \"batch_size\",\n",
        "        \"verbose\",\n",
        "    ]\n",
        "    _estimator_requirements = (BaseEstimator, ClassGradientsMixin)\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        classifier: \"CLASSIFIER_CLASS_LOSS_GRADIENTS_TYPE\",\n",
        "        max_iter: int = 100,\n",
        "        epsilon: float = 1e-6,\n",
        "        nb_grads: int = 10,\n",
        "        class_target = 0,\n",
        "        confidence = 0.7,\n",
        "        max_over = 1,\n",
        "        batch_size: int = 1,\n",
        "        verbose: bool = True,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Create a DeepFool attack instance.\n",
        "        :param classifier: A trained classifier.\n",
        "        :param max_iter: The maximum number of iterations.\n",
        "        :param epsilon: Overshoot parameter.\n",
        "        :param nb_grads: The number of class gradients (top nb_grads w.r.t. prediction) to compute. This way only the\n",
        "                         most likely classes are considered, speeding up the computation.\n",
        "        :param class_target: classe da far predire.\n",
        "        :param confidence: probabilità minima con la quale predire class_target                         \n",
        "        :param max_over: numero massimo di volte che può applicare il parametro overshoot\n",
        "        :param batch_size: Batch size\n",
        "        :param verbose: Show progress bars.\n",
        "        \"\"\"\n",
        "        super().__init__(estimator=classifier)\n",
        "        self.max_iter = max_iter\n",
        "        self.epsilon = epsilon\n",
        "        self.nb_grads = nb_grads\n",
        "        self.class_target=class_target\n",
        "        self.confidence=confidence\n",
        "        self.batch_size = batch_size\n",
        "        self.max_over = max_over\n",
        "        self.verbose = verbose\n",
        "        self._check_params()\n",
        "        if self.estimator.clip_values is None:\n",
        "            logger.warning(\n",
        "                \"The `clip_values` attribute of the estimator is `None`, therefore this instance of DeepFool will by \"\n",
        "                \"default generate adversarial perturbations scaled for input values in the range [0, 1] but not clip \"\n",
        "                \"the adversarial example.\"\n",
        "            )\n",
        "\n",
        "    def generate(self, mask_mod, x: np.ndarray, y: Optional[np.ndarray] = None, enh=False, **kwargs) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Generate adversarial samples and return them in an array.\n",
        "        :param x: An array with the original inputs to be attacked.\n",
        "        :param y: An array with the original labels to be predicted.\n",
        "        :return: An array holding the adversarial examples.\n",
        "        \"\"\"\n",
        "        x_adv = x.astype(ART_NUMPY_DTYPE)\n",
        "\n",
        "        size_init=np.array(x.shape[2:4])\n",
        "        transf_orig=transforms.Resize(size=(size_init[0],size_init[1]),interpolation=InterpolationMode.NEAREST)\n",
        "\n",
        "        #preds = self.estimator.predict(x, batch_size=self.batch_size)\n",
        "\n",
        "        class_pred,prob_preds,preds=test_average(self.estimator,torch.Tensor(x_adv),transf_orig)\n",
        "        preds=np.array(preds).reshape(1,2)\n",
        "\n",
        "        if self.estimator.nb_classes == 2 and preds.shape[1] == 1:\n",
        "            raise ValueError(\n",
        "                \"This attack has not yet been tested for binary classification with a single output classifier.\"\n",
        "            )\n",
        "\n",
        "        if is_probability(preds[0]):\n",
        "            logger.warning(\n",
        "                \"It seems that the attacked model is predicting probabilities. DeepFool expects logits as model output \"\n",
        "                \"to achieve its full attack strength.\"\n",
        "            )\n",
        "\n",
        "        # Determine the class labels for which to compute the gradients\n",
        "        use_grads_subset = self.nb_grads < self.estimator.nb_classes\n",
        "        if use_grads_subset:\n",
        "            # TODO compute set of unique labels per batch\n",
        "            grad_labels = np.argsort(-preds, axis=1)[:, : self.nb_grads]\n",
        "            labels_set = np.unique(grad_labels)\n",
        "        else:\n",
        "            labels_set = np.arange(self.estimator.nb_classes)\n",
        "        sorter = np.arange(len(labels_set))\n",
        "\n",
        "        # Pick a small scalar to avoid division by 0\n",
        "        tol = 10e-8\n",
        "\n",
        "        if ((class_pred==np.argmax(y, axis=1)) and (class_pred!= self.class_target)):\n",
        "            active=True\n",
        "        elif ((class_pred!=np.argmax(y, axis=1)) and (class_pred== self.class_target) and (np.max(prob_preds)<self.confidence)): \n",
        "            active=True\n",
        "        else:  active=False\n",
        "\n",
        "        if active: \n",
        "            x_adv=transf_resize(torch.Tensor(x_adv))\n",
        "            #mask_mod=compute_mask(x_adv)\n",
        "            x_adv=np.array(x_adv)\n",
        "\n",
        "            x_init=transf_resize(torch.Tensor(x.astype(ART_NUMPY_DTYPE)))\n",
        "            x_init=np.array(x_init)\n",
        "\n",
        "        # Compute perturbation with implicit batching\n",
        "        for batch_id in trange(\n",
        "            int(np.ceil(x_adv.shape[0] / float(self.batch_size))), desc=\"DeepFool\", disable=not self.verbose\n",
        "        ):\n",
        "            batch_index_1, batch_index_2 = batch_id * self.batch_size, (batch_id + 1) * self.batch_size\n",
        "            batch = x_adv[batch_index_1:batch_index_2].copy()\n",
        "\n",
        "            # Get predictions and gradients for batch\n",
        "            #f_batch = preds[batch_index_1:batch_index_2] #predizioni\n",
        "            #fk_hat = np.argmax(f_batch, axis=1) #classe predetta\n",
        "            \n",
        "            batch_grd=np.array(trans_norm(torch.Tensor(batch)))\n",
        "            if use_grads_subset:\n",
        "                # Compute gradients only for top predicted classes\n",
        "                grd = np.array([self.estimator.class_gradient(batch_grd, label=_) for _ in labels_set])\n",
        "                grd = np.squeeze(np.swapaxes(grd, 0, 2), axis=0)\n",
        "            else:\n",
        "                # Compute gradients for all classes\n",
        "                grd = self.estimator.class_gradient(batch_grd)\n",
        "\n",
        "            # Get current predictions\n",
        "\n",
        "            #active_indices = np.arange(len(batch))\n",
        "            current_step = 0\n",
        "\n",
        "            while active==True and current_step < self.max_iter:\n",
        "                # Compute difference in predictions and gradients only for selected top predictions\n",
        "                labels_indices = sorter[np.searchsorted(labels_set, class_pred, sorter=sorter)]\n",
        "                grad_diff = (grd - grd[np.arange(len(grd)), labels_indices][:, None])*mask_mod\n",
        "                f_diff = preds[:,labels_set] - preds[np.arange(len(preds)), labels_indices][:, None]\n",
        "\n",
        "                # Choose coordinate and compute perturbation\n",
        "                norm = np.linalg.norm(grad_diff.reshape(len(grad_diff), len(labels_set), -1), axis=2) + tol\n",
        "                value = np.abs(f_diff) / norm\n",
        "                value[np.arange(len(value)), labels_indices] = np.inf\n",
        "                l_var = np.argmin(value, axis=1)\n",
        "                absolute1 = abs(f_diff[np.arange(len(f_diff)), l_var])\n",
        "                draddiff = grad_diff[np.arange(len(grad_diff)), l_var].reshape(len(grad_diff), -1)\n",
        "                pow1 = (\n",
        "                    pow(\n",
        "                        np.linalg.norm(draddiff, axis=1),\n",
        "                        2,\n",
        "                    )\n",
        "                    + tol\n",
        "                )\n",
        "                r_var = absolute1 / pow1\n",
        "                r_var = r_var.reshape((-1,) + (1,) * (len(x.shape) - 1))\n",
        "                r_var = r_var * grad_diff[np.arange(len(grad_diff)), l_var]\n",
        "                #print(\"pert\")\n",
        "                #plt.imshow(r_var[0].transpose(1,2,0)*255)\n",
        "                #plt.show()\n",
        "                r, g, b = r_var[0,0,:,:],r_var[0,1,:,:],r_var[0,2,:,:]\n",
        "                r_var = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
        "\n",
        "                # Add perturbation and clip result\n",
        "                if self.estimator.clip_values is not None:\n",
        "                    batch = np.clip(\n",
        "                        batch\n",
        "                        + r_var * (self.estimator.clip_values[1] - self.estimator.clip_values[0]),\n",
        "                        self.estimator.clip_values[0],\n",
        "                        self.estimator.clip_values[1]\n",
        "                    )\n",
        "                else:\n",
        "                    batch += r_var\n",
        "                \n",
        "                batch=batch.astype(np.single)\n",
        "                if enh: batch=enhanc(batch,mask_mod)\n",
        "                # Recompute prediction for new x\n",
        "                class_pred_i,prob_preds,preds=test_average(self.estimator,torch.Tensor(batch),transf_orig)\n",
        "                preds=np.array(preds).reshape(1,2)\n",
        "                #f_batch = self.estimator.predict(batch)\n",
        "                #fk_i_hat = np.argmax(f_batch, axis=1)\n",
        "                \n",
        "                batch_grd=np.array(trans_norm(torch.Tensor(batch)))\n",
        "                # Recompute gradients for new x\n",
        "                if use_grads_subset:\n",
        "                    # Compute gradients only for (originally) top predicted classes\n",
        "                    grd = np.array([self.estimator.class_gradient(batch_grd, label=_) for _ in labels_set])\n",
        "                    grd = np.squeeze(np.swapaxes(grd, 0, 2), axis=0)\n",
        "                else:\n",
        "                    # Compute gradients for all classes\n",
        "                    grd = self.estimator.class_gradient(batch_grd)\n",
        "\n",
        "                # Stop if misclassification has been achieved\n",
        "                #active_indices = np.where(class_pred_i == class_pred)[0]\n",
        "                if ((class_pred_i==np.argmax(y, axis=1)) and (class_pred_i!= self.class_target)):\n",
        "                  active=True\n",
        "                elif ((class_pred_i!=np.argmax(y, axis=1)) and (class_pred_i== self.class_target) and (np.max(prob_preds)<self.confidence)): \n",
        "                  active=True\n",
        "                else:  active=False\n",
        "\n",
        "                current_step += 1\n",
        "            \n",
        "            ov_it=0\n",
        "            while active and ov_it<self.max_over:\n",
        "              x_adv2 = (1 + self.epsilon) * (batch - x_init)            \n",
        "              batch = x_init + x_adv2\n",
        "              if self.estimator.clip_values is not None:\n",
        "                  np.clip(\n",
        "                      batch,\n",
        "                      self.estimator.clip_values[0],\n",
        "                      self.estimator.clip_values[1],\n",
        "                      out=batch,\n",
        "                  )   \n",
        "              if enh: batch=enhanc(batch,mask_mod)\n",
        "              class_pred,prob_preds,preds=test_average(self.estimator,torch.Tensor(batch),transf_orig)\n",
        "              if ((class_pred==np.argmax(y, axis=1)) and (class_pred!= self.class_target)):\n",
        "                  active=True\n",
        "              elif ((class_pred!=np.argmax(y, axis=1)) and (class_pred== self.class_target) and (np.max(prob_preds)<self.confidence)): \n",
        "                  active=True\n",
        "              else:  active=False    \n",
        "              ov_it+=1\n",
        "              \n",
        "            x_adv=batch\n",
        "        '''      \n",
        "        logger.info(\n",
        "            \"Success rate of DeepFool attack: %.2f%%\",\n",
        "            100 * compute_success(self.estimator, x, y, x_adv, batch_size=self.batch_size),\n",
        "        )\n",
        "        '''\n",
        "        return x_adv\n",
        "\n",
        "    def _check_params(self) -> None:\n",
        "        if not isinstance(self.max_iter, (int, np.int)) or self.max_iter <= 0:\n",
        "            raise ValueError(\"The number of iterations must be a positive integer.\")\n",
        "\n",
        "        if not isinstance(self.nb_grads, (int, np.int)) or self.nb_grads <= 0:\n",
        "            raise ValueError(\"The number of class gradients to compute must be a positive integer.\")\n",
        "\n",
        "        if self.epsilon < 0:\n",
        "            raise ValueError(\"The overshoot parameter must not be negative.\")\n",
        "\n",
        "        if self.batch_size <= 0:\n",
        "            raise ValueError(\"The batch size `batch_size` has to be positive.\")\n",
        "\n",
        "        if not isinstance(self.verbose, bool):\n",
        "            raise ValueError(\"The argument `verbose` has to be of type bool.\")\n",
        "\n",
        "        if isinstance(self.class_target,int):\n",
        "            if self.class_target!=0 and self.class_target!=1:\n",
        "              raise ValueError(\"Attacco destinato alle impronte (0,1)\")\n",
        "        else: \n",
        "          raise ValueError(\"Class_target deve essere intero\")   \n",
        "\n",
        "        if isinstance(self.confidence,float):\n",
        "            if self.confidence <=0 or self.confidence >1:\n",
        "              raise ValueError(\"confidence deve essere compreso tra 0 e 1\")\n",
        "        else: \n",
        "          raise ValueError(\"confidence deve essere float\")\n",
        "\n",
        "        if isinstance(self.max_over,int):\n",
        "            if self.max_over <=0:\n",
        "              raise ValueError(\"max_over deve essere positivo\")\n",
        "        else: \n",
        "          raise ValueError(\"confidence deve essere int\")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2z5CDqCOjhi"
      },
      "source": [
        "# **Esecuzione**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzZkC7HimlHT"
      },
      "source": [
        "##Creazione classificatore ART Pytorch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZL7c94zmlHU"
      },
      "source": [
        "from art.estimators.classification import PyTorchClassifier\n",
        "\n",
        "classifier = PyTorchClassifier(\n",
        "    model=model,\n",
        "    clip_values=(_min,_max),\n",
        "    loss=nn.CrossEntropyLoss(),\n",
        "    optimizer=optim.Adam(model.classifier.parameters(),lr=1e-5),\n",
        "    input_shape=(3, 224, 224), #3,224,224 per APGD\n",
        "    nb_classes=class_number\n",
        ")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m10EkCq5GeEu"
      },
      "source": [
        "##Attacco"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "2yk7RwELGfM4",
        "outputId": "9eb0012c-b075-4f78-d526-d9e5ebfc32c1"
      },
      "source": [
        "from art.utils import to_categorical\n",
        "from PIL import Image, ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "import random\n",
        "import time\n",
        "\n",
        "# FGM\n",
        "#attack = IterativeFastGradientMethod(estimator=classifier, eps=10000,eps_step=0.002,class_target=classes_name.index('Live'),max_iter=30,confidence=0.8) #j=5\n",
        "#usa solo en dentro attacco e questi parametri\n",
        "attack = AutoProjectedGradientDescent_mod(classifier,eps=0.5,eps_step=0.3,nb_random_init=30,max_iter=100,class_target=classes_name.index('Live'),confidence=0.8,verbose=False) #j=5\n",
        "\n",
        "#attack = DeepFool_mod(classifier, epsilon=0.1,max_iter=50,nb_grads=2,class_target=classes_name.index('Live'),confidence=0.8,max_over=10,verbose=False) #aumenta max_over, non alzare iter perché già così impiega quasi 5 min per 10 img\n",
        "\n",
        "x_print=[]\n",
        "y_print=[]\n",
        "x_print_adv=[] \n",
        "y_print_adv=[] \n",
        "perturb_print=[]\n",
        "Test_p = pd.DataFrame()\n",
        "# starting time\n",
        "start = time.time()\n",
        "i=0\n",
        "for input,label in testgen:\n",
        "  i+=1\n",
        "\n",
        "  sys.stdout.write(\"\\rElem: {0}/{1}\".format(i,len(testgen)))\n",
        "  sys.stdout.flush()\n",
        "\n",
        "  y_test=np.array((to_categorical(label.numpy(),2)))\n",
        "  x_test=np.array(input.numpy())\n",
        "  pred,probabilities,_=test_average(classifier,torch.Tensor(x_test),transf_init)\n",
        "  if label==1 and pred==1: #solo se la predice spoof\n",
        "    filename=test_dataset.samples[i-1][0].split('Spoof/')[1]\n",
        "    m=compute_mask(torch.Tensor(x_test),dim_contours=8,n_rect=1)\n",
        "    #cv2_imshow(x_test[0].transpose(1,2,0)*255)\n",
        "    #cv2_imshow(m*255)\n",
        "    nois=(random_noise(np.ones([500,500]), mode='pepper',amount=0.2))+np.random.random([500,500])\n",
        "    nois=np.where(m==0,1,nois)\n",
        "    x_test_p=enhanc(x_test,m)\n",
        "    pred,probabilities,_=test_average(classifier,torch.Tensor(x_test_p),transf_init)\n",
        "    #cv2_imshow(x_test_p[0].transpose(1,2,0)*255)\n",
        "    j=0\n",
        "    m_at = cv2.resize(m, (224,224),interpolation=cv2.INTER_NEAREST)\n",
        "    while (j<20 and (probabilities[0]<0.8 or probabilities_n[0]<0.5)):\n",
        "      j+=1\n",
        "      #print(j)\n",
        "      #x_test_adv=attack.generate(mask=m_at,x=x_test_p,y=y_test,enh=True) #IFGSM\n",
        "      x_test_adv=attack.generate(mask_mod=m_at,x=x_test_p,y=y_test,enh=False) #APGD e DeepFool\n",
        "      x_test_adv=enhanc(x_test_adv,m,[size_init[0],size_init[1]])\n",
        "      #cv2_imshow(x_test_adv[0].transpose(1,2,0)*255)\n",
        "      x_test_p=x_test_adv.copy()\n",
        "      x_test_adv=torch.Tensor(x_test_adv)\n",
        "      x_nois=(x_test_adv*nois).type(torch.float)\n",
        "      x_nois=np.clip(x_nois,0,1)\n",
        "      #x_test_adv=transf_init(x_test_adv)\n",
        "      pred,probabilities,_=test_average(classifier,x_test_adv,transf_init)\n",
        "      _,probabilities_n,_=test_average(classifier,x_nois,transf_init)\n",
        "      #print(probabilities,probabilities_n)\n",
        "\n",
        "    if pred==0:\n",
        "      cv2.imwrite('/content/gdrive/MyDrive/GreenBit_attacks/APGD_final_latex_noise/'+filename,x_test_adv[0].numpy().transpose(1,2,0)*255)\n",
        "      \n",
        "      Test_p = Test_p.append({'name': filename,\n",
        "                              'predicted': classes_name[pred], #qui metti pred\n",
        "                              'real': classes_name[int(label)] ,\n",
        "                              'prob_live': round(probabilities[0]*100,4),\n",
        "                              'prob_live_noise': round(probabilities_n[0]*100,4)},ignore_index = True)\n",
        "\n",
        "# end time\n",
        "end = time.time()\n",
        "\n",
        "\n",
        "# total time taken\n",
        "import datetime\n",
        "time_exe=int(end - start)\n",
        "time_exe=datetime.timedelta(seconds=time_exe)\n",
        "\n",
        "print(f\"\\nRuntime of the program is {time_exe}\")"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elem: 98/250"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-4e4b7e11c844>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0;31m#x_test_adv=attack.generate(mask=m_at,x=x_test_p,y=y_test,enh=True) #IFGSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0mx_test_adv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_mod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm_at\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_test_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#APGD e DeepFool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m       \u001b[0mx_test_adv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menhanc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_adv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msize_init\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize_init\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m       \u001b[0;31m#cv2_imshow(x_test_adv[0].transpose(1,2,0)*255)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0mx_test_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_test_adv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-9a973ca313dc>\u001b[0m in \u001b[0;36menhanc\u001b[0;34m(img, mask, size_out)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#inserire controllo se size_out<350\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfailed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinger_en\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menhance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_g\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0msize_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-61c46cd4775f>\u001b[0m in \u001b[0;36menhance\u001b[0;34m(self, img, resize, size)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__ridge_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# compute major frequency of ridges\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mean_freq\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__ridge_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# filter the image using oriented gabor filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m           \u001b[0mfailed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfailed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-61c46cd4775f>\u001b[0m in \u001b[0;36m__ridge_filter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0morientindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                     \u001b[0morientindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morientindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmaxorientindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0morientindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxorientindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                     \u001b[0morientindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morientindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmaxorientindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mfinalind_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalind_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinalind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBxk_cn3uHGZ",
        "outputId": "4b22105b-ce59-490b-a35b-956082cf0a8a"
      },
      "source": [
        "print(Test_p)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          name predicted  prob_live  prob_live_noise   real\n",
            "0   GreenBit_Latex_004_0_0.png      Live    89.1416          74.1433  Spoof\n",
            "1   GreenBit_Latex_004_0_9.png      Live    90.7081          91.9949  Spoof\n",
            "2   GreenBit_Latex_004_1_0.png      Live    98.5771          68.2952  Spoof\n",
            "3   GreenBit_Latex_004_2_0.png      Live    99.7957          99.7948  Spoof\n",
            "4   GreenBit_Latex_004_2_4.png      Live    98.9206          89.3632  Spoof\n",
            "..                         ...       ...        ...              ...    ...\n",
            "59  GreenBit_Latex_009_6_4.png      Live    94.5767          73.5702  Spoof\n",
            "60  GreenBit_Latex_009_6_9.png      Live    92.1916          92.1999  Spoof\n",
            "61  GreenBit_Latex_009_7_9.png      Live    93.4964          95.8814  Spoof\n",
            "62  GreenBit_Latex_009_8_4.png      Live    92.9026          98.2279  Spoof\n",
            "63  GreenBit_Latex_009_8_9.png      Live    93.3135          93.5338  Spoof\n",
            "\n",
            "[64 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cht3bMJT6PjK"
      },
      "source": [
        "salvataggio dataframe su drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfoCwGQZvxng"
      },
      "source": [
        "Test_p.to_excel(\"/content/gdrive/MyDrive/GreenBit_attacks/apgd_final_latex_noise.xlsx\",index=False)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFKEoI-enb8L"
      },
      "source": [
        "##Testing\n",
        "\n",
        "> Trasforma i due testing in una funzione e richiamala due volte\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MmzfXhFt40W"
      },
      "source": [
        "###Print accuracy test orig from file & load pd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSOr1z85vhl7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bde07eb4-f221-487e-996b-9bb86ec09503"
      },
      "source": [
        "Test_orig=pd.read_excel(pd_preds)\n",
        "print_acc(Test_orig) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 95.56\n",
            "Shape dataframe: (2500, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRmjrhHYsVsN"
      },
      "source": [
        "###Testing immagini spoof predette live prima e dopo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGuElPEOW3dZ"
      },
      "source": [
        "####Accuracy adv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp0jAIC-W5zC",
        "outputId": "faa2f47e-58a5-45db-f656-29b76d668d09"
      },
      "source": [
        "print_acc(Test_p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.0\n",
            "Shape dataframe: (4, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyLoc2K6V_ar"
      },
      "source": [
        "####Spoof"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0SfSelFK2dl"
      },
      "source": [
        "prima"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsNqIDAxsZBA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1584be7b-be58-4b44-cc31-1991dff04b5c"
      },
      "source": [
        "accuracy_class('Spoof',Test_orig)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# imgSpoof: 1500\n",
            "Numero di predizioni Spoof giuste: 1390\n",
            "Accuracy : 92.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AciVwgFyK3Xc"
      },
      "source": [
        "dopo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2xU5gUQuy7y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f932ac36-b17a-4909-a8d0-33b45fa9d1d6"
      },
      "source": [
        "accuracy_class('Spoof',Test_p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# imgSpoof: 4\n",
            "Numero di predizioni Spoof giuste: 4\n",
            "Accuracy : 100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4Q78IIqWCTI"
      },
      "source": [
        "####Live"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtPKGqzEWCTI"
      },
      "source": [
        "prima"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtB5Te35WCTJ",
        "outputId": "d445ae85-eba8-41b9-d12c-b192a69dc17c"
      },
      "source": [
        "accuracy_class('Live',Test_orig)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# imgLive: 1000\n",
            "Numero di predizioni Live giuste: 842\n",
            "Accuracy : 84.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3VNm1BCWCTJ"
      },
      "source": [
        "dopo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssG-LK1zWCTJ",
        "outputId": "4d77bf2f-7083-47f2-b4c5-b5a0ed6b3305"
      },
      "source": [
        "accuracy_class('Live',Test_p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# imgLive: 0\n",
            "Numero di predizioni Live giuste: 0\n",
            "Accuracy : nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in long_scalars\n",
            "  del sys.path[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMl35zPf3mlm"
      },
      "source": [
        "####numero img spoof predette live con prob >70%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSN-zFU23r6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b74e42d5-f0a2-42d9-f5e6-7cc3dd70cb86"
      },
      "source": [
        "p=Test_p.loc[Test_p['real']=='Spoof']\n",
        "p=p.loc[p['predicted']=='Live']\n",
        "print(\"img spoof-live: \"+str(p.count(0)[0]))\n",
        "p=p.loc[p['prob_live']>=0.7]\n",
        "print(\"img con prob_live>70%: \"+str(p.count(0)[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img spoof-live: 48\n",
            "img con prob_live>70%: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppH0s3Ov618F"
      },
      "source": [
        "##subplot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr0m-v0meeS2"
      },
      "source": [
        "###test finale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSs9SSpU7xgV"
      },
      "source": [
        "#da usare nel test finale\n",
        "preds_orig=Test_orig[1000:].predicted.values[list_print]\n",
        "print(preds_orig)\n",
        "print_subplot(perturb_print,x_print,y_print,preds_orig,x_print_adv,y_print_adv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGRODgf-egr4"
      },
      "source": [
        "###dataset ridotto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFV6X6RZ6ZA7"
      },
      "source": [
        "#da usare solo con dataset ridotto\n",
        "preds_orig=[]\n",
        "for input,_ in testgen:\n",
        "  preds_orig_i,_,_=test_average(classifier,input,transf_init)\n",
        "  preds_orig.append(preds_orig_i)\n",
        "preds_orig=np.array(preds_orig)\n",
        "#preds_orig=preds_orig[list_print]\n",
        "preds_orig=np.array([classes_name[i] for i in preds_orig])\n",
        "print_subplot(perturb_print,x_print,y_print,preds_orig,x_print_adv,y_print_adv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-bsx7fXWquY"
      },
      "source": [
        "#salvataggio e lettura immagine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnV-dDcP3euu",
        "outputId": "9fe6fe6f-7ed2-4813-a8b2-5c78c95b6921"
      },
      "source": [
        "#for i in range(len(list_print)):  \n",
        "save_read(np.array(x_print_adv[9]),classifier,transf_init)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valori img originale:\n",
            "[0.00126301 0.99873704]\n",
            "valori dopo salvataggio/lettura:\n",
            "[0.00126301 0.99873704]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amasebQU62zK"
      },
      "source": [
        "#Predizioni per immagini da drive prelevate singolarmente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fimc3_LklhIf",
        "outputId": "6bb30aac-536f-4114-ab31-2bada718c7f4"
      },
      "source": [
        "import os\n",
        "Test_cagliari = pd.DataFrame()\n",
        "path_images=\"/content/gdrive/MyDrive/HiScan_attacks/APGD_final_latex/\"\n",
        "directory = os.listdir(path_images)\n",
        "i=0\n",
        "for input in directory:\n",
        "  i+=1\n",
        "  sys.stdout.write(\"\\rElem: {0}/{1}\".format(i,len(directory)))\n",
        "  sys.stdout.flush()\n",
        "  img=cv2.imread(path_images+input)\n",
        "  img=(img-np.min(img))/(np.max(img)-np.min(img))\n",
        "  pred,probabilities,_=test_average(classifier,torch.Tensor(img.transpose(2,0,1)).unsqueeze_(0),transf_init)\n",
        "  Test_cagliari = Test_cagliari.append({'name': input,\n",
        "                                        'predicted': classes_name[pred], #qui metti pred\n",
        "                                        'prob_live': probabilities[0]*100,},ignore_index = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elem: 218/218"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyJRn0weykPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcef71cd-e62e-4629-e6d8-5032340e8b7a"
      },
      "source": [
        "p=Test_cagliari.loc[Test_cagliari['prob_live']>80]\n",
        "print(p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                         name predicted  prob_live\n",
            "0    HiScan_Latex_004_0_0.bmp      Live  89.755082\n",
            "1    HiScan_Latex_004_0_9.bmp      Live  93.422091\n",
            "3    HiScan_Latex_004_1_4.bmp      Live  89.812499\n",
            "4    HiScan_Latex_004_1_5.bmp      Live  91.027677\n",
            "5    HiScan_Latex_004_1_9.bmp      Live  99.936897\n",
            "..                        ...       ...        ...\n",
            "213  HiScan_Latex_020_8_4.bmp      Live  82.766956\n",
            "214  HiScan_Latex_020_8_9.bmp      Live  81.339920\n",
            "215  HiScan_Latex_020_9_0.bmp      Live  95.871395\n",
            "216  HiScan_Latex_020_9_4.bmp      Live  83.924711\n",
            "217  HiScan_Latex_020_9_9.bmp      Live  96.365345\n",
            "\n",
            "[216 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCuC3NThyA64"
      },
      "source": [
        "Test_cagliari.to_excel(\"/content/gdrive/MyDrive/HiScan_attacks/apgd_final_latex.xlsx\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path_init=\"/content/gdrive/MyDrive/Dataset_cagliari/deepfool_adv_enh/\"\n",
        "directory = os.listdir(path_init) #lista di Fi_gb\n",
        "i=0\n",
        "for dir in directory: #Fi_gb\n",
        "  i+=1\n",
        "  sys.stdout.write(\"\\rElem: {0}/{1}\".format(i,len(directory)))\n",
        "  sys.stdout.flush()  \n",
        "  Test_cagliari = pd.DataFrame()\n",
        "  directory1=os.listdir(path_init+dir)\n",
        "  for dir1 in directory1: #98 e 99\n",
        "    dir_path=os.listdir(path_init+dir+'/'+dir1+'/Live')\n",
        "    for dir_in in dir_path: #cartelle con varie img (pres e secc)\n",
        "      dir_path_in=os.listdir(path_init+dir+'/'+dir1+'/Live/'+dir_in) #lista immagini (pres e secc)\n",
        "      path_in=path_init+dir+'/'+dir1+'/Live/'+dir_in #path della singola cartella con immagini\n",
        "      for input in dir_path_in: #singole immagini\n",
        "        if input.endswith('.bmp'): \n",
        "          img=cv2.imread(path_in+'/'+input)\n",
        "          img=(img-np.min(img))/(np.max(img)-np.min(img))\n",
        "          #img_tens=torch.Tensor(img.transpose(2,0,1)).unsqueeze_(0)\n",
        "          #m=compute_mask(img_tens,n_contours=6)\n",
        "          #img_en=enhanc(np.array(img_tens),m,size_init[0])\n",
        "          pred,probabilities,_=test_average(classifier,torch.Tensor(img.transpose(2,0,1)).unsqueeze_(0),transf_init)\n",
        "          Test_cagliari = Test_cagliari.append({'name': (dir1+'/'+dir_in+'/'+input),\n",
        "                                                'predicted': classes_name[pred], #qui metti pred\n",
        "                                                'prob_live': probabilities[0]*100,},ignore_index = True)\n",
        "  Test_cagliari.to_excel(path_init+dir+\"/\"+dir+\".xlsx\",index=False)  \n",
        "  print(Test_cagliari)"
      ],
      "metadata": {
        "id": "5i4KmzJ5zllq",
        "outputId": "5f262de0-2e2c-4214-a4a1-185afe41527c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elem: 1/3                                        name predicted     prob_live\n",
            "0    99_99_0/secc_M_05/FLAT_MIDDLE_RIGHT.bmp     Spoof  1.273985e-05\n",
            "1     99_99_0/secc_M_05/FLAT_THUMB_RIGHT.bmp     Spoof  3.652939e-05\n",
            "2      99_99_0/secc_M_05/FLAT_RING_RIGHT.bmp     Spoof  6.116795e-07\n",
            "3     99_99_0/secc_M_05/FLAT_MIDDLE_LEFT.bmp     Spoof  2.640037e-05\n",
            "4     99_99_0/secc_M_05/FLAT_LITTLE_LEFT.bmp     Spoof  6.133923e-04\n",
            "..                                       ...       ...           ...\n",
            "345   98_99_0/pres_A_02/FLAT_LITTLE_LEFT.bmp     Spoof  5.399979e-02\n",
            "346     98_99_0/pres_A_02/FLAT_RING_LEFT.bmp     Spoof  6.092159e-04\n",
            "347  98_99_0/pres_A_02/FLAT_MIDDLE_RIGHT.bmp     Spoof  4.596200e-02\n",
            "348   98_99_0/pres_A_02/FLAT_INDEX_RIGHT.bmp     Spoof  7.932430e-04\n",
            "349    98_99_0/pres_A_02/FLAT_INDEX_LEFT.bmp     Spoof  9.735674e-02\n",
            "\n",
            "[350 rows x 3 columns]\n",
            "Elem: 2/3                                        name predicted  prob_live\n",
            "0     99_99_0/secc_M_06/FLAT_THUMB_RIGHT.bmp      Live  99.991333\n",
            "1      99_99_0/secc_M_06/FLAT_THUMB_LEFT.bmp      Live  99.669313\n",
            "2      99_99_0/secc_M_06/FLAT_RING_RIGHT.bmp      Live  99.471337\n",
            "3     99_99_0/secc_M_06/FLAT_MIDDLE_LEFT.bmp      Live  99.912316\n",
            "4     99_99_0/secc_M_06/FLAT_INDEX_RIGHT.bmp      Live  99.994159\n",
            "..                                       ...       ...        ...\n",
            "275    98_99_0/pres_M_03/FLAT_THUMB_LEFT.bmp     Spoof  33.775350\n",
            "276  98_99_0/pres_M_03/FLAT_LITTLE_RIGHT.bmp      Live  97.985154\n",
            "277   98_99_0/pres_M_03/FLAT_LITTLE_LEFT.bmp     Spoof  23.859881\n",
            "278    98_99_0/pres_M_03/FLAT_INDEX_LEFT.bmp      Live  98.101002\n",
            "279   98_99_0/pres_M_03/FLAT_INDEX_RIGHT.bmp      Live  83.814812\n",
            "\n",
            "[280 rows x 3 columns]\n",
            "Elem: 3/3                                        name predicted  prob_live\n",
            "0     99_99_0/secc_M_02/FLAT_THUMB_RIGHT.bmp     Spoof   0.139539\n",
            "1      99_99_0/secc_M_02/FLAT_THUMB_LEFT.bmp     Spoof   0.000528\n",
            "2    99_99_0/secc_M_02/FLAT_MIDDLE_RIGHT.bmp     Spoof   3.137937\n",
            "3       99_99_0/secc_M_02/FLAT_RING_LEFT.bmp     Spoof   0.005663\n",
            "4     99_99_0/secc_M_02/FLAT_MIDDLE_LEFT.bmp     Spoof   5.429535\n",
            "..                                       ...       ...        ...\n",
            "345   98_99_0/pres_A_01/FLAT_LITTLE_LEFT.bmp     Spoof   0.011055\n",
            "346   98_99_0/pres_A_01/FLAT_INDEX_RIGHT.bmp     Spoof  25.826198\n",
            "347  98_99_0/pres_A_01/FLAT_MIDDLE_RIGHT.bmp     Spoof   2.367166\n",
            "348   98_99_0/pres_A_01/FLAT_MIDDLE_LEFT.bmp     Spoof   0.115378\n",
            "349    98_99_0/pres_A_01/FLAT_INDEX_LEFT.bmp     Spoof   1.101665\n",
            "\n",
            "[350 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Test_cagliari)"
      ],
      "metadata": {
        "id": "GM83rNa30JRw",
        "outputId": "9ae20f05-34a3-4cfe-b2e7-9887470ff5dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        name predicted  prob_live\n",
            "0     99_99_0/secc_M_06/FLAT_THUMB_RIGHT.bmp     Spoof   7.679517\n",
            "1      99_99_0/secc_M_06/FLAT_THUMB_LEFT.bmp      Live  79.577732\n",
            "2      99_99_0/secc_M_06/FLAT_RING_RIGHT.bmp     Spoof   5.606416\n",
            "3       99_99_0/secc_M_06/FLAT_RING_LEFT.bmp     Spoof   0.399980\n",
            "4    99_99_0/secc_M_06/FLAT_MIDDLE_RIGHT.bmp     Spoof   3.000792\n",
            "..                                       ...       ...        ...\n",
            "345   98_99_0/secc_A_02/FLAT_MIDDLE_LEFT.bmp     Spoof   0.000004\n",
            "346   98_99_0/secc_A_02/FLAT_LITTLE_LEFT.bmp     Spoof   0.001643\n",
            "347  98_99_0/secc_A_02/FLAT_LITTLE_RIGHT.bmp     Spoof   0.000013\n",
            "348   98_99_0/secc_A_02/FLAT_INDEX_RIGHT.bmp     Spoof   0.001396\n",
            "349    98_99_0/secc_A_02/FLAT_INDEX_LEFT.bmp     Spoof   0.002245\n",
            "\n",
            "[350 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path_init=\"/content/gdrive/MyDrive/Dataset_cagliari/deepfool_adv_enh/\"\n",
        "directory = os.listdir(path_init) #lista di Fi_gb\n",
        "i=0\n",
        "for dir in directory: #Fi_gb\n",
        "  i+=1\n",
        "  sys.stdout.write(\"\\rElem: {0}/{1}\".format(i,len(directory)))\n",
        "  sys.stdout.flush()  \n",
        "  Test_cagliari=pd.read_excel(path_init+dir+'/'+dir+'.xlsx')\n",
        "  c=Test_cagliari[Test_cagliari['predicted']=='Live'].count(0)[0]\n",
        "  print(dir)\n",
        "  print(\"#live: \",c)"
      ],
      "metadata": {
        "id": "TGBQ7Cec-CZC",
        "outputId": "2c18e240-1630-4256-eccc-04563d0373c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elem: 1/3F2_gb_deepfool_latex\n",
            "#live:  24\n",
            "Elem: 2/3F3_gb_deepfool_latex\n",
            "#live:  108\n",
            "Elem: 3/3F1_gb_deepfool_latex\n",
            "#live:  60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Studio noise"
      ],
      "metadata": {
        "id": "bG4vOrYFMJC7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "fun test first 5 crop"
      ],
      "metadata": {
        "id": "cWuxCnSiMc68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_average_crop(classifier,input,transf_init): #testing normale ma c indica quante tra le 5 patch principali vengono classificate bene\n",
        "  '''\n",
        "  model: model trained\n",
        "  test_loader: dataloader \n",
        "  return: pred: classe predetta, probabilities, values\n",
        "  '''\n",
        "\n",
        "  def calc_size(n):\n",
        "    '''\n",
        "    n: int \n",
        "    return: 80% of n\n",
        "    '''\n",
        "    return tuple(int(np.ceil(i * (80/100))) for i in n)\n",
        "\n",
        "  preds=[]\n",
        "  #value=[]\n",
        "  prob=nn.Softmax(dim=0)\n",
        "  \n",
        "  if input.shape[2]==224:\n",
        "    input=transf_init(input)\n",
        "\n",
        "  n=input.shape\n",
        "  n_mod=calc_size(n[2:4])\n",
        "  crop_transform=transforms.TenCrop((n_mod[0],n_mod[1]))\n",
        "  crops=crop_transform(input)\n",
        "  live=0\n",
        "  spoof=0\n",
        "  i=0\n",
        "  crop_pred=0\n",
        "  for crop in crops:\n",
        "    i+=1\n",
        "    crop=data_transform_test(crop) #resize 224\n",
        "    #cv2_imshow(np.array(crop[0]).transpose(1,2,0)*255)\n",
        "    #cv2.imwrite('patch_print/print_patch'+str(i)+'.bmp',np.array(crop[0]).transpose(1,2,0)*255)\n",
        "    #print(torch.min(crop),torch.max(crop))\n",
        "    outputs = classifier.predict(crop)\n",
        "    if i in range(1,6):\n",
        "      m=crop.shape\n",
        "      m_mod=calc_size(m[2:4])\n",
        "      crop_transform2=transforms.TenCrop((m_mod[0],m_mod[1]))\n",
        "      crops2=crop_transform2(crop)\n",
        "      live2=0\n",
        "      spoof2=0\n",
        "      for crop2 in crops2:\n",
        "        crop2=data_transform_test(crop2) #resize 224      \n",
        "        outputs2 = classifier.predict(crop2)\n",
        "        live2+=outputs[0][0]\n",
        "        spoof2+=outputs[0][1]\n",
        "      live2=live2/10\n",
        "      spoof2=spoof2/10\n",
        "      crop_pred+=1-np.argmax([live2,spoof2])\n",
        "    live+=outputs[0][0]\n",
        "    spoof+=outputs[0][1]\n",
        "  live=live/10\n",
        "  spoof=spoof/10\n",
        "  values=[live,spoof]\n",
        "  predicted=np.argmax(values)\n",
        "  probabilities=prob(torch.Tensor(values)).numpy()\n",
        "\n",
        "\n",
        "\n",
        "  return predicted,probabilities,values,crop_pred"
      ],
      "metadata": {
        "id": "kdZzEX_pMcer"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from skimage.util import random_noise\n",
        "Test_a = pd.DataFrame()\n",
        "path_images=\"/content/gdrive/MyDrive/GreenBit_attacks/APGD_final/\"\n",
        "directory = os.listdir(path_images)\n",
        "i=0\n",
        "for input in directory:\n",
        "  i+=1\n",
        "  sys.stdout.write(\"\\rElem: {0}/{1}\".format(i,len(directory)))\n",
        "  sys.stdout.flush()\n",
        "  img=cv2.imread(path_images+input)\n",
        "  #cv2_imshow(img)\n",
        "  img=(img-np.min(img))/(np.max(img)-np.min(img))\n",
        "  img=torch.Tensor(img.transpose(2,0,1)).unsqueeze_(0)\n",
        "  m=compute_mask(img,dim_contours=8,n_rect=1)\n",
        "  nois=(random_noise(np.ones([500,500]), mode='pepper',amount=0.1))+np.random.random([500,500])\n",
        "  nois=np.where(m==0,1,nois)\n",
        "  img=(img*nois).type(torch.float)\n",
        "  img=np.clip(img,0,1)\n",
        "  #cv2_imshow(np.array(img[0]).transpose(1,2,0)*255)\n",
        "  pred,probabilities,_,c=test_average_crop(classifier,img,transf_init)\n",
        "  Test_a = Test_a.append({'name': input,\n",
        "                          'predicted': classes_name[pred], #qui metti pred\n",
        "                          'prob_live': probabilities[0]*100,\n",
        "                          'n_crop': c},ignore_index = True)\n",
        "  #56 0.2 +rand\n",
        "  #34 0.1 senza +rand\n",
        "  #88 0.1 + rand"
      ],
      "metadata": {
        "id": "KBc-JbPVMKjm",
        "outputId": "fc25a21a-c554-4e3b-b12b-46d5a50ae06e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elem: 210/210"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Test_a[Test_a['predicted']=='Live'])"
      ],
      "metadata": {
        "id": "LfoHNrgKQ_2r",
        "outputId": "75c09d0e-7b23-497b-82bc-98286b874bdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     n_crop                        name predicted  prob_live\n",
            "0       3.0  GreenBit_Latex_004_0_0.png      Live  66.558051\n",
            "3       3.0  GreenBit_Latex_004_1_0.png      Live  50.522900\n",
            "4       2.0  GreenBit_Latex_004_1_4.png      Live  62.631047\n",
            "5       3.0  GreenBit_Latex_004_2_0.png      Live  86.717063\n",
            "7       4.0  GreenBit_Latex_004_3_0.png      Live  86.014158\n",
            "..      ...                         ...       ...        ...\n",
            "199     4.0  GreenBit_Latex_020_6_4.png      Live  64.098930\n",
            "204     5.0  GreenBit_Latex_020_8_0.png      Live  99.259984\n",
            "205     2.0  GreenBit_Latex_020_8_4.png      Live  90.737259\n",
            "206     4.0  GreenBit_Latex_020_8_9.png      Live  94.378382\n",
            "208     5.0  GreenBit_Latex_020_9_4.png      Live  97.280598\n",
            "\n",
            "[88 rows x 4 columns]\n"
          ]
        }
      ]
    }
  ]
}