{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Addestramento_scanner_Pytorch(Test_media)_255.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nGmEBN4oxqUO",
        "pIqUXUmoA541",
        "-ycpjmyb4Cxe",
        "dherpFOH0Uau",
        "O4hHndtkosDV",
        "ZazQQE8ypKP9",
        "3OpXAJ1SpiwI",
        "9XepXK-ny3XN",
        "PBSyijWDqY4-",
        "O_c2L3v0r2xG",
        "VRhT4XM1sVm-",
        "kJ6xDO5GGZY5",
        "tFFWkmjX2j5P"
      ],
      "mount_file_id": "1LFvCjg8zmdzf9GGORH3M3sLCejtQ0qUI",
      "authorship_tag": "ABX9TyPXSX87+QjELV/U4eN9kp99",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umbertogagl97/Tesi/blob/main/Impronte/Addestramento_scanner_Pytorch(Test_media)_255.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcoLiAGAtqBh"
      },
      "source": [
        "# **Init**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGmEBN4oxqUO"
      },
      "source": [
        "## Import librerie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01RXI-DDIb3C"
      },
      "source": [
        "#Librerie\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import copy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIqUXUmoA541"
      },
      "source": [
        "##Check device\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2pe5Zh2A4Ui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eaca5ff-7ce0-4888-fa81-9b5bffd21b58"
      },
      "source": [
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.9.0+cu111\n",
            "True\n",
            "Tesla P100-PCIE-16GB\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ycpjmyb4Cxe"
      },
      "source": [
        "##Def path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hMuySVI4GgY"
      },
      "source": [
        "#scanner\n",
        "scanner_name = 'HiScan'\n",
        "\n",
        "#salvataggio modello\n",
        "path_model_save = F\"/content/gdrive/My Drive/ModelliCNN/Scanner/Final/{scanner_name}_255\"\n",
        "\n",
        "#dataset\n",
        "pathDataset=F'/content/gdrive/MyDrive/Dataset_impronte/training/{scanner_name}_patch'\n",
        "pathTestset=F'/content/gdrive/MyDrive/Dataset_impronte/test/{scanner_name}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dherpFOH0Uau"
      },
      "source": [
        "##Collegamento google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyvTOQw-aHRP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a5e5f5c-e158-487f-ca9f-4195bc6d4ecc"
      },
      "source": [
        "#collegamento google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X10jetEyAax"
      },
      "source": [
        "#**Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4b2eLxnouZ8"
      },
      "source": [
        "##Caricamento dataset e creazione Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS-Uzs8BjD4R"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "data_transform=transforms.Compose([transforms.Resize([224,224]),\n",
        "        transforms.ToTensor(),\n",
        "        lambda x: (x*255).type(torch.uint8)\n",
        "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "\n",
        "train_dataset = datasets.ImageFolder(pathDataset,transform=data_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxWBDGmnjSge",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "940cebfb-1f2c-4b34-c629-d4430dac02e5"
      },
      "source": [
        "print(len(train_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZazQQE8ypKP9"
      },
      "source": [
        "##Nomi classi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYk4cr4eo9Ir",
        "outputId": "d1d13ef6-b496-4f3a-8b17-cba3b4c81763"
      },
      "source": [
        "classes_name=train_dataset.classes\n",
        "class_number=len(classes_name)\n",
        "print(classes_name)\n",
        "print(class_number)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Live', 'Spoof']\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XepXK-ny3XN"
      },
      "source": [
        "# **Creazione modello**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBSyijWDqY4-"
      },
      "source": [
        "##Load model pre-trained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GymaBXhaPMvL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d802aba8-8b17-4d2c-e40f-46ba266cfba6"
      },
      "source": [
        "#model_conv = models.densenet201(pretrained=True,progress=True)\n",
        "model_conv = models.vgg19(pretrained=True,progress=True)\n",
        "for param in model_conv.parameters():\n",
        "  param.requires_grad = False #non modifico i parametri della parte riguardante le features durante il training, poiché voglio usare quella parte così com'è stata pre-addestrata\n",
        "\n",
        "print(model_conv)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): ReLU(inplace=True)\n",
            "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (24): ReLU(inplace=True)\n",
            "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): ReLU(inplace=True)\n",
            "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): ReLU(inplace=True)\n",
            "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_c2L3v0r2xG"
      },
      "source": [
        "##Aggiunta classificatore e freeze parametri"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1qeI3_3qKJJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eda3217-8469-47d8-9c02-ca10fca1d3bc"
      },
      "source": [
        "model_conv.classifier[6]=nn.Linear(4096,2)\n",
        "#model_conv.classifier=nn.Linear(1920,2)\n",
        "\n",
        "for param in model_conv.classifier.parameters():\n",
        "  param.requires_grad = True #invece pongo il calcolo del gradiente del classificatore, ovvero attivo il calcolo dei pesi\n",
        "\n",
        "print(model_conv) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): ReLU(inplace=True)\n",
            "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (24): ReLU(inplace=True)\n",
            "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): ReLU(inplace=True)\n",
            "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): ReLU(inplace=True)\n",
            "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geVE_mjLtMpz"
      },
      "source": [
        "#**Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSAc21gAk70m"
      },
      "source": [
        "##Def train definitiva"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Xk71F1Yk9yZ"
      },
      "source": [
        "from collections import OrderedDict\n",
        "from functools import partial\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "from functools import partial\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "##### Data utils #####\n",
        "\n",
        "def log_to_message(log):\n",
        "    fmt = \"{0}: {1}\"\n",
        "    return \"    \".join(fmt.format(k, v) for k, v in log.items())\n",
        "\n",
        "\n",
        "class ProgressBar(object):\n",
        "    \"\"\"Cheers @ajratner\"\"\"\n",
        "\n",
        "    def __init__(self, n, length=40):\n",
        "        # Protect against division by zero\n",
        "        self.n      = max(1, n)\n",
        "        self.nf     = float(n)\n",
        "        self.length = length\n",
        "        # Precalculate the i values that should trigger a write operation\n",
        "        #self.ticks = set([round(i/100.0 * n) for i in range(101)])\n",
        "        #self.ticks.add(n-1)\n",
        "        self.ticks=range(n)\n",
        "        self.bar(0)\n",
        "\n",
        "    def bar(self, i, message=\"\"):\n",
        "        \"\"\"Assumes i ranges through [0, n-1]\"\"\"\n",
        "        if i in self.ticks:\n",
        "            b = int(np.ceil(((i+1) / self.nf) * self.length))\n",
        "            sys.stdout.write(\"\\r[{0}{1}] {2}%\\t{3}\".format(\n",
        "                \"=\"*b, \" \"*(self.length-b), int(100*((i+1) / self.nf)), message\n",
        "            ))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "    def close(self, message=\"\"):\n",
        "        # Move the bar to 100% before closing\n",
        "        self.bar(self.n-1)\n",
        "        sys.stdout.write(\"{0}\\n\\n\".format(message))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "\n",
        "\n",
        "def training_f(train_dataset, numEpochs, model_conv, criterionCNN, optimizer_conv,batch_size, validation_split=0):\n",
        "  \n",
        "  best_acc = 0\n",
        "  best_loss=0\n",
        "  best_epoca = 0\n",
        "  \n",
        "  best_model_wts = copy.deepcopy(model_conv.state_dict())\n",
        "\n",
        "  if validation_split:\n",
        "      train_size = int(len(train_dataset) * 0.8)\n",
        "      val_size = len(train_dataset) - train_size\n",
        "      train_dataset, val_set = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "      traingen = torch.utils.data.DataLoader(train_dataset, pin_memory=True, batch_size=batch_size, shuffle=True,num_workers=4)\n",
        "      valgen = torch.utils.data.DataLoader(val_set, pin_memory=True, batch_size=batch_size)\n",
        "  \n",
        "  else: traingen = torch.utils.data.DataLoader(train_dataset, pin_memory=True, batch_size=batch_size, shuffle=True,num_workers=4)\n",
        "      \n",
        "  for epochs in range(1,numEpochs + 1):\n",
        "    \n",
        "    print(\"Epoch {0} / {1}\".format(epochs, numEpochs))\n",
        "    log = OrderedDict()\n",
        "    modelLoss_train = 0.0\n",
        "    modelAcc_train = 0.0\n",
        "    \n",
        "    model_conv.train() \n",
        "    \n",
        "    totalSize = 0\n",
        "    batch_i=0\n",
        "    num_batch=int(np.ceil(len(train_dataset)/batch_size))\n",
        "    pb = ProgressBar(num_batch)\n",
        "    #for each batch: operazioni per gli algoritmi di ottimizzazione\n",
        "    for inputs,labels in traingen:\n",
        "      inputs = inputs.type(torch.FloatTensor).cuda()\n",
        "      labels = labels.cuda()\n",
        "      \n",
        "      batch_i=batch_i+1\n",
        "      \n",
        "      optimizer_conv.zero_grad()\n",
        "      model_conv.zero_grad()\n",
        "      \n",
        "      y = model_conv(inputs)\n",
        "      outp, preds = torch.max(y, 1)   \n",
        "        \n",
        "      lossCNN = criterionCNN(y, labels) #media per batch\n",
        "\n",
        "      modelLoss_train += lossCNN.item() * inputs.size(0)\n",
        "      totalSize += inputs.size(0)\n",
        "      modelAcc_train += torch.sum(preds == labels.data).item()\n",
        "\n",
        "      log['batch']=\"{0} / {1}\".format(batch_i, num_batch)\n",
        "      log['loss'] =round(modelLoss_train/totalSize,4)\n",
        "      log['acc']=round(modelAcc_train/totalSize,4)\n",
        "      log['Validation']=False\n",
        "      pb.bar(batch_i, log_to_message(log))\n",
        "\n",
        "      lossCNN.backward()  # pred = f(x)   -> loss = L(f(x), l_true)\n",
        "      \n",
        "      optimizer_conv.step()\n",
        "    \n",
        "    #calcolo loss e accuracy\n",
        "    modelLoss_epoch_train = modelLoss_train/totalSize\n",
        "    modelAcc_epoch_train  = modelAcc_train/totalSize\n",
        "    \n",
        "    log['loss']=round(modelLoss_epoch_train,2)\n",
        "    log['acc']=round(modelAcc_epoch_train*100,2)\n",
        "    pb.bar(num_batch-1, log_to_message(log))\n",
        "\n",
        "    if validation_split:    \n",
        "        #validation\n",
        "               \n",
        "        model_conv.eval()\n",
        "        totalSize_val = 0\n",
        "        modelLoss_val = 0.0\n",
        "        modelAcc_val = 0.0\n",
        "\n",
        "        log['Validation']=True\n",
        "        log['loss_val']=modelLoss_val\n",
        "        log['acc_val']=modelAcc_val\n",
        "        pb.bar(num_batch-1, log_to_message(log))\n",
        "        batch_val_i=0\n",
        "        size_valgen=len(valgen)\n",
        "        for inputs,labels in valgen:\n",
        "          inputs = inputs.type(torch.FloatTensor).cuda()\n",
        "          labels = labels.cuda()\n",
        "          batch_val_i+=1\n",
        "          y = model_conv(inputs)\n",
        "          outp, preds = torch.max(y, 1)\n",
        "          lossCNN = criterionCNN(y, labels)\n",
        "\n",
        "          modelLoss_val += lossCNN.item() * inputs.size(0)\n",
        "          totalSize_val += inputs.size(0)\n",
        "          modelAcc_val += torch.sum(preds == labels.data).item()        \n",
        "\n",
        "          log['batch_val']=\"{0} / {1}\".format(batch_val_i,size_valgen )\n",
        "          log['loss_val'] =round(modelLoss_val/totalSize_val,4)\n",
        "          log['acc_val']=round(modelAcc_val/totalSize_val,4)\n",
        "          pb.bar(num_batch-1, log_to_message(log))\n",
        "\n",
        "        modelLoss_epoch_val=modelLoss_val/totalSize_val\n",
        "        modelAcc_epoch_val = modelAcc_val/totalSize_val\n",
        "\n",
        "        log['loss_val'] =round(modelLoss_epoch_val,2)\n",
        "        log['acc_val']=round(modelAcc_epoch_val*100,2)\n",
        "        \n",
        "        if (modelAcc_epoch_val > best_acc) or (modelAcc_epoch_val == best_acc and modelLoss_epoch_val < best_loss) :\n",
        "          best_acc = modelAcc_epoch_val\n",
        "          best_loss = modelLoss_epoch_val\n",
        "          best_epoca = epochs\n",
        "          best_model_wts = copy.deepcopy(model_conv.state_dict()) #salvo come modello quello che mi ha restituito risultati migliori in validation\n",
        "          \n",
        "        pb.close(log_to_message(log))\n",
        "\n",
        "    else: \n",
        "      best_model_wts = copy.deepcopy(model_conv.state_dict())\n",
        "      pb.close(log_to_message(log))\n",
        "\n",
        "\n",
        "  #pb.close(log_to_message(log))\n",
        "\n",
        "  model_conv.load_state_dict(best_model_wts)\n",
        "  return model_conv\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRhT4XM1sVm-"
      },
      "source": [
        "##Set iperparametri"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH1X3BB5sX6w"
      },
      "source": [
        "#iper-parametri iniziali\n",
        "learning_rate = 1e-6\n",
        "num_epoch = 20\n",
        "batch_size = 128\n",
        "momentum=0.9\n",
        "#richiamo funzione che effettua il training con validation\n",
        "\n",
        "model_conv = model_conv.cuda() #sposta i calcoli sulla gpu\n",
        "criterion = nn.CrossEntropyLoss() #criterio dell'aggiornamento del gradiente: minimizzazione funzione loss entropia\n",
        "optimizer_conv = optim.SGD(model_conv.classifier.parameters(),lr=learning_rate,momentum=momentum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44j4JhI0ti8T"
      },
      "source": [
        "##Addestramento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Cu1TY-Ttlgc",
        "outputId": "de7f925b-d026-430c-c15d-46576359d892"
      },
      "source": [
        "best_model=training_f(train_dataset, num_epoch, model_conv, criterion, optimizer_conv,batch_size=batch_size, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 20\n",
            "\r[=                                       ] 0%\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[========================================] 100%\tbatch: 125 / 125    loss: 3.0    acc: 59.03    Validation: True    loss_val: 0.71    acc_val: 77.48    batch_val: 32 / 32\n",
            "\n",
            "Epoch 2 / 20\n",
            "[========================================] 100%\tbatch: 125 / 125    loss: 1.81    acc: 69.27    Validation: True    loss_val: 0.58    acc_val: 81.25    batch_val: 32 / 32\n",
            "\n",
            "Epoch 3 / 20\n",
            "[========================================] 100%\tbatch: 125 / 125    loss: 1.51    acc: 72.47    Validation: True    loss_val: 0.49    acc_val: 83.25    batch_val: 32 / 32\n",
            "\n",
            "Epoch 4 / 20\n",
            "[========================================] 100%\tbatch: 125 / 125    loss: 1.3    acc: 74.39    Validation: True    loss_val: 0.45    acc_val: 84.5    batch_val: 32 / 32\n",
            "\n",
            "Epoch 5 / 20\n",
            "[========================================] 100%\tbatch: 125 / 125    loss: 1.18    acc: 75.81    Validation: True    loss_val: 0.42    acc_val: 85.02    batch_val: 32 / 32\n",
            "\n",
            "Epoch 6 / 20\n",
            "[========================================] 100%\tbatch: 125 / 125    loss: 1.12    acc: 76.86    Validation: True    loss_val: 0.41    acc_val: 85.6    batch_val: 32 / 32\n",
            "\n",
            "Epoch 7 / 20\n",
            "[========================================] 100%\tbatch: 125 / 125    loss: 1.04    acc: 77.89    Validation: True    loss_val: 0.38    acc_val: 86.15    batch_val: 32 / 32\n",
            "\n",
            "Epoch 8 / 20\n",
            "[========================================] 100%\tbatch: 125 / 125    loss: 0.99    acc: 78.23    Validation: True    loss_val: 0.35    acc_val: 86.92    batch_val: 32 / 32\n",
            "\n",
            "Epoch 9 / 20\n",
            "[========================================] 100%\tbatch: 125 / 125    loss: 0.95    acc: 78.54    Validation: True    loss_val: 0.35    acc_val: 87.17    batch_val: 32 / 32\n",
            "\n",
            "Epoch 10 / 20\n",
            "[========================================] 100%\tbatch: 125 / 125    loss: 0.86    acc: 79.95    Validation: True    loss_val: 0.34    acc_val: 87.25    batch_val: 32 / 32\n",
            "\n",
            "Epoch 11 / 20\n",
            "[========================================] 100%\tbatch: 125 / 125    loss: 0.84    acc: 79.76    Validation: True    loss_val: 0.35    acc_val: 87.42    batch_val: 32 / 32\n",
            "\n",
            "Epoch 12 / 20\n",
            "[========================================] 100%\tbatch: 125 / 125    loss: 0.85    acc: 79.89    Validation: True    loss_val: 0.34    acc_val: 87.65    batch_val: 32 / 32\n",
            "\n",
            "Epoch 13 / 20\n",
            "[========================================] 100%\tbatch: 125 / 125    loss: 0.8    acc: 80.24    Validation: True    loss_val: 0.33    acc_val: 87.88    batch_val: 32 / 32\n",
            "\n",
            "Epoch 14 / 20\n",
            "[========================================] 100%\tbatch: 125 / 125    loss: 0.76    acc: 81.18    Validation: True    loss_val: 0.32    acc_val: 88.28    batch_val: 32 / 32\n",
            "\n",
            "Epoch 15 / 20\n",
            "[========================================] 100%\tbatch: 125 / 125    loss: 0.75    acc: 81.1    Validation: True    loss_val: 0.3    acc_val: 88.52    batch_val: 32 / 32\n",
            "\n",
            "Epoch 16 / 20\n",
            "[========================================] 100%\tbatch: 125 / 125    loss: 0.72    acc: 81.88    Validation: True    loss_val: 0.29    acc_val: 88.75    batch_val: 32 / 32\n",
            "\n",
            "Epoch 17 / 20\n",
            "[========================================] 100%\tbatch: 125 / 125    loss: 0.69    acc: 81.64    Validation: True    loss_val: 0.29    acc_val: 88.85    batch_val: 32 / 32\n",
            "\n",
            "Epoch 18 / 20\n",
            "[========================================] 100%\tbatch: 125 / 125    loss: 0.68    acc: 82.36    Validation: True    loss_val: 0.2121    acc_val: 0.9062    batch_val: 1 / 32"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFFWkmjX2j5P"
      },
      "source": [
        "#**Salvataggio modello**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAie77UO2nTs"
      },
      "source": [
        "#salva modello su drive\n",
        "torch.save(best_model.state_dict(),path_model_save)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAU1QYvhrGVs"
      },
      "source": [
        "#**Testing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1l4P6ibEIb8"
      },
      "source": [
        "##test2 (tensori)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrMu7XJISbBr"
      },
      "source": [
        "###Caricamento dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWhHJqlqK_Mp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f95021f3-2b2c-4925-80d2-258822aa21fc"
      },
      "source": [
        "data_transform_test=transforms.Compose([transforms.ToTensor(),\n",
        "        lambda x: (x*255).type(torch.uint8)\n",
        "        ])\n",
        "\n",
        "test_dataset = datasets.ImageFolder(pathTestset,transform=data_transform_test)\n",
        "print(len(test_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdk85-2SSwCW"
      },
      "source": [
        "###Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWJD7Vq1TTBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f677e82-3939-4270-c9a2-157886f306a1"
      },
      "source": [
        "testgen=torch.utils.data.DataLoader(test_dataset, pin_memory=True, batch_size=1,num_workers=4)\n",
        "print(len(testgen))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-56wdwb5TLHl"
      },
      "source": [
        "###calcolo dim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXwz6REjS5XY"
      },
      "source": [
        "def calc_size(n):\n",
        "  return tuple(int(np.ceil(i * (80/100))) for i in n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVwvG_5SKo7x"
      },
      "source": [
        "###test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpYx-bzhEJl_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "257475e3-d318-417e-f338-2eadcf2e229b"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#best_model=model_conv\n",
        "Test = pd.DataFrame()\n",
        "best_model.eval()\n",
        "best_model.cuda()\n",
        "\n",
        "prob = nn.Softmax()\n",
        "\n",
        "data_transform_test= transforms.Compose([transforms.Resize([224,224]),\n",
        "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "i=0\n",
        "for input,label in testgen:\n",
        "  i+=1\n",
        "  sys.stdout.write(\"\\rImage {0}/{1}\".format(i,len(testgen)))\n",
        "  sys.stdout.flush()\n",
        "  n=input.shape\n",
        "  n_mod=calc_size(n[2:4])\n",
        "  crop_transform=transforms.TenCrop((n_mod[0],n_mod[1])).to(device)\n",
        "  crops=crop_transform(input)\n",
        "  live=0\n",
        "  spoof=0\n",
        "  for crop in crops:\n",
        "    crop=data_transform_test(crop).to(device)\n",
        "    crop=crop.type(torch.FloatTensor).cuda()\n",
        "    outputs = best_model(crop)\n",
        "    live+=outputs[0][0]\n",
        "    spoof+=outputs[0][1]\n",
        "  live=live/10\n",
        "  spoof=spoof/10\n",
        "  outputs=torch.Tensor([live,spoof])\n",
        "  outputs=prob(outputs)\n",
        "  predicted=np.argmax(outputs)\n",
        "  Test = Test.append({'real': classes_name[int(label)] ,\n",
        "                        'predicted': classes_name[predicted],\n",
        "                        'value_pred_live': outputs[0].numpy(),\n",
        "                        'value_pred_spoof': outputs[1].numpy()},ignore_index = True)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 3/2500"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 2500/2500"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRRSYT01GRfB"
      },
      "source": [
        "###accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yidZaf2vGRfC",
        "outputId": "b4c285f0-a2d1-42d1-ab47-178267e242e6"
      },
      "source": [
        "true_label = Test.real.values\n",
        "predicted = Test.predicted.values\n",
        "\n",
        "print(round((np.sum((true_label == predicted).astype(int)))/Test.shape[0],4)*100)\n",
        "#88"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86.83999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjMJIiX0GRfC",
        "outputId": "48a07c87-a2ce-4253-d572-344ad4cdbe7c"
      },
      "source": [
        "print(Test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2500, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c28JtuWWz8Tw"
      },
      "source": [
        "#Save pd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRoB0J0xaVmo"
      },
      "source": [
        "\n",
        "Test.to_excel(scanner_name+'2.xlsx',index=False)\n",
        "!cp '{scanner_name}2.xlsx' '/content/gdrive/MyDrive/Dataset_impronte/test/Preds_value/'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}